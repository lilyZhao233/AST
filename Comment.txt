 unexpected data appeared in an objectinputstream trying to read an object this exception occurs when the stream contains primitive data instead of the object expected by readobject the eof flag in the exception is true to indicate that no more primitive data is available the count field contains the number of bytes available to read 
 signals that an image format exception of some sort has occurred note that the classes in the com sun image codec jpeg package are not part of the core java apis they are a part of sun s jdk and jre distributions although other licensees may choose to distribute these classes developers cannot depend on their availability in non sun implementations we expect that equivalent functionality will eventually be available in a core api or standard extension 
 signals that a truncated file was detected this object contains the raster bufferedimage that has the partially decoded image data in it there is no indication of the portion of the raster that may or may not be good note that the classes in the com sun image codec jpeg package are not part of the core java apis they are a part of sun s jdk and jre distributions although other licensees may choose to distribute these classes developers cannot depend on their availability in non sun implementations we expect that equivalent functionality will eventually be available in a core api or standard extension 
 exception thrown when a ber encoding decoding error occurs this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an incorrect security level is handled this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an error occurs in an snmpsecuritymodel this api is a sun microsystems internal api and is subject to change without notice 
 constructs a new snmpstatusexception with the specified status error 
 is used internally to signal that the size of a pdu exceeds the packet size limitation you will not usually need to use this class except if you decide to implement your own link com sun jmx snmp snmppdufactory snmpdufactory object the varbindcount property contains the number of snmpvarbind successfully encoded before the exception was thrown its value is  when this number is unknown this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an snmpaccesscontrolsubsystem doesn t know the passed id this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when a needed model is not present in the engine this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an snmplcd has no modellcd associated to the model this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an snmpmsgprocessingsubsystem doesn t know the passed id this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when an snmpsecuritysubsystem doesn t know the passed id this api is a sun microsystems internal api and is subject to change without notice 
 this exception is thrown when the handled snmpsubsystem is unknown this api is a sun microsystems internal api and is subject to change without notice 
 thrown when the bcel attempts to read a class file and determines that the file is malformed or otherwise cannot be interpreted as a class file 
 this class represents an entry in the exception table of the code attribute and is used only there it contains a range in which a particular exception handler is active 
 thrown on internal errors extends runtimeexception so it hasn t to be declared in the throws clause every time 
 thrown by instructionlist remove when one or multiple disposed instruction are still being referenced by a instructiontargeter object i e the instructiontargeter has to be notified that one of the instructionhandle it is referencing is being removed from the instructionlist and thus not valid anymore making this an exception instead of a return value forces the user to handle these case explicitely in a try catch the following code illustrates how this may be done try il delete start ih end ih catch targetlostexception e instructionhandle targets   e gettargets for int i  i 
 exception thrown to indicate a syntax error in a regular expression this is a non checked exception because you should only have problems compiling a regular expression during development if you are making regular expresion programs dynamically then you can catch it if you wish but should not be forced to 
 author g todd miller
 author g todd miller
 author morten jorgensen
 author jacek ambroziak
 author jacek ambroziak
 base class for datatype exceptions for dtd types the exception can be created from an error message for schema types it needs an error code as defined in appendix c of the structure spec plus an array of arguents for error message substitution 
 a runtime exception that s thrown if an error happens when the application tries to get a dv factory instance 
 datatype exception for invalid facet this exception is only used by schema datatypes 
 datatype exception for invalid values 
 xerces internal
 signals that a malformed byte sequence was detected by a java io reader that decodes bytes of a given encoding into characters 
 xerces internal
 xpath exception 
 this exception might be thrown by any constraint checking method 
 wraps link saxexception and make it an unchecked exception xerces xni doesn t allow link saxexception to be thrown so when the user supplied error handler throws it it needs to be tunneled through xerces it is a bug if this exception leaks to the application fixme use xniexception for this purpose it s already doing this kind of saxexception tunneling 
 an xni parser configuration exception this exception class extends xniexception in order to differentiate between general parsing errors and configuration errors 
 a parsing exception this exception is different from the standard xni exception in that it stores the location in the document or its entities where the exception occurred 
 this exception is the base exception of all xni exceptions it can be constructed with an error message or used to wrap another exception object note by extending the java runtimeexception xni handlers and components are not required to catch xni exceptions but may explicitly catch them if so desired 
 simple implementation of domexception review several classes were implementing this internally it makes more sense to have one shared version 
 create a new dtmexception 
 signal catalog exception this exception is thrown if an error occurs loading a catalog file 
 class canonicalizationexception
 constructor invalidcanonicalizerexception
 param msgid
 constructor algorithmalreadyregisteredexception
 this exception is thrown if decoding of base data fails 
 the mother of all exceptions in this bundle it allows exceptions to have their messages translated to the different locales the xmlsecurity en properties file contains this line xml wrongelement   can t create a  from a  element usage in the java source is object exargs   constants tag transforms badelement throw new xmlsecurityexception xml wrongelement exargs additionally if another exception has been caught we can supply it too try catch exception oldex object exargs   constants tag transforms badelement throw new xmlsecurityexception xml wrongelement exargs oldex 
 the mother of all runtime exceptions in this bundle it allows exceptions to have their messages translated to the different locales the xmlsecurity en properties file contains this line xml wrongelement   can t create a  from a  element usage in the java source is object exargs   constants tag transforms badelement throw new xmlsecurityexception xml wrongelement exargs additionally if another exception has been caught we can supply it too try catch exception oldex object exargs   constants tag transforms badelement throw new xmlsecurityexception xml wrongelement exargs oldex 
 constructor contenthandleralreadyregisteredexception
 constructor invalidkeyresolverexception
 constructor keyresolverexception
 constructor storageresolverexception
 raised when the computed hash value doesn t match the given digestvalue additional human readable info is passed to the constructor this being the benefit of raising an exception or returning a value 
 raised if testing the signature value over digestvalue fails because of invalid signature 
 thrown by link com sun org apache xml internal security signature signedinfo verify whentesting the signature fails because of uninitialized link com sun org apache xml internal security signature reference s 
 raised if verifying a link com sun org apache xml internal security signature reference failsbecause of an uninitialized link com sun org apache xml internal security signature xmlsignatureinput 
 all xml signature related exceptions inherit herefrom 
 author christian geuer pollmann
 author christian geuer pollmann
 this exception is thrown if something related to the link com sun org apache xml internal security utils resolver resourceresolver goes wrong 
 this class is for throwing important checked exceptions over non checked methods it should be used with care and in limited circumstances this class is a copy of the one in com sun org apache xml internal utils it exists to cut the serializers dependancy on that package this class is not a public api it is only public because it is used by com sun org apache xml internal serializer 
 this is a special exception that is used to stop parsing when search for an element for instance when searching for xml stylesheet pis it is used to stop the parse once the document element is found 
 primary checked exception 
 certain functions may throw this error if they are paired with the incorrect parser 
 a new exception to add support for dom level  xpath api this class is needed to throw a org wc dom domexception with proper error code in createexpression method of xpathevaluatorimpl a dom level  class this class extends transformerexception because the error message includes information about where the xpath problem is in the stylesheet as well as the xpath expression itself 
 an exception that is thrown if the wrong number of arguments to an exception are specified by the stylesheet 
 this class implements an exception object that all xpath classes will throw in case of an error this class extends transformerexception and may hold other exceptions in the case of nested exceptions printstacktrace will dump all the traces of the nested exceptions not just the trace of this object 
 derived from xpathexception in order that xpath processor exceptions may be specifically caught 
 signals that an abstract window toolkit exception has occurred 
 constructs a cmmexception with the specified detail message 
 constructs a profiledataexception with the specified detail message 
 a class to encapsulate mimetype parsing related exceptions
 signals that the requested data is not supported in this flavor 
 this exception is thrown by various methods in the java awt dnd package it is usually thrown to indicate that the target in question is unable to undertake the requested operation that the present time since the underlying dnd system is not in the appropriate state 
 thrown by method createfont in the font class to indicate that the specified font is bad 
 the illegalpathstateexception represents an exception that is thrown if an operation is performed on a path that is in an illegal state with respect to the particular operation being performed such as appending a path segment to a link generalpath without an initial moveto 
 the noninvertibletransformexception class represents an exception that is thrown if an operation is performed requiring the inverse of an link affinetransform object but theaffinetransform is in a non invertible state 
 thrown when code that is dependent on a keyboard display or mouse is called in an environment that does not support a keyboard display or mouse 
 signals that an awt component is not in an appropriate state for the requested operation 
 the imagingopexception is thrown if one of the link bufferedimageop or link rasterop filter methods cannotprocess the image 
 the rasterformatexception is thrown if there is invalid layout information in the link raster 
 the printerabortexception class is a subclass of link printerexception and is used to indicate that a useror application has terminated the print job while it was in the process of printing 
 constructs a new printerexception object with the specified detail message 
 the printerioexception class is a subclass of link printerexception and is used to indicate that an io errorof some sort has occurred while printing as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the ioexception that terminated the print job that is provided at construction time and accessed via the link getioexception method is now known as the cause and may be accessed via the link throwable getcause method as well as the aforementioned legacy method 
 constructs an introspectionexception with a detailed message 
 constructs a propertyvetoexception with a detailed message 
 base class for character conversion exceptions 
 signals that an end of file or end of stream has been reached unexpectedly during input this exception is mainly used by data input streams to signal end of stream note that many other input operations return a special value on end of stream rather than throwing an exception 
 signals that an attempt to open the file denoted by a specified pathname has failed this exception will be thrown by the link fileinputstream link fileoutputstream and link randomaccessfile constructors when a filewith the specified pathname does not exist it will also be thrown by these constructors if the file does exist but for some reason is inaccessible for example when an attempt is made to open a read only file for writing 
 signals that an io operation has been interrupted an interruptedioexception is thrown to indicate that an input or output transfer has been terminated because the thread performing it was interrupted the field link bytestransferred indicates how many bytes were successfully transferred before the interruption occurred 
 thrown when the serialization runtime detects one of the following problems with a class the serial version of the class does not match that of the class descriptor read from the stream the class contains unknown datatypes the class does not have an accessible no arg constructor 
 indicates that one or more deserialized objects failed validation tests the argument should provide the reason for the failure 
 signals that an io exception of some sort has occurred this class is the general class of exceptions produced by failed or interrupted io operations 
 thrown when serialization or deserialization is not active 
 thrown when an instance is required to have a serializable interface the serialization runtime or the class of the instance can throw this exception the argument should be the name of the class 
 superclass of all exceptions specific to object stream classes 
 exception indicating the failure of an object read operation due to unread primitive data or the end of data belonging to a serialized object in the stream this exception may be thrown in two cases an attempt was made to read an object when the next element in the stream is primitive data in this case the optionaldataexception s length field is set to the number of bytes of primitive data immediately readable from the stream and the eof field is set to false an attempt was made to read past the end of data consumable by a class defined readobject or readexternal method in this case the optionaldataexception s eof field is set to true and the length field is set to  
 thrown when control information that was read from an object stream violates internal consistency checks 
 signals that a sync operation has failed 
 wraps an link ioexception with an unchecked exception 
 the character encoding is not supported 
 signals that a malformed string in modified utf  format has been read in a data input stream or by any class that implements the data input interface see the datainput class description for the format in which modified utf  strings are read and written 
 signals that one of the objectstreamexceptions was thrown during a write operation thrown during a read operation when one of the objectstreamexceptions was thrown during a write operation the exception that terminated the write can be found in the detail field the stream is reset to it s initial state and all references to objects already deserialized are discarded as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the exception causing the abort that is provided at construction time and accessed via the public link detail field is now known as thecause and may be accessed via the link throwable getcause method as well as the aforementioned legacy field 
 thrown to indicate that a program has attempted to access an element of an annotation whose type has changed after the annotation was compiled or serialized this exception can be thrown by the linkplain java lang reflect annotatedelement api used to read annotationsreflectively 
 thrown to indicate that a program has attempted to access an element of an annotation type that was added to the annotation type definition after the annotation was compiled or serialized this exception will not be thrown if the new element has a default value this exception can be thrown by the linkplain java lang reflect annotatedelement api used to read annotationsreflectively 
 thrown when an exceptional arithmetic condition has occurred for example an integer divide by zero throws an instance of this class code arithmeticexception objects may be constructed by thevirtual machine as if linkplain throwable throwable string throwable boolean boolean suppression were disabled and or thestack trace was not writable 
 thrown to indicate that an array has been accessed with an illegal index the index is either negative or greater than or equal to the size of the array 
 thrown to indicate that an attempt has been made to store the wrong type of object into an array of objects for example the following code generates an arraystoreexception object x   new string  x    new integer  
 thrown to indicate that the code has attempted to cast an object to a subclass of which it is not an instance for example the following code generates a classcastexception object x   new integer  system out println string x 
 thrown when an application tries to load in a class through its string name using the forname method in class class the findsystemclass method in class classloader the loadclass method in class classloader but no definition for the class with the specified name could be found as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the optional exception that was raised while loading the class that may be provided at construction time and accessed via the link getexception method isnow known as the cause and may be accessed via the link throwable getcause method as well as the aforementioned legacy method 
 thrown to indicate that the clone method in class object has been called to clone an object but that the object s class does not implement the cloneable interface applications that override the clone method can also throw this exception to indicate that an object could not or should not be cloned 
 thrown when an application tries to access an enum constant by name and the enum type contains no constant with the specified name this exception can be thrown by the linkplain java lang reflect annotatedelement api used to read annotationsreflectively 
 the class code exception and its subclasses are a form of code throwable that indicates conditions that a reasonableapplication might want to catch the class code exception and any subclasses that are not alsosubclasses of link runtimeexception are checkedexceptions checked exceptions need to be declared in a method or constructor s code throws clause if they can be thrownby the execution of the method or constructor and propagate outside the method or constructor boundary 
 an illegalaccessexception is thrown when an application tries to reflectively create an instance other than an array set or get a field or invoke a method but the currently executing method does not have access to the definition of the specified class field method or constructor 
 thrown to indicate that a method has been passed an illegal or inappropriate argument 
 thrown to indicate that a thread has attempted to wait on an object s monitor or to notify other threads waiting on an object s monitor without owning the specified monitor 
 signals that a method has been invoked at an illegal or inappropriate time in other words the java environment or java application is not in an appropriate state for the requested operation 
 thrown to indicate that a thread is not in an appropriate state for the requested operation see for example the suspend and resume methods in class thread 
 thrown to indicate that an index of some sort such as to an array to a string or to a vector is out of range applications can subclass this class to indicate similar exceptions 
 thrown when an application tries to create an instance of a class using the code newinstance method in class code class but the specified class object cannot be instantiated the instantiation can fail for a variety of reasons including but not limited to the class object represents an abstract class an interface an array class a primitive type or code void the class has no nullary constructor 
 thrown by an implementation of link java lang instrument classfiletransformer transform classfiletransformer transform when its input parameters are invalid this may occur either because the initial class file bytes were invalid or a previously applied transform corrupted the bytes 
 thrown by an implementation of link java lang instrument instrumentation redefineclasses instrumentation redefineclasses when one of the specified classes cannot be modified 
 thrown when a thread is waiting sleeping or otherwise occupied and the thread is interrupted either before or during the activity occasionally a method may wish to test whether the current thread has been interrupted and if so to immediately throw this exception the following code can be used to achieve this effect if thread interrupted clears interrupted status throw new interruptedexception 
 constructs a code lambdaconversionexception 
 thrown to indicate that code has attempted to call a method handle via the wrong method type as with the bytecode representation of normal java method calls method handle calls are strongly typed to a specific type descriptor associated with a call site this exception may also be thrown when two method handles are composed and the system detects that their types cannot be matched up correctly this amounts to an early evaluation of the type mismatch at method handle construction time instead of when the mismatched method handle is called 
 thrown if an application tries to create an array with negative size 
 signals that the class doesn t have a field of a specified name 
 thrown when a particular method cannot be found 
 thrown when an application attempts to use code null in acase where an object is required these include calling the instance method of a code null object accessing or modifying the field of a code null object taking the length of code null as if it were an array accessing or modifying the slots of code null as if itwere an array throwing code null as if it were a code throwable value applications should throw instances of this class to indicate other illegal uses of the code null object code nullpointerexception objects may be constructed by thevirtual machine as if linkplain throwable throwable string throwable boolean boolean suppression were disabled and or thestack trace was not writable 
 thrown to indicate that the application has attempted to convert a string to one of the numeric types but that the string does not have the appropriate format 
 invocationtargetexception is a checked exception that wraps an exception thrown by an invoked method or constructor as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the target exception that is provided at construction time and accessed via the link gettargetexception method is now known as the cause and may be accessed via the link throwable getcause method as well as the aforementioned legacy method 
 thrown when a semantically malformed parameterized type is encountered by a reflective method that needs to instantiate it for example if the number of type arguments to a parameterized type is wrong 
 thrown when link java lang reflect executable getparameters thejava lang reflect package attempts to read method parameters froma class file and determines that one or more parameters are malformed the following is a list of conditions under which this exception can be thrown the number of parameters parameter count is wrong for the method a constant pool index is out of bounds a constant pool index does not refer to a utf  entry a parameter s name is or contains an illegal character the flags field contains an illegal flag something other than final synthetic or mandated see link java lang reflect executable getparameters for moreinformation 
 thrown by a method invocation on a proxy instance if its invocation handler s link invocationhandler invoke invoke method throws achecked exception a code throwable that is not assignableto code runtimeexception or code error that is not assignable to any of the exception types declared in the code throws clause of the method that was invoked on theproxy instance and dispatched to the invocation handler an code undeclaredthrowableexception instance containsthe undeclared checked exception that was thrown by the invocation handler and it can be retrieved with the code getundeclaredthrowable method code undeclaredthrowableexception extends code runtimeexception so it is an unchecked exception that wraps a checked exception as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the undeclared checked exception that was thrown by the invocation handler that may be provided at construction time and accessed via the link getundeclaredthrowable method is now known as thecause and may be accessed via the link throwable getcause method as well as the aforementioned legacymethod 
 common superclass of exceptions thrown by reflective operations in core reflection 
 code runtimeexception is the superclass of thoseexceptions that can be thrown during the normal operation of the java virtual machine code runtimeexception and its subclasses are uncheckedexceptions unchecked exceptions do not need to be declared in a method or constructor s code throws clause if theycan be thrown by the execution of the method or constructor and propagate outside the method or constructor boundary 
 thrown by the security manager to indicate a security violation 
 thrown by code string methods to indicate that an indexis either negative or greater than the size of the string for some methods such as the charat method this exception also is thrown when the index is equal to the size of the string 
 thrown when an application tries to access a type using a string representing the type s name but no definition for the type with the specified name can be found this exception differs from link classnotfoundexception in that classnotfoundexception is achecked exception whereas this exception is unchecked note that this exception may be used when undefined type variables are accessed as well as when types e g classes interfaces or annotation types are loaded in particular this exception can be thrown by the linkplain java lang reflect annotatedelement api used to read annotationsreflectively 
 thrown to indicate that the requested operation is not supported this class is a member of the java collections framework 
 signals that an error occurred while attempting to bind a socket to a local address and port typically the port is in use or the requested local address could not be assigned 
 signals that an error occurred while attempting to connect a socket to a remote address and port typically the connection was refused remotely e g no process is listening on the remote address port 
 thrown to indicate that a http request needs to be retried but cannot be retried automatically due to streaming mode being enabled 
 thrown to indicate that a malformed url has occurred either no legal protocol could be found in a specification string or the string could not be parsed 
 signals that an error occurred while attempting to connect a socket to a remote address and port typically the remote host cannot be reached because of an intervening firewall or if an intermediate router is down 
 signals that an icmp port unreachable message has been received on a connected datagram 
 thrown to indicate that there is an error in the underlying protocol such as a tcp error 
 thrown to indicate that there is an error creating or accessing a socket 
 signals that a timeout has occurred on a socket read or accept 
 thrown to indicate that the ip address of a host could not be determined 
 thrown to indicate that an unknown service exception has occurred either the mime type returned by a url connection does not make sense or the application is attempting to write to a read only url connection 
 checked exception thrown to indicate that a string could not be parsed as a uri reference 
 unchecked exception thrown when a relative put operation reaches the target buffer s limit 
 unchecked exception thrown when a relative get operation reaches the source buffer s limit 
 unchecked exception thrown when an attempt is made to initiate an accept operation on a channel and a previous accept operation has not completed 
 unchecked exception thrown when an attempt is made to bind the socket a network oriented channel that is already bound 
 unchecked exception thrown when an attempt is made to connect a link socketchannel that is already connected 
 checked exception received by a thread when another thread closes the channel or the part of the channel upon which it is blocked in an io operation 
 unchecked exception thrown when an attempt is made to use a selection key that is no longer valid 
 checked exception received by a thread when another thread interrupts it while it is blocked in an io operation upon a channel before this exception is thrown the channel will have been closed and the interrupt status of the previously blocked thread will have been set 
 checked exception thrown when an attempt is made to invoke or complete an io operation upon channel that is closed or at least closed to that operation that this exception is thrown does not necessarily imply that the channel is completely closed a socket channel whose write half has been shut down for example may still be open for reading 
 unchecked exception thrown when an attempt is made to invoke an io operation upon a closed selector 
 unchecked exception thrown when an attempt is made to connect a link socketchannel for which a non blocking connection operation is already inprogress 
 checked exception received by a thread when another thread interrupts it while it is waiting to acquire a file lock before this exception is thrown the interrupt status of the previously blocked thread will have been set 
 unchecked exception thrown when a blocking mode specific operation is invoked upon a channel in the incorrect blocking mode 
 unchecked exception thrown when an attempt is made to open a channel in a group that was not created by the same provider 
 unchecked exception thrown when an attempt is made to register a channel with a selector that was not created by the provider that created the channel 
 checked exception received by a thread when a timeout elapses before an asynchronous operation completes 
 unchecked exception thrown when the link socketchannel finishconnect finishconnect method of a link socketchannel is invoked without firstsuccessfully invoking its link socketchannel connect connect method 
 unchecked exception thrown when an attempt is made to read from a channel that was not originally opened for reading 
 unchecked exception thrown when an attempt is made to write to a channel that was not originally opened for writing 
 unchecked exception thrown when an attempt is made to invoke an io operation upon a server socket channel that is not yet bound 
 unchecked exception thrown when an attempt is made to invoke an io operation upon a socket channel that is not yet connected 
 unchecked exception thrown when an attempt is made to acquire a lock on a region of a file that overlaps a region already locked by the same java virtual machine or when another thread is already waiting to lock an overlapping region of the same file 
 unchecked exception thrown when an attempt is made to read from an asynchronous socket channel and a previous read has not completed 
 unchecked exception thrown when an attempt is made to construct a channel in a group that is shutdown or the completion handler for an io operation cannot be invoked because the channel group has terminated 
 unchecked exception thrown when an attempt is made to invoke a network operation upon an unresolved socket address 
 unchecked exception thrown when an attempt is made to bind or connect to a socket address of a type that is not supported 
 unchecked exception thrown when an attempt is made to write to an asynchronous socket channel and a previous write has not completed 
 checked exception thrown when a character encoding or decoding error occurs 
 unchecked exception thrown when a string that is not a legal charset name is used as such 
 checked exception thrown when an input byte sequence is not legal for given charset or an input character sequence is not a legal sixteen bit unicode sequence 
 checked exception thrown when an input character or byte sequence is valid but cannot be mapped to an output byte or character sequence 
 unchecked exception thrown when no support is available for a requested charset 
 checked exception thrown when a file system operation is denied typically due to a file permission or other access check this exception is not related to the link java security accesscontrolexception accesscontrolexception or link securityexception thrown by access controllers or security managers whenaccess to a file is denied 
 checked exception thrown when a file cannot be moved as an atomic file system operation 
 checked exception thrown when a lookup of link userprincipal fails becausethe principal does not exist 
 unchecked exception thrown when an attempt is made to invoke an operation on a directory stream that is closed 
 runtime exception thrown if an io error is encountered when iterating over the entries in a directory the io error is retrieved as an link ioexception using the link getcause getcause method 
 checked exception thrown when a file system operation fails because a directory is not empty 
 checked exception thrown when an attempt is made to create a file or directory and a file of that name already exists 
 constructs an instance of this class 
 thrown when a file system operation fails on one or two files this class is the general class for file system exceptions 
 checked exception thrown when a file system loop or cycle is encountered 
 constructs an instance of this class 
 unchecked exception thrown when path string cannot be converted into a link path because the path string contains invalid characters orthe path string is invalid for other file system specific reasons 
 checked exception thrown when an attempt is made to access a file that does not exist 
 checked exception thrown when a file system operation intended for a directory fails because the file is not a directory 
 checked exception thrown when a file system operation fails because a file is not a symbolic link 
 constructs an instance of this class 
 constructs an instance of this class 
 unchecked exception thrown when an attempt is made to update an object associated with a link filesystem isreadonly read only code filesystem 
 unchecked exception thrown when an attempt is made to reset a buffer when its mark is not defined 
 unchecked exception thrown when a content mutation method such as put or compact is invoked upon a read only buffer 
 an accessexception is thrown by certain methods of the java rmi naming class specifically bind rebind and unbind and methods of the java rmi activation activationsystem interface to indicate that the caller does not have permission to perform the action requested by the method call if the method was invoked from a non local host then an accessexception is thrown 
 this exception is thrown by the rmi runtime when activation fails during a remote call to an activatable object 
 general exception used by the activation interfaces as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the detail exception that may be provided at construction time and accessed via the public link detail field is now known as the cause and may beaccessed via the link throwable getcause method as well asthe aforementioned legacy field invoking the method link throwable initcause throwable on aninstance of activationexception always throws link illegalstateexception 
 an unknowngroupexception is thrown by methods of classes and interfaces in the java rmi activation package when the activationgroupid parameter to the method is determined to be invalid i e not known by the activationsystem an unknowngroupexception is also thrown if the activationgroupid in an activationdesc refers to a group that is not registered with the activationsystem
 an unknownobjectexception is thrown by methods of classes and interfaces in the java rmi activation package when the activationid parameter to the method is determined to be invalid an activationid is invalid if it is not currently known by the activationsystem an activationid is obtained by the activationsystem registerobject method an activationid is also obtained during the activatable register call 
 an alreadyboundexception is thrown if an attempt is made to bind an object in the registry to a name that already has an associated binding 
 a connectexception is thrown if a connection is refused to the remote host for a remote method call 
 a connectioexception is thrown if an ioexception occurs while making a connection to the remote host for a remote method call 
 a marshalexception is thrown if a java io ioexception occurs while marshalling the remote call header arguments or return value for a remote method call a marshalexception is also thrown if the receiver does not support the protocol version of the sender if a marshalexception occurs during a remote method call the call may or may not have reached the server if the call did reach the server parameters may have been deserialized a call may not be retransmitted after a marshalexception and reliably preserve at most once call semantics 
 a nosuchobjectexception is thrown if an attempt is made to invoke a method on an object that no longer exists in the remote virtual machine if a nosuchobjectexception occurs attempting to invoke a method on a remote object the call may be retransmitted and still preserve rmi s at most once call semantics a nosuchobjectexception is also thrown by the method java rmi server remoteobject tostub and by the unexportobject methods of java rmi server unicastremoteobject and java rmi activation activatable and
 a notboundexception is thrown if an attempt is made to lookup or unbind in the registry a name that has no associated binding 
 a remoteexception is the common superclass for a number of communication related exceptions that may occur during the execution of a remote method call each method of a remote interface an interface that extends java rmi remote must list remoteexception in its throws clause as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the wrapped remote exception that may be provided at construction time and accessed via the public link detail field is now known as the cause andmay be accessed via the link throwable getcause method as well asthe aforementioned legacy field invoking the method link throwable initcause throwable on aninstance of remoteexception always throws link illegalstateexception 
 an rmisecurityexception signals that a security exception has occurred during the execution of one of java rmi rmisecuritymanager s methods 
 an exportexception is a remoteexception thrown if an attempt to export a remote object fails a remote object is exported via the constructors and exportobject methods of java rmi server unicastremoteobject and java rmi activation activatable 
 a servercloneexception is thrown if a remote exception occurs during the cloning of a unicastremoteobject as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the nested exception that may be provided at construction time and accessed via the public link detail field is now known as the cause and may beaccessed via the link throwable getcause method as well asthe aforementioned legacy field invoking the method link throwable initcause throwable on aninstance of servercloneexception always throws link illegalstateexception 
 an servernotactiveexception is an exception thrown during a call to remoteserver getclienthost if the getclienthost method is called outside of servicing a remote method call 
 this exception is thrown when a call is received that does not match the available skeleton it indicates either that the remote method names or signatures in this interface have changed or that the stub class used to make the call and the skeleton receiving the call were not generated by the same version of the stub compiler rmic 
 a skeletonnotfoundexception is thrown if the skeleton corresponding to the remote object being exported is not found skeletons are no longer required so this exception is never thrown 
 an obsolete subclass of link exportexception 
 a serverexception is thrown as a result of a remote method invocation when a remoteexception is thrown while processing the invocation on the server either while unmarshalling the arguments or executing the remote method itself a serverexception instance contains the original remoteexception that occurred as its cause 
 from a server executing on jdk nbsp   a serverruntimeexception is thrown as a result of a remote method invocation when a runtimeexception is thrown while processing the invocation on the server either while unmarshalling the arguments executing the remote method itself or marshalling the return value a serverruntimeexception instance contains the original runtimeexception that occurred as its cause a serverruntimeexception is not thrown from servers executing on the java  platform v  or later versions 
 a stubnotfoundexception is thrown if a valid stub class could not be found for a remote object when it is exported a stubnotfoundexception may also be thrown when an activatable object is registered via the java rmi activation activatable register method 
 an unexpectedexception is thrown if the client of a remote method call receives as a result of the call a checked exception that is not among the checked exception types declared in the throws clause of the method in the remote interface 
 an unknownhostexception is thrown if a java net unknownhostexception occurs while creating a connection to the remote host for a remote method call 
 an unmarshalexception can be thrown while unmarshalling the parameters or results of a remote method call if any of the following conditions occur if an exception occurs while unmarshalling the call header if the protocol for the return value is invalid if a java io ioexception occurs unmarshalling parameters on the server side or the return value on the client side if a java lang classnotfoundexception occurs during unmarshalling parameters or return values if no skeleton can be loaded on the server side note that skeletons are required in the   stub protocol but not in the   stub protocol if the method hash is invalid i e missing method if there is a failure to create a remote reference object for a remote object s stub when it is unmarshalled 
 this exception is thrown by the accesscontroller to indicate that a requested access to a critical system resource such as the file system or the network is denied the reason to deny access can vary for example the requested permission might be of an incorrect type contain an invalid value or request access that is not allowed according to the security policy such information should be given whenever possible at the time the exception is thrown 
 this is an exception that is thrown whenever a reference is made to a non existent acl access control list 
 this is an exception that is thrown whenever an attempt is made to delete the last owner of an access control list 
 this is an exception that is thrown whenever the modification of an object such as an access control list is only allowed to be done by an owner of the object but the principal attempting the modification is not an owner 
 certificate encoding exception this is thrown whenever an error occurs while attempting to encode a certificate 
 this exception indicates one of a variety of certificate problems 
 certificate expired exception this is thrown whenever the current code date or the specified code date is after the code notafter date time specified in the validity periodof the certificate 
 certificate is not yet valid exception this is thrown whenever the current code date or the specified code date is before the code notbefore date time in the certificatevalidity period 
 certificate parsing exception this is thrown whenever an invalid der encoded certificate is parsed or unsupported der features are found in the certificate 
 an exception that indicates an x  certificate is revoked a code certificaterevokedexception contains additional informationabout the revoked certificate such as the date on which the certificate was revoked and the reason it was revoked 
 an exception indicating one of a variety of problems encountered when building a certification path with a code certpathbuilder a code certpathbuilderexception provides support for wrappingexceptions the link getcause getcause method returns the throwable if any that caused this exception to be thrown concurrent access unless otherwise specified the methods defined in this class are not thread safe multiple threads that need to access a single object concurrently should synchronize amongst themselves and provide the necessary locking multiple threads each manipulating separate objects need not synchronize 
 an exception indicating one of a variety of problems encountered when validating a certification path a code certpathvalidatorexception provides support for wrappingexceptions the link getcause getcause method returns the throwable if any that caused this exception to be thrown a code certpathvalidatorexception may also include thecertification path that was being validated when the exception was thrown the index of the certificate in the certification path that caused the exception to be thrown and the reason that caused the failure use the link getcertpath getcertpath link getindex getindex and link getreason getreason methods to retrieve this information concurrent access unless otherwise specified the methods defined in this class are not thread safe multiple threads that need to access a single object concurrently should synchronize amongst themselves and provide the necessary locking multiple threads each manipulating separate objects need not synchronize 
 an exception indicating one of a variety of problems retrieving certificates and crls from a code certstore a code certstoreexception provides support for wrappingexceptions the link getcause getcause method returns the throwable if any that caused this exception to be thrown concurrent access unless otherwise specified the methods defined in this class are not thread safe multiple threads that need to access a single object concurrently should synchronize amongst themselves and provide the necessary locking multiple threads each manipulating separate objects need not synchronize 
 crl certificate revocation list exception 
 this is the generic message digest exception 
 the code generalsecurityexception class is a genericsecurity exception class that provides type safety for all the security related exception classes that extend from it 
 this is the exception for invalid or inappropriate algorithm parameters 
 this is the exception for invalid keys invalid encoding wrong length uninitialized etc 
 this exception designed for use by the jca jce engine classes is thrown when an invalid parameter is passed to a method 
 this is the basic key exception 
 this is the general key management exception for all operations dealing with key management examples of subclasses of keymanagementexception that developers might create for giving more detailed information could include keyidconflictexception keyauthorizationfailureexception expiredkeyexception 
 this is the generic keystore exception 
 this exception is thrown when a particular cryptographic algorithm is requested but is not available in the environment 
 this exception is thrown when a particular security provider is requested but is not available in the environment 
 this exception is thrown by code doprivileged privilegedexceptionaction and code doprivileged privilegedexceptionaction accesscontrolcontext context to indicatethat the action being performed threw a checked exception the exception thrown by the action can be obtained by calling the code getexception method in effect an code privilegedactionexception is a wrapper for an exception thrown by a privileged action as of release   this exception has been retrofitted to conform to the general purpose exception chaining mechanism the exception thrown by the privileged computation that is provided at construction time and accessed via the link getexception method is now known as thecause and may be accessed via the link throwable getcause method as well as the aforementioned legacy method 
 a runtime exception for provider exceptions such as misconfiguration errors or unrecoverable internal errors which may be subclassed by providers to throw specialized provider specific runtime errors 
 this is the generic signature exception 
 this is the exception for invalid key specifications 
 this is the exception for invalid parameter specifications 
 this exception is thrown if an entry in the keystore cannot be recovered 
 this exception is thrown if a key in the keystore cannot be recovered 
 the subclass of link sqlexception thrown when an erroroccurs during a batch update operation in addition to the information provided by link sqlexception a batchupdateexception provides the update counts for all commands that were executed successfully during the batch update that is all commands that were executed before the error occurred the order of elements in an array of update counts corresponds to the order in which commands were added to the batch after a command in a batch update fails to execute properly and a batchupdateexception is thrown the driver may or may not continue to process the remaining commands in the batch if the driver continues processing after a failure the array returned by the method batchupdateexception getupdatecounts will have an element for every command in the batch rather than only elements for the commands that executed successfully before the error in the case where the driver continues processing commands the array element for any command that failed is statement execute failed a jdbc driver implementation should use the constructor code batchupdateexception string reason string sqlstate int vendorcode long updatecounts throwable cause instead ofconstructors that take code int for the update counts to avoid thepossibility of overflow if code statement executelargebatch method is invoked it is recommended that code getlargeupdatecounts be called instead of code getupdatecounts in order to avoid a possible overflow of the integer update count 
 the subclass of link sqlexception is thrown when one or more client info propertiescould not be set on a connection in addition to the information provided by sqlexception a sqlclientinfoexception provides a list of client info properties that were not set some databases do not allow multiple client info properties to be set atomically for those databases it is possible that some of the client info properties had been set even though the connection setclientinfo method threw an exception an application can use the getfailedproperties method to retrieve a list of client info properties that were not set the properties are identified by passing a map lt string clientinfostatus gt to the appropriate sqlclientinfoexception constructor 
 the subclass of link sqlexception thrown when the sqlstate class valueis  or under vendor specified conditions this indicates various data errors including but not limited to data conversion errors division by  and invalid arguments to functions please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 constructs a sqlexception object with a given reason sqlstate and vendorcode the cause is not initialized and may subsequently be initialized by a call to the link throwable initcause java lang throwable method 
 the subclass of link sqlexception thrown when the sqlstate class value is a the value is zero a this indicates that the jdbc driver does not support an optional jdbc feature optional jdbc features can fall into the fallowing categories no support for an optional feature no support for an optional overloaded method no support for an optional mode for a method the mode for a method is determined based on constants passed as parameter values to a method 
 the subclass of link sqlexception thrown when the sqlstate class valueis  or under vendor specified conditions this indicates that an integrity constraint foreign key primary key or unique key has been violated please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception thrown when the sqlstate class valueis  or under vendor specified conditions this indicates that the authorization credentials presented during connection establishment are not valid please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception thrown for the sqlstateclass value  or under vendor specified conditions this indicates that the connection operation that failed will not succeed if the operation is retried without the cause of the failure being corrected please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception thrown when an instance where a retryof the same operation would fail unless the cause of the sqlexception is corrected 
 the subclass of link sqlexception thrown in situations where apreviously failed operation might be able to succeed if the application performs some recovery steps and retries the entire transaction or in the case of a distributed transaction the transaction branch at a minimum the recovery operation must include closing the current connection and getting a new connection 
 the subclass of link sqlexception thrown when the sqlstate class valueis  or under vendor specified conditions this indicates that the in progress query has violated sql syntax rules please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception thrown when the timeout specified by code statement setquerytimeout code drivermanager setlogintimeout code datasource setlogintimeout code xadatasource setlogintimeout has expired this exception does not correspond to a standard sqlstate 
 the subclass of link sqlexception thrown when the sqlstate class valueis  or under vendor specified conditions this indicates that the current statement was automatically rolled back by the database because of deadlock or other transaction serialization failures please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception for the sqlstate classvalue  or under vendor specified conditions this indicates that the connection operation that failed might be able to succeed if the operation is retried without any application level changes please consult your driver vendor documentation for the vendor specified conditions for which this exception may be thrown 
 the subclass of link sqlexception is thrown in situations where apreviously failed operation might be able to succeed when the operation is retried without any intervention by application level functionality 
 signals that an error has been reached unexpectedly while parsing 
 exception used to indicate a problem while calculating a date time this exception is used to indicate problems with creating querying and manipulating date time objects 
 an exception thrown when an error occurs during parsing this exception includes the text being parsed and the error index 
 unsupportedtemporaltypeexception indicates that a chronofield or chronounit is not supported for a temporal class 
 thrown to indicate a problem with time zone configuration this exception is used to indicate a problems with the configured time zone rules 
 exception thrown when a thread tries to wait upon a barrier that is in a broken state or which enters the broken state while the thread is waiting 
 exception indicating that the result of a value producing task such as a link futuretask cannot be retrieved because the task was cancelled 
 exception thrown when an error or other exception is encountered in the course of completing a result or task 
 exception thrown when attempting to retrieve the result of a task that aborted by throwing an exception this exception can be inspected using the link getcause method 
 exception thrown by an link executor when a task cannot beaccepted for execution 
 exception thrown when a blocking operation times out blocking operations for which a timeout is specified need a means to indicate that the timeout has occurred for many such operations it is possible to return a value that indicates timeout when that is not possible or desirable then code timeoutexception should bedeclared and thrown 
 this exception may be thrown by methods that have detected concurrent modification of an object when such modification is not permissible for example it is not generally permissible for one thread to modify a collection while another thread is iterating over it in general the results of the iteration are undefined under these circumstances some iterator implementations including those of all the general purpose collection implementations provided by the jre may choose to throw this exception if this behavior is detected iterators that do this are known as fail fast iterators as they fail quickly and cleanly rather that risking arbitrary non deterministic behavior at an undetermined time in the future note that this exception does not always indicate that an object has been concurrently modified by a different thread if a single thread issues a sequence of method invocations that violates the contract of an object the object may throw this exception for example if a thread modifies a collection directly while it is iterating over the collection with a fail fast iterator the iterator will throw this exception note that fail fast behavior cannot be guaranteed as it is generally speaking impossible to make any hard guarantees in the presence of unsynchronized concurrent modification fail fast operations throw code concurrentmodificationexception on a best effort basis therefore it would be wrong to write a program that depended on this exception for its correctness code concurrentmodificationexception should be used only to detect bugs 
 unchecked exception thrown when duplicate flags are provided in the format specifier unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 thrown by methods in the stack class to indicate that the stack is empty 
 unchecked exception thrown when a conversion and flag are incompatible unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when the formatter has been closed unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when a character with an invalid unicode code point as defined by link character isvalidcodepoint is passed to the link formatter unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when the argument corresponding to the format specifier is of an incompatible type unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when a format string contains an illegal syntax or a format specifier that is incompatible with the given arguments only explicit subtypes of this exception which correspond to specific errors should be instantiated 
 unchecked exception thrown when an illegal combination flags is given unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when the precision is a negative value other than  the conversion does not support a precision or the value is otherwise unsupported 
 unchecked exception thrown when the format width is a negative value other than  or is otherwise unsupported 
 thrown by methods in link locale and link locale builder toindicate that an argument is not a well formed bcp  tag 
 thrown by a scanner to indicate that the token retrieved does not match the pattern for the expected type or that the token is out of range for the expected type 
 thrown to indicate that an operation could not complete because the input did not conform to the appropriate xml document type for a collection of properties as per the link properties specification note that although invalidpropertiesformatexception inherits serializable interface from exception it is not intended to be serializable appropriate serialization methods are implemented to throw notserializableexception 
 signals that an error of some sort has occurred while reading from or writing to a jar file 
 unchecked exception thrown when there is a format specifier which does not have a corresponding argument or if an argument index refers to an argument that does not exist unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when the format width is required unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 signals that a resource is missing 
 thrown by various accessor methods to indicate that the element being requested does not exist 
 thrown to indicate that a preferences operation could not complete because of a failure in the backing store or a failure to contact the backing store 
 thrown to indicate that an operation could not complete because the input did not conform to the appropriate xml document type for a collection of preferences as per the link preferences specification 
 unchecked exception thrown to indicate a syntax error in a regular expression pattern 
 the toomanylistenersexception exception is used as part of the java event model to annotate and implement a unicast special case of a multicast event source the presence of a throws toomanylistenersexception clause on any given concrete implementation of the normally multicast void addxyzeventlistener event listener registration pattern is used to annotate that interface as implementing a unicast listener special case that is that one and only one listener may be registered on the particular event listener source concurrently 
 unchecked exception thrown when an unknown conversion is given unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 unchecked exception thrown when an unknown flag is given unless otherwise specified passing a null argument to any method or constructor in this class will cause a link nullpointerexception to be thrown 
 signals that a data format error has occurred 
 signals that a zip exception of some sort has occurred 
 indicates a link filer detected an attempt to open a file thatwould violate the guarantees provided by the code filer those guarantees include not creating the same file more than once not creating multiple files corresponding to the same type and not creating files for types with invalid names 
 constructs an iioexception with a given message string no underlying cause is set getcause will return null 
 an iioinvalidtreeexception is thrown when an attempt by an iiometadata object to parse a tree of iiometadatanodes fails the node that led to the parsing error may be stored as with any parsing error the actual error may occur at a different point that that where it is detected the node returned by getoffendingnode should merely be considered as a clue to the actual nature of the problem 
 this method is new in the dom l for attr interface could throw domexception here but its probably ok to always return false one reason for this is we have no good way to document this exception since this class iioattr is not a public class the rest of the methods that throw domexception are publically documented as such on iiometadatanode 
 indicates that an unknown kind of annotation value was encountered this can occur if the language evolves and new kinds of annotation values can be stored in an annotation may be thrown by an linkplain annotationvaluevisitor annotation value visitor toindicate that the visitor was created for a prior version of the language 
 indicates that an unknown kind of element was encountered this can occur if the language evolves and new kinds of elements are added to the code element hierarchy may be thrown by an linkplain elementvisitor element visitor to indicate that thevisitor was created for a prior version of the language 
 thrown when an application attempts to access the link class objectcorresponding to a link typemirror 
 thrown when an application attempts to access a sequence of link class objects each corresponding to a link typemirror 
 indicates that an unknown kind of type was encountered this can occur if the language evolves and new kinds of types are added to the code typemirror hierarchy may be thrown by a linkplain typevisitor type visitor to indicate that the visitor was createdfor a prior version of the language 
 superclass of exceptions which indicate that an unknown kind of entity was encountered this situation can occur if the language evolves and new kinds of constructs are introduced subclasses of this exception may be thrown by visitors to indicate that the visitor was created for a prior version of the language a common superclass for those exceptions allows a single catch block to have code handling them uniformly 
 the specified attribute does not exist or cannot be retrieved 
 thrown when an invalid mbean attribute is passed to a query constructing method this exception is used internally by jmx during the evaluation of a query user code does not usually see it 
 thrown when an invalid expression is passed to a method for constructing a query this exception is used internally by jmx during the evaluation of a query user code does not usually see it 
 thrown when an invalid string operation is passed to a method for constructing a query 
 the mbean is already registered in the repository 
 the specified mbean does not exist in the repository 
 an exception occurred during the introspection of an mbean 
 thrown when an attempt is made to apply either of the following a subquery expression to an mbean or a qualified attribute expression to an mbean of the wrong class this exception is used internally by jmx during the evaluation of a query user code does not usually see it 
 the value specified is not valid for the attribute 
 exceptions thrown by jmx implementations it does not include the runtime exceptions 
 runtime exceptions emitted by jmx implementations 
 the specified mbean listener does not exist in the repository 
 the format of the string does not correspond to a valid objectname 
 represents user defined exceptions thrown by mbean methods in the agent it wraps the actual user defined exception thrown this exception will be built by the mbeanserver when a call to an mbean method results in an unknown exception 
 wraps exceptions thrown by the preregister prederegister methods of the mbeanregistration interface 
 exception thrown when an invalid target object type is specified the serialversionuid of this class is l 
 this exception is thrown when an xml formatted string is being parsed into modelmbean objects or when xml formatted strings are being created from modelmbean objects it is also used to wrapper exceptions from xml parsers that may be used the serialversionuid of this class is l 
 exception thrown by the monitor when a monitor setting becomes invalid while the monitor is running as the monitor attributes may change at runtime a check is performed before each observation if a monitor attribute has become invalid a monitor setting exception is thrown 
 exception which occurs when trying to register an object in the mbean server that is not a jmx compliant mbean 
 this runtime exception is thrown to indicate that a method parameter which was expected to be an item name of a composite data or a row index of a tabular data is not valid 
 this runtime exception is thrown to indicate that the open type of an open data value is not the one expected 
 this runtime exception is thrown to indicate that the index of a row to be added to a tabular data instance is already used to refer to another row in this tabular data instance 
 this checked exception is thrown when an open type an open data or an open mbean metadata info instance could not be constructed because one or more validity constraints were not met 
 represents exceptions thrown in the mbean server when performing operations on mbeans 
 represents exceptions thrown in the mbean server when using the java lang reflect classes to invoke methods on mbeans it wraps the actual java lang exception thrown 
 this exception is raised when relation id provided for a relation is already used 
 this exception is raised when an invalid relation service is provided 
 invalid relation type this exception is raised when in a relation type there is already a relation type with that name or the same name has been used for two different role infos or no role info provided or one null role info provided 
 this exception is raised when in a role info its minimum degree is greater than its maximum degree 
 role value is invalid this exception is raised when in a role the number of referenced mbeans in given value is less than expected minimum degree or the number of referenced mbeans in provided value exceeds expected maximum degree or one referenced mbean in the value is not an object of the mbean class expected for that role or an mbean provided for that role does not exist 
 this class is the superclass of any exception which can be raised during relation management 
 this exception is raised when there is no relation for a given relation id in a relation service 
 this exception is raised when an access is done to the relation service and that one is not registered 
 this exception is raised when there is no relation type with given name in relation service 
 this exception is raised when there is no role info with given name in a given relation type 
 this exception is raised when a role in a relation does not exist or is not readable or is not settable 
 exception thrown by link jmxconnectorfactory and link jmxconnectorserverfactory when a provider exists forthe required protocol but cannot be used for some reason 
 exception thrown as the result of a remote link mbeanserver method invocation when an error is thrown while processing the invocation in the remote mbean server a jmxservererrorexception instance contains the original error that occurred as its cause 
 when a java lang error occurs in the agent it should be caught and re thrown as a runtimeerrorexception 
 represents runtime exceptions thrown by mbean methods in the agent it wraps the actual java lang runtimeexception exception thrown this exception will be built by the mbeanserver when a call to an mbean method throws a runtime exception 
 represents runtime exceptions thrown in the agent when performing operations on mbeans it wraps the actual java lang runtimeexception thrown 
 represents exceptions raised when a requested service is not supported 
 this exception is thrown when an authentication error occurs while accessing the naming or directory service an authentication error can happen for example when the credentials supplied by the user program is invalid or otherwise fails to authenticate the user to the naming directory service if the program wants to handle this exception in particular it should catch authenticationexception explicitly before attempting to catch namingexception after catching authenticationexception the program could reattempt the authentication by updating the resolved context s environment properties with the appropriate appropriate credentials synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when the particular flavor of authentication requested is not supported for example if the program is attempting to use strong authentication but the directory naming supports only simple authentication this exception would be thrown identification of a particular flavor of authentication is provider and server specific it may be specified using specific authentication schemes such those identified using sasl or a generic authentication specifier such as simple and strong if the program wants to handle this exception in particular it should catch authenticationnotsupportedexception explicitly before attempting to catch namingexception after catching authenticationnotsupportedexception the program could reattempt the authentication using a different authentication flavor by updating the resolved context s environment properties accordingly synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown to indicate that the operation reached a point in the name where the operation cannot proceed any further when performing an operation on a composite name a naming service provider may reach a part of the name that does not belong to its namespace at that point it can construct a cannotproceedexception and then invoke methods provided by javax naming spi namingmanager such as getcontinuationcontext to locate another provider to continue the operation if this is not possible this exception is raised to the caller of the context operation if the program wants to handle this exception in particular it should catch cannotproceedexception explicitly before attempting to catch namingexception a cannotproceedexception instance is not synchronized against concurrent multithreaded access multiple threads trying to access and modify cannotproceedexception should lock the object 
 this exception is thrown when the client is unable to communicate with the directory or naming service the inability to communicate with the service might be a result of many factors such as network partitioning hardware or interface problems failures on either the client or server side this exception is meant to be used to capture such communication problems synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when there is a configuration problem this can arise when installation of a provider was not done correctly or if there are configuration problems with the server or if configuration information required to access the provider or service is malformed or missing for example a request to use ssl as the security protocol when the service provider software was not configured with the ssl component would cause such an exception another example is if the provider requires that a url be specified as one of the environment properties but the client failed to provide it synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when attempting to destroy a context that is not empty if the program wants to handle this exception in particular it should catch contextnotemptyexception explicitly before attempting to catch namingexception for example after catching contextnotemptyexception the program might try to remove the contents of the context before reattempting the destroy synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when an operation attempts to add an attribute that already exists synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when an attempt is made to add or remove or modify an attribute its identifier or its values that conflicts with the attribute s schema definition or the attribute s state it is thrown in response to dircontext modifyattributes it contains a list of modifications that have not been performed in the order that they were supplied to modifyattributes if the list is null none of the modifications were performed successfully an attributemodificationexception instance is not synchronized against concurrent multithreaded access multiple threads trying to access and modify a single attributemodification instance should lock the object 
 this exception is thrown when an attempt is made to add to create an attribute with an invalid attribute identifier the validity of an attribute identifier is directory specific synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when an attempt is made to add or modify an attribute set that has been specified incompletely or incorrectly this could happen for example when attempting to add or modify a binding or to create a new subcontext without specifying all the mandatory attributes required for creation of the object another situation in which this exception is thrown is by specification of incompatible attributes within the same attribute set or attributes in conflict with that specified by the object s schema synchronization and serialization issues that apply to namingexception apply directly here 
 this class is thrown when an attempt is made to add to an attribute a value that conflicts with the attribute s schema definition this could happen for example if attempting to add an attribute with no value when the attribute is required to have at least one value or if attempting to add more than one value to a single valued attribute or if attempting to add a value that conflicts with the syntax of the attribute synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when the specification of the searchcontrols for a search operation is invalid for example if the scope is set to a value other than object scope onelevel scope subtree scope this exception is thrown synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when the specification of a search filter is invalid the expression of the filter may be invalid or there may be a problem with one of the parameters passed to the filter synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when attempting to access an attribute that does not exist synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a method in some ways violates the schema an example of schema violation is modifying attributes of an object that violates the object s schema definition another example is renaming or moving an object to a part of the namespace that violates the namespace s schema definition synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when resources are not available to complete the requested operation this might due to a lack of resources on the server or on the client there are no restrictions to resource types as different services might make use of different resources such restrictions might be due to physical limits and or administrative quotas examples of limited resources are internal buffers memory network bandwidth insufficientresourcesexception is different from limitexceededexception in that the latter is due to user system specified limits see limitexceededexception for details synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when the naming operation being invoked has been interrupted for example an application might interrupt a thread that is performing a search if the search supports being interrupted it will throw interruptednamingexception whether an operation is interruptible and when depends on its implementation as provided by the service providers different implementations have different ways of protecting their resources and objects from being damaged due to unexpected interrupts synchronization and serialization issues that apply to namingexception apply directly here 
 this exception indicates that the name being specified does not conform to the naming syntax of a naming system this exception is thrown by any of the methods that does name parsing such as those in context dircontext compositename and compoundname synchronization and serialization issues that apply to namingexception apply directly here 
 this abstract class is used to represent an ldap referral exception it extends the base referralexception by providing a getreferralcontext method that accepts request controls ldapreferralexception is an abstract class concrete implementations of it determine its synchronization and serialization properties a control array passed as a parameter to the getreferralcontext method is owned by the caller the service provider will not modify the array or keep a reference to it although it may keep references to the individual control objects in the array 
 this exception is thrown when a method terminates abnormally due to a user or system specified limit this is different from a insufficientresourceexception in that limitexceededexception is due to a user system specified limit for example running out of memory to complete the request would be an insufficient resource the client asking for  answers and getting back  is a size limit exception examples of these limits include client and server configuration limits such as size time number of hops etc synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is used to describe problems encounter while resolving links addition information is added to the base namingexception for pinpointing the problem with the link analogous to how namingexception captures name resolution information linkexception captures link name resolution information pinpointing the problem encountered while resolving a link all these fields may be null link resolved name portion of link name that has been resolved link resolved object object to which resolution of link name proceeded link remaining name portion of link name that has not been resolved link explanation detail explaining why link resolution failed a linkexception instance is not synchronized against concurrent multithreaded access multiple threads trying to access and modify a single linkexception instance should lock the object 
 this exception is thrown when a loop was detected will attempting to resolve a link or an implementation specific limit on link counts has been reached synchronization and serialization issues that apply to linkexception apply directly here 
 this exception is thrown when a malformed link was encountered while resolving or constructing a link synchronization and serialization issues that apply to linkexception apply directly here 
 this exception is thrown by methods to indicate that a binding cannot be added because the name is already bound to another object synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a component of the name cannot be resolved because it is not bound synchronization and serialization issues that apply to namingexception apply directly here 
 this is the superclass of all exceptions thrown by operations in the context and dircontext interfaces the nature of the failure is described by the name of the subclass this exception captures the information pinpointing where the operation failed such as where resolution last proceeded to resolved name portion of name that has been resolved resolved object object to which resolution of name proceeded remaining name portion of name that has not been resolved explanation detail explaining why name resolution failed root exception the exception that caused this naming exception to be thrown null is an acceptable value for any of these fields when null it means that no such information has been recorded for that field a namingexception instance is not synchronized against concurrent multithreaded access multiple threads trying to access and modify a single namingexception instance should lock the object this exception has been retrofitted to conform to the general purpose exception chaining mechanism the root exception or root cause is the same object as the cause returned by the link throwable getcause method 
 this is the superclass of security related exceptions thrown by operations in the context and dircontext interfaces the nature of the failure is described by the name of the subclass if the program wants to handle this exception in particular it should catch namingsecurityexception explicitly before attempting to catch namingexception a program might want to do this for example if it wants to treat security related exceptions specially from other sorts of naming exception synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when no initial context implementation can be created the policy of how an initial context implementation is selected is described in the documentation of the initialcontext class this exception can be thrown during any interaction with the initialcontext not only when the initialcontext is constructed for example the implementation of the initial context might lazily retrieve the context only when actual methods are invoked on it the application should not have any dependency on when the existence of an initial context is determined synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when attempting to perform an operation for which the client has no permission the access control permission model is dictated by the directory naming server synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a naming operation proceeds to a point where a context is required to continue the operation but the resolved object is not a context for example context destroy requires that the named object be a context if it is not notcontextexception is thrown another example is a non context being encountered during the resolution phase of the context methods it is also thrown when a particular subtype of context is required such as a dircontext and the resolved object is a context but not of the required subtype synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a context implementation does not support the operation being invoked for example if a server does not support the context bind method it would throw operationnotsupportedexception when the bind method is invoked on it synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown to indicate that the result being returned or returned so far is partial and that the operation cannot be completed for example when listing a context this exception indicates that returned results only represents some of the bindings in the context synchronization and serialization issues that apply to namingexception apply directly here 
 this abstract class is used to represent a referral exception which is generated in response to a referral such as that returned by ldap v servers a service provider provides a subclass of referralexception by providing implementations for getreferralinfo and getreferralcontext and appropriate constructors and or corresponding set methods the following code sample shows how referralexception can be used code while true try bindings   ctx listbindings name while bindings hasmore b   bindings next break catch referralexception e ctx   e getreferralcontext referralexception is an abstract class concrete implementations determine its synchronization and serialization properties an environment parameter passed to the getreferralcontext method is owned by the caller the service provider will not modify the object or keep a reference to it but may keep a reference to a clone of it 
 this exception is thrown when attempting to communicate with a directory or naming service and that service is not available it might be unavailable for different reasons for example the server might be too busy to service the request or the server might not be registered to service any requests etc synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a method produces a result that exceeds a size related limit this can happen for example if the result contains more objects than the user requested or when the size of the result exceeds some implementation specific limit synchronization and serialization issues that apply to namingexception apply directly here 
 this exception is thrown when a method does not terminate within the specified time limit this can happen for example if the user specifies that the method should take no longer than  seconds and the method fails to complete with  seconds synchronization and serialization issues that apply to namingexception apply directly here 
 thrown to indicate that the requested operation cannot be performed because the set is unmodifiable 
 interface attributeexception is a mixin interface which a subclass of link printexception printexception can implement to report an error conditioninvolving one or more printing attributes that a particular print service instance does not support either the attribute is not supported at all or the attribute is supported but the particular specified value is not supported the print service api does not define any print exception classes that implement interface attributeexception that being left to the print service implementor s discretion 
 interface flavorexception is a mixin interface which a subclass of link printexception printexception can implement to report an error conditioninvolving a doc flavor or flavors class link javax print docflavor docflavor the print service api does not define any print exception classes that implement interface flavorexception that being left to the print service implementor s discretion 
 construct a print exception with the given detail message 
 interface uriexception is a mixin interface which a subclass of link printexception printexception can implement to report an error conditioninvolving a uri address the print service api does not define any print exception classes that implement interface uriexception that being left to the print service implementor s discretion 
 the generic exception class for the scripting apis checked exception types thrown by underlying scripting implementations must be wrapped in instances of scriptexception the class has members to store line and column numbers and filenames if this information is available 
 signals that a code callbackhandler does notrecognize a particular code callback 
 signals that a code destroy operation failed this exception is thrown by credentials implementing the code destroyable interface when the code destroy method fails 
 a generic account exception 
 signals that a user account has expired this exception is thrown by loginmodules when they determine that an account has expired for example a code loginmodule after successfully authenticating a user may determine that the user s account has expired in this case the code loginmodule throws this exception to notify the application the application can then take the appropriate steps to notify the user 
 signals that an account was locked this exception may be thrown by a loginmodule if it determines that authentication is being attempted on a locked account 
 signals that an account was not found this exception may be thrown by a loginmodule if it is unable to locate an account necessary to perform authentication 
 a generic credential exception 
 signals that a code credential has expired this exception is thrown by loginmodules when they determine that a code credential has expired for example a code loginmodule authenticating a userin its code login method may determine that the user spassword although entered correctly has expired in this case the code loginmodule throws this exception to notifythe application the application can then take the appropriate steps to assist the user in updating the password 
 signals that a credential was not found this exception may be thrown by a loginmodule if it is unable to locate a credential necessary to perform authentication 
 signals that user authentication failed this exception is thrown by loginmodules if authentication failed for example a code loginmodule throws this exception ifthe user entered an incorrect password 
 this is the basic login exception 
 signals that a code refresh operation failed this exception is thrown by credentials implementing the code refreshable interface when the code refresh method fails 
 certificate encoding exception this is thrown whenever an error occurs whilst attempting to encode a certificate note the classes in the package code javax security cert exist for compatibility with earlier versions of the java secure sockets extension jsse new applications should instead use the standard java se certificate classes located in code java security cert 
 this exception indicates one of a variety of certificate problems note the classes in the package code javax security cert exist for compatibility with earlier versions of the java secure sockets extension jsse new applications should instead use the standard java se certificate classes located in code java security cert 
 certificate expired exception this is thrown whenever the current code date or the specified code date is after the code notafter date time specified in the validity periodof the certificate note the classes in the package code javax security cert exist for compatibility with earlier versions of the java secure sockets extension jsse new applications should instead use the standard java se certificate classes located in code java security cert 
 certificate is not yet valid exception this is thrown whenever the current code date or the specified code date is before the code notbefore date time in the certificatevalidity period note the classes in the package code javax security cert exist for compatibility with earlier versions of the java secure sockets extension jsse new applications should instead use the standard java se certificate classes located in code java security cert 
 certificate parsing exception this is thrown whenever invalid der encoded certificate is parsed or unsupported der features are found in the certificate note the classes in the package code javax security cert exist for compatibility with earlier versions of the java secure sockets extension jsse new applications should instead use the standard java se certificate classes located in code java security cert 
 this exception is thrown by a sasl mechanism implementation to indicate that the sasl exchange has failed due to reasons related to authentication such as an invalid identity passphrase or key note that the lack of an authenticationexception does not mean that the failure was not due to an authentication error a sasl mechanism implementation might throw the more general saslexception instead of authenticationexception if it is unable to determine the nature of the failure or if does not want to disclose the nature of the failure for example due to security reasons 
 this class represents an error that has occurred when using sasl 
 an invalidmididataexception indicates that inappropriate midi data was encountered this often means that the data is invalid in and of itself from the perspective of the midi specification an example would be an undefined status byte however the exception might simply mean that the data was invalid in the context it was used or that the object to which the data was given was unable to parse or use it for example a file reader might not be able to parse a type  midi file even though that format is defined in the midi specification 
 a midiunavailableexception is thrown when a requested midi component cannot be opened or created because it is unavailable this often occurs when a device is in use by another application more generally it can occur when there is a finite number of a certain kind of resource that can be used for some purpose and all of them are already in use perhaps all by this application for an example of the latter case see the link transmitter setreceiver receiver setreceiver method oftransmitter 
 a lineunavailableexception is an exception indicating that a line cannot be opened because it is unavailable this situation arises most commonly when a requested line is already in use by another application 
 an unsupportedaudiofileexception is an exception indicating that an operation failed because a file did not contain valid data of a recognized file type and format 
 creates a new serialexception with the specified message 
 indicates an error with syncfactory mechanism a disconnected rowset implementation cannot be used without a syncprovider being successfully instantiated
 indicates an error with the syncprovider mechanism this exception is created by a syncprovider abstract class extension if it encounters violations in reading from or writing to the originating data source if it is implemented to do so the syncprovider object may also create a syncresolver object and either initialize the syncproviderexception object with it at construction time or set it with the syncprovider object at a later time the method acceptchanges will throw this exception after the writer has finished checking for conflicts and has found one or more conflicts an application may catch a syncproviderexception object and call its getsyncresolver method to get its syncresolver object see the code fragment in the interface comment for syncresolver for an example this syncresolver object will mirror the rowset object that generated the exception except that it will contain only the values from the data source that are in conflict all other values in the syncresolver object will be null the syncresolver object may be used to examine and resolve each conflict in a row and then go to the next row with a conflict to repeat the procedure a syncproviderexception object may or may not contain a description of the condition causing the exception the inherited method getmessage may be called to retrieve the description if there is one 
 this exception is to report bad locations within a document model that is attempts to reference a location that doesn t exist warning serialized objects of this class will not be compatible with future swing releases the current serialization support is appropriate for short term storage or rmi between applications running the same version of swing as of   support for long term storage of all javabeans trade has been added to the java beans package please see link java beans xmlencoder 
 changedcharsetexception as the name indicates is an exception thrown when the charset is changed 
 exception used to stop and expand collapse from happening see how to write a tree will expand listener in the java tutorial for further information and examples 
 thrown when an undoableedit is told to redo and can t warning serialized objects of this class will not be compatible with future swing releases the current serialization support is appropriate for short term storage or rmi between applications running the same version of swing as of   support for long term storage of all javabeans trade has been added to the java beans package please see link java beans xmlencoder 
 thrown when an undoableedit is told to undo and can t warning serialized objects of this class will not be compatible with future swing releases the current serialization support is appropriate for short term storage or rmi between applications running the same version of swing as of   support for long term storage of all javabeans trade has been added to the java beans package please see link java beans xmlencoder 
 an exception that indicates the requested look amp feel management classes are not present on the user s system warning serialized objects of this class will not be compatible with future swing releases the current serialization support is appropriate for short term storage or rmi between applications running the same version of swing as of   support for long term storage of all javabeans trade has been added to the java beans package please see link java beans xmlencoder 
 exception that represents a failure in a jaxb operation this exception differs from link jaxbexception in thatthis is an unchecked exception while jaxbexception is a checked exception 
 this is the root exception class for all jaxb exceptions 
 this exception indicates that an error has occurred while performing a marshal operation that the provider is unable to recover from the validationeventhandler can cause this exception to be thrown during the marshal operations see link validationeventhandler handleevent validationevent validationeventhandler handleevent validationevent 
 this exception indicates that an error was encountered while getting or setting a property 
 this exception indicates that a violation of a dynamically checked type constraint was detected this exception can be thrown by the generated setter methods of the schema derived java content classes however since fail fast validation is an optional feature for jaxb providers to support not all setter methods will throw this exception when a type constraint is violated if this exception is throw while invoking a fail fast setter the value of the property is guaranteed to remain unchanged as if the setter were never called 
 this exception indicates that an error has occurred while performing an unmarshal operation that prevents the jaxb provider from completing the operation the validationeventhandler can cause this exception to be thrown during the unmarshal operations see link validationeventhandler handleevent validationevent validationeventhandler handleevent validationevent 
 this exception indicates that an error has occurred while performing a validate operation the validationeventhandler can cause this exception to be thrown during the validate operations see link validationeventhandler handleevent validationevent validationeventhandler handleevent validationevent 
 indicates an exceptional condition that occurred while executing a transform algorithm a transformexception can contain a cause another throwable that caused this transformexception to get thrown 
 indicates an exceptional condition that occurred during the xml signature generation or validation process an xmlsignatureexception can contain a cause another throwable that caused this xmlsignatureexception to get thrown 
 indicates an exceptional condition thrown by a link keyselector a keyselectorexception can contain a cause another throwable that caused this keyselectorexception to get thrown 
 indicates an exceptional condition that occurred during the xml marshalling or unmarshalling process a marshalexception can contain a cause another throwable that caused this marshalexception to get thrown 
 this exception is thrown when a particular xml mechanism is requested but is not available in the environment a nosuchmechanismexception can contain a cause another throwable that caused this nosuchmechanismexception to get thrown 
 indicates an exceptional condition thrown while dereferencing a link urireference a urireferenceexception can contain a cause another throwable that caused this urireferenceexception to get thrown 
 indicates a serious configuration error 
 indicates a serious configuration error 
 constructs a soapexception object with the given string as the reason for the exception being thrown 
 the base exception for unexpected processing errors this exception class is used to report well formedness errors as well as unexpected processing conditions 
 create a new transformerconfigurationexception with the string specified as an error message 
 method getlocator retrieves an instance of a sourcelocator object that specifies where an error occurred 
 the httpexception exception represents a xml http fault since there is no standard format for faults or exceptions in xml http messaging only the http status code is captured 
 the protocolexception class is a base class for exceptions related to a specific protocol binding subclasses are used to communicate protocol level fault information to clients and may be used on the server to control the protocol specific fault representation 
 the soapfaultexception exception represents a soap   or   fault a soapfaultexception wraps a saaj soapfault that manages the soap specific representation of faults the createfault method of javax xml soap soapfactory may be used to create an instance of javax xml soap soapfault for use with the constructor soapbinding contains an accessor for the soapfactory used by the binding instance note that the value of getfault is the only part of the exception used when searializing a soap fault refer to the soap specification for a complete description of soap faults 
 the webserviceexception class is the base exception class for all jax ws api runtime exceptions 
 xpathexception represents a generic xpath exception 
 xpathexpressionexception represents an error in an xpath expression 
 xpathfactoryconfigurationexception represents a configuration error in a xpathfactory environment 
 xpathfunctionexception represents an error with an xpath function 
 this exception is thrown whenever a gss api error occurs including any mechanism specific error it may contain both the major and the minor gss api status codes major error codes are those defined at the gss api level in this class minor error codes are mechanism specific error codes that can provide additional information the underlying mechanism implementation is responsible for setting appropriate minor status codes when throwing this exception aside from delivering the numeric error codes to the caller this class performs the mapping from their numeric values to textual representations 
 constructs an applicationexception from the corba repository id of the exception and an input stream from which the exception data can be read as its parameters 
 the indirection exception is a java specific system exception it is thrown when the orb s input stream is called to demarshal a value that is encoded as an indirection that is in the process of being demarshaled this can occur when the orb input stream calls the valuehandler to demarshal an rmi value whose state contains a recursive reference to itself because the top level valuehandler read value call has not yet returned a value the orb input stream s indirection table does not contain an entry for an object with the stream offset specified by the indirection tag the stream offset is returned in the exception s offset field 
 constructs an unknownexception object 
 the root class for all corba standard exceptions these exceptions may be thrown as a result of any corba operation invocation and may also be returned by many standard corba api methods the standard exceptions contain a minor code allowing more detailed specification and a completion status this class is subclassed to generate each one of the set of standard orb exceptions systemexception extends java lang runtimeexception thus none of the systemexception exceptions need to be declared in signatures of the java methods mapped from operations in idl interfaces 
 a class that contains user exceptions returned by the server when the client uses the dii to make an invocation any user exception returned from the server is enclosed in an any object contained in the unknownuserexception object this is available from the environment object returned by the method request env 
 the root class for corba idl defined user exceptions all corba user exceptions are checked exceptions which means that they need to be declared in method signatures 
 if an attempt is made to use an object that is not or is no longer usable 
 event operations may throw an eventexception as specified in their method descriptions see also the document object model dom level  events specification 
 range operations may throw a rangeexception as specified in their method descriptions see also the document object model dom level  traversal and range specification 
 encapsulate a general sax error or warning this module both source code and documentation is in the public domain and comes with no warranty see http www saxproject org for further information this class can contain basic error or warning information from either the xml parser or the application a parser writer or application writer can subclass it to provide additional functionality sax handlers may throw this exception or any exception subclassed from it if the application needs to pass through other types of exceptions it must wrap those exceptions in a saxexception or an exception derived from a saxexception if the parser or application needs to include information about a specific location in an xml document it should use the link org xml sax saxparseexception saxparseexception subclass 
 exception class for an unrecognized identifier this module both source code and documentation is in the public domain and comes with no warranty see http www saxproject org for further information an xmlreader will throw this exception when it finds an unrecognized feature or property identifier sax applications and extensions may use this class for other similar purposes 
 exception class for an unsupported operation this module both source code and documentation is in the public domain and comes with no warranty see http www saxproject org for further information an xmlreader will throw this exception when it recognizes a feature or property identifier but cannot perform the requested operation setting a state or value other sax applications and extensions may use this class for similar purposes 
 encapsulate an xml parse error or warning this module both source code and documentation is in the public domain and comes with no warranty see http www saxproject org for further information this exception may include information for locating the error in the original xml document as if it came from a link locator object note that although the application will receive a saxparseexception as the argument to the handlers in the link org xml sax errorhandler errorhandler interface the application is not actually required to throw the exception instead it can simply read the information in it and take a different action since this exception is a subclass of link org xml sax saxexception saxexception it inherits the ability to wrap another exception 
java io filenotfoundexception ioexception 
java io ioexception 
java lang interruptedexception 
 process the given comet event 
 the background thread that listens for incoming tcp ip connections and hands them off to an appropriate processor 
 the dofilter method of the filter is called by the container each time a request response pair is passed through the chain due to a client request for a resource at the end of the chain the filterchain passed into this method allows the filter to pass on the request and response to the next entity in the chain this method first examines the request to check whether the client support compression it simply just pass the request and response if there is no support for compression if the compression support is available it creates a compressionservletresponsewrapper object which compresses the content and modifies the header if the content length is big enough it then invokes the next entity in the chain using the filterchain object chain dofilter 
javax servlet servletexception 
 close this output stream causing any buffered data to be flushed and any further output data to throw an ioexception 
 flush any buffered data for this output stream which also causes the response to be committed 
 write the specified byte to our output stream 
 write b length bytes from the specified byte array to our output stream 
 write len bytes from the specified byte array starting at the specified offset to our output stream 
 create and return a servletoutputstream to write the content associated with this response 
 flush the buffer and commit this response 
 return the servlet output stream associated with this response 
 return the writer associated with this response 
java io unsupportedencodingexception ioexception 
 time the processing that is performed by all subsequent filters in the current filter stack including the ultimately invoked servlet 
java lang illegalargumentexception runtimeexception 
java lang arrayindexoutofboundsexception indexoutofboundsexception runtimeexception 
java lang numberformatexception illegalargumentexception runtimeexception 
java lang illegalaccessexception reflectiveoperationexception 
java lang reflect invocationtargetexception reflectiveoperationexception 
 since el  
javax el elexception runtimeexception 
java beans introspectionexception 
java lang nullpointerexception runtimeexception 
javax el propertynotfoundexception elexception runtimeexception 
javax el propertynotwritableexception elexception runtimeexception 
 create a new link expressionfactory passing in the provided link properties search order is the same as link newinstance 
java lang classnotfoundexception reflectiveoperationexception 
java lang securityexception runtimeexception 
java lang instantiationexception reflectiveoperationexception 
 called by the servlet container to indicate to a servlet that the servlet is being placed into service see link servlet init this implementation stores the link servletconfig object it receivesfrom the servlet container for later use when overriding this form of the method call super init config 
 called by the server via the service method to allow a servlet to handle a get request overriding this method to support a get request also automatically supports an http head request a head request is a get request that returns no body in the response only the request header fields when overriding this method read the request data write the response headers get the response s writer or output stream object and finally write the response data it s best to include content type and encoding when using a printwriter object to return the response set the content type before accessing the printwriter object the servlet container must write the headers before committing the response because in http the headers must be sent before the response body where possible set the content length header with the link javax servlet servletresponse setcontentlength method to allow the servlet container to use a persistent connection to return its response to the client improving performance the content length is automatically set if the entire response fits inside the response buffer when using http   chunked encoding which means that the response has a transfer encoding header do not set the content length header the get method should be safe that is without any side effects for which users are held responsible for example most form queries have no side effects if a client request is intended to change stored data the request should use some other http method the get method should also be idempotent meaning that it can be safely repeated sometimes making a method safe also makes it idempotent for example repeating queries is both safe and idempotent but buying a product online or modifying data is neither safe nor idempotent if the request is incorrectly formatted doget returns an http bad request message 
 receives an http head request from the protected service method and handles the request the client sends a head request when it wants to see only the headers of a response such as content type or content length the http head method counts the output bytes in the response to set the content length header accurately if you override this method you can avoid computing the response body and just set the response headers directly to improve performance make sure that the dohead method you write is both safe and idempotent that is protects itself from being called multiple times for one http head request if the http head request is incorrectly formatted dohead returns an http bad request message 
 called by the server via the service method to allow a servlet to handle a post request the http post method allows the client to send data of unlimited length to the web server a single time and is useful when posting information such as credit card numbers when overriding this method read the request data write the response headers get the response s writer or output stream object and finally write the response data it s best to include content type and encoding when using a printwriter object to return the response set the content type before accessing the printwriter object the servlet container must write the headers before committing the response because in http the headers must be sent before the response body where possible set the content length header with the link javax servlet servletresponse setcontentlength method to allow the servlet container to use a persistent connection to return its response to the client improving performance the content length is automatically set if the entire response fits inside the response buffer when using http   chunked encoding which means that the response has a transfer encoding header do not set the content length header this method does not need to be either safe or idempotent operations requested through post can have side effects for which the user can be held accountable for example updating stored data or buying items online if the http post request is incorrectly formatted dopost returns an http bad request message 
 called by the server via the service method to allow a servlet to handle a put request the put operation allows a client to place a file on the server and is similar to sending a file by ftp when overriding this method leave intact any content headers sent with the request including content length content type content transfer encoding content encoding content base content language content location content md and content range if your method cannot handle a content header it must issue an error message http  not implemented and discard the request for more information on http   see rfc  this method does not need to be either safe or idempotent operations that doput performs can have side effects for which the user can be held accountable when using this method it may be useful to save a copy of the affected url in temporary storage if the http put request is incorrectly formatted doput returns an http bad request message 
 called by the server via the service method to allow a servlet to handle a delete request the delete operation allows a client to remove a document or web page from the server this method does not need to be either safe or idempotent operations requested through delete can have side effects for which users can be held accountable when using this method it may be useful to save a copy of the affected url in temporary storage if the http delete request is incorrectly formatted dodelete returns an http bad request message 
 called by the server via the service method to allow a servlet to handle a trace request a trace returns the headers sent with the trace request to the client so that they can be used in debugging there s no need to override this method 
 receives standard http requests from the public service method and dispatches them to the domethod methods defined in this class this method is an http specific version of the link javax servlet servlet service method there s noneed to override this method 
 dispatches client requests to the protected service method there s no need to override this method 
 inheritdoc the default behavior of this method is to return link httpservletrequest authenticate httpservletresponse on the wrapped request object 
 inheritdoc the default behavior of this method is to return link httpservletrequest login string string on the wrapped request object 
 inheritdoc the default behavior of this method is to return link httpservletrequest logout on the wrapped request object 
 inheritdoc the default behavior of this method is to return link httpservletrequest getparts on the wrapped request object 
java lang illegalstateexception runtimeexception 
 inheritdoc the default behavior of this method is to return link httpservletrequest getpart string on the wrapped request object 
 the default behavior of this method is to call senderror int sc string msg on the wrapped response object 
 the default behavior of this method is to call senderror int sc on the wrapped response object 
 the default behavior of this method is to return sendredirect string location on the wrapped response object 
 parses data from an html form that the client sends to the server using the http post method and the application x www form urlencoded mime type the data sent by the post method contains key value pairs a key can appear more than once in the post data with different values however the key appears only once in the hashtable with its value being an array of strings containing the multiple values sent by the post method the keys and values in the hashtable are stored in their decoded form so any  characters are converted to spaces and characters sent in hexadecimal notation like xx are converted to ascii characters 
 clear the body without throwing any exceptions 
 reads the input stream one line at a time starting at an offset reads bytes into an array until it reads a certain number of bytes or reaches a newline character which it reads into the array as well this method returns  if it reaches the end of the input stream before reading the maximum number of bytes 
 writes a string to the client without a carriage return line feed crlf character at the end 
 writes a boolean value to the client with no carriage return line feed crlf character at the end 
 writes a character to the client with no carriage return line feed crlf at the end 
 writes an int to the client with no carriage return line feed crlf at the end 
 writes a long value to the client with no carriage return line feed crlf at the end 
 writes a float value to the client with no carriage return line feed crlf at the end 
 writes a double value to the client with no carriage return line feed crlf at the end 
 writes a carriage return line feed crlf to the client 
 writes a string to the client followed by a carriage return line feed crlf 
 writes a boolean value to the client followed by a carriage return line feed crlf 
 writes a character to the client followed by a carriage return line feed crlf 
 writes an int to the client followed by a carriage return line feed crlf character 
 writes a long value to the client followed by a carriage return line feed crlf 
 writes a float value to the client followed by a carriage return line feed crlf 
 writes a double value to the client followed by a carriage return line feed crlf 
 the default behavior of this method is to set the character encoding on the wrapped request object 
 the default behavior of this method is to return getinputstream on the wrapped request object 
 the default behavior of this method is to return getreader on the wrapped request object 
 the default behavior of this method is to return getoutputstream on the wrapped response object 
 the default behavior of this method is to return getwriter on the wrapped response object 
 the default behavior of this method is to call flushbuffer on the wrapped response object 
javax servlet jsp jspexception 
 create query string for the specified command 
 execute the specified command based on the configured properties 
org apache catalina ant buildexception object 
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
java net malformedurlexception ioexception 
java net protocolexception ioexception 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 create jkstatus link load balance example http localhost status cmd update mime txt w lb lf false ls true worker example http localhost status cmd update mime txt w node l lb wf  wd false ws false 
 get jmxconnection default look at jmx server project reference from jmxopen task 
 get value from mbeans attribute 
javax management malformedobjectnameexception operationsexception jmexception 
javax management mbeanexception jmexception 
javax management attributenotfoundexception operationsexception jmexception 
javax management instancenotfoundexception operationsexception jmexception 
javax management reflectionexception jmexception 
 this method evaluates the condition it support for operation    long and double 
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
java lang exception throwable object 
 create new mbean and when set from classloader objectname
javax management instancealreadyexistsexception operationsexception jmexception 
javax management mbeanregistrationexception mbeanexception jmexception 
javax management notcompliantmbeanexception operationsexception jmexception 
 return the value
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
 param jmxserverconnection
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
 param jmxserverconnection
 call mbean server for some mbeans with same domain attributes with attributebinding true you can save all attributes from all found objects as your ant properties
 param jmxserverconnection
javax management introspectionexception operationsexception jmexception 
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
 param jmxserverconnection
javax management invalidattributevalueexception operationsexception jmexception 
 get mbean attribute from mbean server
 execute the specified command this logic only performs the common attribute validation required by all subclasses it does not perform any functional logic directly 
 create a new jmx connection with auth when username and password is set 
 get current connection from ref parameter or create a new one 
 get jmxconnection
 convert string to datatype fixme how we can transfer values from ant project reference store ref 
java net unknownhostexception ioexception 
 execute the specified command based on the configured properties the input stream will be closed upon completion of this task whether it was executed successfully or not 
 unregister mbean
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the requested operation 
 execute the specified command this logic only performs the common attribute validation required by all subclasses it does not perform any functional logic directly 
org xml sax saxexception 
 execute the requested operation 
 enforce the security restrictions in the web application deployment descriptor of our associated context 
 authenticate the user making this request based on the login configuration of the link context with which this authenticator isassociated return true if any specified constraint has been satisfied or false if we have created a response challenge already 
 authenticate the user making this request based on the specified login configuration return true if any specified constraint has been satisfied or false if we have created a response challenge already 
java security nosuchalgorithmexception generalsecurityexception 
 authenticate the user making this request based on the specified login configuration return true if any specified constraint has been satisfied or false if we have created a response challenge already 
 authenticate the user making this request based on the specified login configuration return true if any specified constraint has been satisfied or false if we have created a response challenge already 
 called to forward to the login page
 called to forward to the error page
 restore the original request from information stored in our session if the original request is no longer present because the session timed out return false otherwise return true 
 save the original request information into our session 
 perform single sign on support processing for this request 
javax security auth login loginexception generalsecurityexception 
org ietf jgss gssexception 
java security privilegedactionexception 
 authenticate the user by checking for the existence of a certificate chain validating it against the trust manager for the connector and then validating the user s identity against the configured realm 
 pause the connector 
 pause the connector 
org apache catalina lifecycleexception 
 begin processing requests via this connector 
 terminate processing requests via this connector 
 event method 
 service method 
 parse additional request parameters 
 extract the path parameters from the request this assumes parameters are of the form path name value name value etc currently only really interested in the session id that will be in this form other parameters can safely be ignored 
 character conversion of the uri 
 close the stream since we re cycle we can t allow the call to super close which would permanently disable us 
 will send the buffer to the client 
 reads new bytes in the byte chunk 
 close the output buffer this tries to calculate the response size if the response has not been committed yet 
 flush bytes or chars contained in the buffer 
 flush bytes or chars contained in the buffer 
 sends the buffer data to the client output checking the state of response and calling the right interceptors 
 convert the chars to bytes then send the data to the client 
 append a string to the buffer
 read the reader wrapping the input stream for this request the default implementation wraps a bufferedreader around the servlet input stream returned by createinputstream 
 set the specified request attribute to the specified value 
 overrides the name of the character encoding used in the body of this request this method must be called prior to reading request parameters or reading input using getreader 
 return the value of the specified header as an integer or  if there is no such header for this request 
 return the principal that has been authenticated for this request 
 throws ioexception if an io error occurs
 inheritdoc 
 inheritdoc 
org apache tomcat util http fileupload fileuploadexception 
 inheritdoc 
 deprecated will be removed in tomcat   x 
javax naming namingexception 
 parse request parameters 
 read post body in an array 
 read chunked post body 
 parse accept language header value 
 sets the response status to link httpservletresponse sc switching protocols and flushes the response protocol specific headers must have already been set before this method is called 
 perform whatever actions are required to flush and close the output stream or writer in a single operation 
 return a printwriter that can be used to render error messages regardless of whether a stream or writer has already been acquired 
 flush the buffer and commit this response 
 return the writer associated with this response 
 clear any content written to the buffer 
 send an error response with the specified status and a default message 
 send a temporary redirect to the specified redirect location url 
 internal method that allows a redirect to be sent with a status other than link httpservletresponse sc found  no attempt is made tovalidate the status code 
 convert if necessary and return the absolute url that represents the resource referenced by this possibly relative url if this url is already absolute return it unchanged 
 return a requestdispatcher instance that acts as a wrapper for the resource at the given path the path must begin with a and is interpreted as relative to the current context root 
 return the url to the resource that is mapped to a specified path the path must begin with a and is interpreted as relative to the current context root 
java net urisyntaxexception 
 return the requested resource as an inputstream the path must be specified according to the rules described under getresource if no such resource can be identified return null 
 internal implementation of getresourcespath logic 
 add filter to context 
 add filter to context 
 add filter to context 
 add servlet to context 
 add servlet to context 
 add servlet to context 
 list resource paths recursively and store all of them in the given set 
java lang throwable object 
 deprecated as of java servlet api   with no direct replacement 
 use reflection to invoke the requested method cache the method object to speed up the process
 use reflection to invoke the requested method cache the method object to speed up the process
java lang nosuchmethodexception reflectiveoperationexception 
 use reflection to invoke the requested method cache the method object to speed up the process
 executes the method of the specified applicationcontext
 forward this request and response to another resource for processing any runtime exception ioexception or servletexception thrown by the called servlet will be propagated to the caller 
 prepare the request based on the filter configuration 
 include the response from another resource in the current response any runtime exception ioexception or servletexception thrown by the called servlet will be propagated to the caller 
 ask the resource represented by this requestdispatcher to process the associated request and create or append to the associated response implementation note this implementation assumes that no filters are applied to a forwarded or included resource because they were already done for the original request 
 invoke the next filter in this chain passing the specified request and response if there are no more filters in this chain invoke the service method of the servlet itself 
 骞?乑鰦  gL卿h filter dofilter 拐  gL servlet service 拐 愩檮?
java lang classcastexception runtimeexception 
 process the event using the security manager if the option is enabled 
 construct a new applicationfilterconfig for the specified filter definition 
 return the application filter we are configured for 
 release the filter instance associated with this filterconfig if there is one 
 construct and return a filterchain implementation that will wrap the execution of the specified servlet instance if we should not execute a filter chain at all return null 
 disallow senderror calls on an included response 
 disallow senderror calls on an included response 
 disallow sendredirect calls on an included response 
 primary entry point for startup and shutdown events 
 set the loader with which this container is associated 
 set the manager with which this container is associated 
 set the cluster with which this container is associated 
 set the realm with which this container is associated 
 process the specified request to produce the corresponding response by invoking the first valve in our pipeline if any or the basic valve otherwise 
 remove an existing child container from association with this parent container 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
java util concurrent executionexception 
 call postconstruct method on the specified instance recursively from deepest superclass to actual class 
 call predestroy method on the specified instance recursively from deepest superclass to actual class 
 inject resources in specified instance 
 inject resources in specified field 
 inject resources in specified method 
 primary entry point for startup and shutdown events 
javax xml parsers parserconfigurationexception 
org wc dom domexception runtimeexception 
org wc dom ls lsexception runtimeexception 
 acknowledge the occurrence of the specified event 
 create and initialize the jndi naming context 
 create an objectname for this contextresource object 
 set the specified ejbs in the naming context 
 set the specified environment entries in the naming context 
 set the specified web service in the naming context 
 set the specified resources in the naming context 
 set the specified resources in the naming context 
 set the specified resource link in the naming context 
 set the specified ejbs in the naming context 
 set the specified environment entries in the naming context 
 set the specified local ejbs in the naming context 
 set the specified web services in the naming context 
 set the specified resources in the naming context 
 set the specified resources in the naming context 
 set the specified resources in the naming context 
 create all intermediate subcontexts 
 return the locale to character set mapper for this context 
 set the naming resources for this web application 
 set the java class name of the wrapper implementation used for servlets registered in this context 
 get the absolute path to the work dir to avoid duplication 
 factory method to create and return a new wrapper instance of the java implementation class appropriate for this context implementation the constructor of the instantiated wrapper will have been called but no properties will have been set 
 reload this web application if reloading is supported implementation note this method is designed to deal with reloads required by changes to classes in the underlying repositories of our class loader and changes to the web xml file it does not handle changes to any context xml file if the context xml has changed you should stop this context and create and start a new context instance instead note that there is additional code in coyoteadapter postparserequest to handle mapping requests to paused contexts 
 configure and initialize the set of filters for this context return true if all filter initialization completed successfully or false otherwise 俚?we垁U 助web xml銗鳆刦ilterMn醥
 configure the set of instantiated application event listeners for this context return true if all listeners wre initialized successfully or false otherwise 
 send an application stop event to all interested listeners return true if all events were sent successfully or false otherwise 
 allocate resources including proxy return true if initialization was successfull or false otherwise 
 deallocate resources and destroy proxy 
 load and initialize all servlets marked load on startup in the web application deployment descriptor 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 destroy needs to clean up the context completely the problem is that undoing all the config in start and restoring a fresh state is impossible after stop destroy init start we should have the same state as if a fresh start was done i e read modified web xml etc this can only be done by completely removing the context object and remapping a new one or by cleaning up everything xxx should this be done in stop 
 set the appropriate context attribute for our work directory 
 jsr deploymentdescriptor attribute
javax management listenernotfoundexception operationsexception jmexception 
 remove a jmx notificationlistener 
 select the appropriate child wrapper to process this request based on the specified request uri if no matching wrapper can be found return an appropriate http error 
 select the appropriate child wrapper to process this request based on the specified request uri if no matching wrapper can be found return an appropriate http error 
 select the appropriate child host to process this request based on the requested server name if no matching host can be found return an appropriate http error 
 process comet event 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 select the appropriate child context to process this request based on the specified request uri if no matching context can be found return an appropriate http error 
 process comet event 
 handle the http status code and corresponding message generated while processing the specified request to produce the specified response any exceptions that occur during generation of the error report are logged and swallowed 
 handle the specified throwable encountered while processing the specified request to produce the specified response any exceptions that occur during generation of the exception report are logged and swallowed 
 handle an http status code or java exception by forwarding control to the location included in the specified errorpage object it is assumed that the caller has already recorded any request attributes that are to be forwarded to this page return true if we successfully utilized the specified error page location or false if the default error report should be rendered 
 start link valve s in this pipeline and implement the requirements of link lifecyclebase startinternal 
 stop link valve s in this pipeline and implement the requirements of link lifecyclebase stopinternal 
 set the valve instance that has been distinguished as the basic valve for this pipeline if any prior to setting the basic valve the valve s setcontainer will be called if it implements contained with the owning container as an argument the method may throw an illegalargumentexception if this valve chooses not to be associated with this container or illegalstateexception if it is already associated with a different container 
 add a new valve to the end of the pipeline associated with this container prior to adding the valve the valve s setcontainer method will be called if it implements contained with the owning container as an argument the method may throw an illegalargumentexception if this valve chooses not to be associated with this container or illegalstateexception if it is already associated with a different container 
 remove the specified valve from the pipeline associated with this container if it is found otherwise do nothing if the valve is found and removed the valve s setcontainer null method will be called if it implements contained 
 wait until a proper shutdown command is received then return this keeps the main thread alive the thread pool listening for http connections is daemon threads 
java net socketexception ioexception 
 write the configuration information for this entire server out to the server xml configuration file 
 write the configuration information for context out to the specified configuration file 
 start nested components link service s and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop nested components link service s and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 invoke a pre startup initialization this is used to allow connectors to bind to restricted ports under unix operating environments 
 add a new connector to the set of defined connectors and associate it with this service s container 
 remove the specified connector from the set associated from this service the removed connector will also be disassociated from our container 
 adds a named executor to the service
 removes an executor from the service
 start nested components link executor s link connector s and link container s and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop nested components link executor s link connector s and link container s and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 invoke a pre startup initialization this is used to allow connectors to bind to restricted ports under unix operating environments 
 start the component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop the component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 set the load on startup order value from a possibly null string per the specification any missing or non numeric value is converted to a zero so that this servlet will still be loaded at startup time but in an arbitrary order 
 gets the names of the methods supported by the underlying servlet this is the same set of methods included in the allow response header in response to an options request method processed by the underlying servlet 
 allocate an initialized instance of this servlet that is ready to have its service method called if the servlet class does not implement singlethreadmodel the only initialized instance may be returned immediately if the servlet class implements singlethreadmodel the wrapper implementation must ensure that this instance is not allocated again until it is deallocated by a call to deallocate 
 load and initialize an instance of this servlet if there is not already at least one initialized instance this can be used for example to load servlets that are marked in the deployment descriptor to be loaded at server startup time implementation note servlets whose classnames begin with org apache catalina so called container servlets are loaded by the same classloader that loaded this class rather than the classloader for the current web application this gives such classes access to catalina internals which are prevented for classes loaded for web applications 
 load and initialize an instance of this servlet if there is not already at least one initialized instance this can be used for example to load servlets that are marked in the deployment descriptor to be loaded at server startup time 
 unload all initialized instances of this servlet after calling the destroy method for each instance this can be used for example prior to shutting down the entire servlet engine or prior to reloading all of the classes from the loader associated with our loader s repository 
 return true if the specified class name represents a container provided servlet class that should be loaded by the server class loader 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 invoke the servlet we are managing respecting the rules regarding servlet lifecycle and singlethreadmodel support 
 process a comet event the main differences here are to not use senderror the response is committed to avoid creating a new filter chain which would work but be pointless and a few very minor tweaks 
 set the error code hack for default xmlmapper data type 
 add an environment entry for this web application 
 add a resource reference for this web application 
 add a resource link for this web application 
 remove any environment entry with the specified name 
 remove any resource reference with the specified name 
 remove any resource link with the specified name 
 close those resources that an explicit close may help clean up faster 
 clean up a resource by calling the defined close method for example closing a database connection pool will close it s open connections this will happen on gc but that leaves db connections open that may cause issues 
 handles a cors request of type link corsrequesttype simple 
 handles a request that s not a cors request but is a valid request i e it is not a cross origin request this implementation just forwards the request down the filter chain 
 parses each param value and populates configuration variables if a param is provided it overrides the default 
 checks if a given origin is valid or not criteria if an encoded character is present in origin it s not valid if origin is null it s valid origin should be a valid link uri 
 convert a comma delimited list of numbers into an code int 
 parse configuration lines like code access plus  month  days  hours or code modification  day  hours  seconds 
 extract the desired request property and pass it along with the specified request and response objects and associated filter chain to the protected process method to perform the actual filtering 
 extract the desired request property and pass it along with the comet event and filter chain to the protected process method to perform the actual filtering 
 extract the desired request property and pass it along with the specified request and response objects and associated filter chain to the protected process method to perform the actual filtering 
 extract the desired request property and pass it along with the comet event and filter chain to the protected process method to perform the actual filtering 
java text parseexception 
 wrap the incoming request in a link xforwardedrequest if the http header x forwareded for is not empty 
 log the interesting request parameters invoke the next filter in the sequence and log the interesting response parameters 
 perform the filtering that has been configured for this filter matching against the specified request property 
 perform the filtering that has been configured for this filter matching against the specified request property 
 select and set if specified the character encoding to be used to interpret request parameters for this request 
 check for the broken ms webdav client and if detected issue a re direct that hopefully will cause the non broken client to be used 
 callback from the cluster when a message is received the cluster will broadcast it invoking the messagereceived on the receiver 
 create factory for all transported war files
 install a new web application whose web application archive is at the specified url into this container and all the other members of the cluster with the specified context name if this application is successfully installed locally a containerevent of type install event will be sent to all registered listeners with the newly created context as an argument 
 remove an existing web application attached to the specified context name if this application is successfully removed a containerevent of type remove event will be sent to all registered listeners with the removed context as an argument deletes the web application war file and or directory if they exist in the host s appbase 
 return a file object representing the application root directory for our associated host 
 invoke the remove method on the deployer 
 invoke the check method on the deployer 
 invoke the check method on the deployer 
 invoke the check method on the deployer 
 invoke the check method on the deployer 
 copy a file to the specified temp directory 
 private constructor either instantiates a factory to read or write when openforwrite  true then a the file f will be created and an output stream is opened to write to it when openforwrite  false an input stream is opened the file has to exist 
 creates a factory to read or write from a file when opening for read the readmessage can be invoked and when opening for write the writemessage can be invoked 
 reads file data into the file message and sets the size totallength totalnrofmsgs and the message number if eof is reached the factory returns null and closes itself otherwise the same message is returned as was passed in this makes sure that not more memory is ever used to remember neither the file message or the factory are thread safe dont hand off the message to one thread and read the same with another 
 writes a message to file if msg getmessagenumber    msg gettotalnrofmsgs the output stream will be closed after writing 
 closes the factory its streams and sets all its references to null
 example usage 
javax management runtimeoperationsexception jmruntimeexception runtimeexception 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal starts the cluster communication channel this will connect with the other nodes in the cluster and request the current session state to be transferred to this node 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal this will disconnect the cluster communication channel and stop the listener thread 
java util regex patternsyntaxexception illegalargumentexception runtimeexception 
 serialize sessionid
 load sessionid
 load deltarequest from external node load the class at container classloader
 serialize deltarequest
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 wait that cluster session state is transfer or timeout after  sec with statetransfertimeout     wait that backup is transfered forever mode 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 when the request has been completed the replication valve will notify the manager and the manager will decide whether any replication is needed or not if there is a need for replication the manager will create a session message and that will be replicated the cluster determines where it gets sent session expiration also calls this method but with expires    true 
 this method is called by the received thread when a sessionmessage has been received from one of the other nodes in the cluster 
 handle receive session delta
 handle receive session is access at other node primary session is now false 
 handle receive session is expire at other node expire session also here 
 handle receive sessions from other not restart 
 handle receive that other node want all sessions restart a send all sessions with one message b send session at blocks after sending send state is complete transfered
 handle receive change sessionid at other node
 send a block of session to sender
 serialize deltarequest
 returns a diff and sets the dirty map to false
 applies a diff to an existing object 
 read a serialized version of the contents of this session object from the specified object input stream without requiring that the standardsession itself have been serialized 
 write a serialized version of the contents of this session object to the specified object output stream without requiring that the standardsession itself have been serialized 
 read a serialized version of this session object from the specified object input stream implementation note the reference to the owning manager is not restored by this method and must be set explicitly 
 write a serialized version of this session object to the specified object output stream implementation note the owning manager will not be stored in the serialized representation of this session after calling readobject you must set the associated manager explicitly implementation note any attribute that is not serializable will be unbound from the session with appropriate actions if it implements httpsessionbindinglistener if you do not want any such attributes be sure the distributable property of the associated manager is set to true 
 detect possible the jvmroute change at cluster backup node 
 callback from the cluster when a message is received the cluster will broadcast it invoking the messagereceived on the receiver 
 log the interesting request parameters invoke the next valve in the sequence and log the interesting response parameters 
 mark request that processed at primary node with attribute primaryindicatorname
 start cluster and implement the requirements of link org apache catalina util lifecyclebase startinternal 
org apache catalina tribes channelexception 
 stop cluster and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 send a cluster message to one member
java sql sqlexception 
 returns a copy of this class loader without any class file transformers this is a tool often used by java persistence api providers to inspect entity classes in the absence of any instrumentation something that can t be guaranteed within the context of a link java lang instrument classfiletransformer s link java lang instrument classfiletransformer transform classloader string class java security protectiondomain byte transform method the returned class loader s resource cache will have been cleared so that classes already instrumented will not be retained or returned 
 returns a copy of this class loader without any class file transformers this is a tool often used by java persistence api providers to inspect entity classes in the absence of any instrumentation something that can t be guaranteed within the context of a link java lang instrument classfiletransformer s link java lang instrument classfiletransformer transform classloader string class java security protectiondomain byte transform method the returned class loader s resource cache will have been cleared so that classes already instrumented will not be retained or returned 
 change the work directory 
 add a new repository to the set of places this classloader can look for classes to be loaded 
 have one or more classes or resources been modified so that a reload is appropriate 
 find the specified class in our local repositories if possible if not found throw classnotfoundexception 
 find the resource with the given name a resource is some data images audio text etc that can be accessed by class code in a way that is independent of the location of the code the name of a resource is a separated path name that identifies the resource if the resource cannot be found return null this method searches according to the following algorithm returning as soon as it finds the appropriate url if the resource cannot be found returns null if the delegate property is set to true call the getresource method of the parent class loader if any call findresource to find this resource in our locally defined repositories call the getresource method of the parent class loader if any 
 load the class with the specified name this method searches for classes in the same manner as loadclass string boolean with false as the second argument 
 returns the search path of urls for loading classes and resources this includes the original list of urls specified to the constructor along with any urls subsequently appended by the addurl method 
 used to periodically signal to the classloader to release jar resources 
 deregister any jdbc drivers registered by the webapp that the webapp forgot this is made unnecessary complex because a drivermanager checks the class loader of the calling class it would be much easier if it checked the context class loader b using reflection would create a dependency on the drivermanager implementation which can and has changed we can t just create an instance of jdbcleakprevention as it will be loaded by the common class loader since it s class file is in the catalina home lib directory this would fail drivermanager s check on the class loader of the calling class so we load the bytes via our parent class loader but define the class with this class loader so the jdbcleakprevention looks like a webapp class to the drivermanager if only apps cleaned up after themselves 
java lang classformaterror linkageerror error throwable object 
java lang nosuchfieldexception reflectiveoperationexception 
 analyzes the given thread local map object also pass in the field that points to the internal table to save re calculating it on every call to this method 
 this depends on the internals of the sun jvm so it does everything by reflection 
 clear the link resourcebundle cache of any bundles loaded by thisclass loader or any class loader where this loader is a parent class loader whilst link resourcebundle clearcache could be used thereare complications around the link org apache jasper servlet jasperloader that mean a reflectionbased approach is more likely to be complete the resourcebundle is using weakreferences so it shouldn t be pinning the class loader in memory however it is therefore clear ou the references 
 used to periodically signal to the classloader to release jar resources 
 find specified resource in local repositories 
java lang instrument illegalclassformatexception 
 check the specified jar file and return true if it does not contain any of the trigger classes 
 get url 
 get the uri for the given file 
 start associated link classloader and implement the requirementsof link org apache catalina util lifecyclebase startinternal 
 stop associated link classloader and implement the requirementsof link org apache catalina util lifecyclebase stopinternal 
 create associated classloader 
 configure the repositories for our class loader based on the associated context 
 copy directory 
 copy a file to the specified temp directory this is required only because jasper depends on it 
 process a get request for the specified resource 
 initialize this servlet 
 add a host using the specified parameters 
 remove the specified host 
 start the host with the specified name 
 stop the host with the specified name 
 process a get request for the specified resource 
 process a post request for the specified resource 
 render a html list of the currently active contexts in our virtual host and memory and server status information 
 process a get request for the specified resource 
 process a post request for the specified resource 
 render a html list of the currently active contexts in our virtual host and memory and server status information 
 extract the expiration request parameter
 param req
 param cn name of the application for which the sessions will be listed
 param req
 process a get request for the specified resource 
javax management operationsexception jmexception 
 sets an mbean attribute s value 
 invokes an operation on an mbean 
 set the wrapper with which we are associated 
 process a get request for the specified resource 
 process a put request for the specified resource 
 initialize this servlet 
 store server configuration 
 deploy a web application archive included in the current request at the specified context path 
 install an application for the specified path from the specified web application archive 
 install an application for the specified path from the specified web application archive 
 render a list of available global jndi resources 
 list the resources of the given context 
 extract the expiration request parameter
 start the web application at the specified context path 
 stop the web application at the specified context path 
 undeploy the web application at the specified context path 
 return a file object representing the application root directory for our associated host 
 invoke the isdeployed method on the deployer 
 invoke the isserviced method on the deployer 
 invoke the addserviced method on the deployer 
 invoke the removeserviced method on the deployer 
 upload the war file included in this request and store it at the specified file location 
 copy the specified file or directory to the destination 
 copy the specified file or directory to the destination 
 initialize this servlet 
 finalize this servlet 
 process a get request for the specified resource 
 write the os state mode  will generate html mode  will generate xml 
 write connector state 
 write processor state 
 write applications state 
 write context state 
 write detailed information about a manager 
 write jsp monitoring information 
 write detailed information about a wrapper 
 obtain and return the value of a specific attribute of this mbean 
javax management modelmbean invalidtargetobjecttypeexception 
 set the value of a specific attribute of this mbean 
 add a new child container to those associated with this container if supported won t start the child yet has to be started with a call to start method after necessary configurations are done 
 remove an existing child container from association with this parent container 
 adds a valve to this container instance 
 remove an existing valve 
 add a lifecycleevent listener to this component 
 remove a lifecycleevent listeners from this component 
 list the class name of each of the lifecycle listeners added to this container 
 list the class name of each of the container listeners added to this container 
 set the value of a specific attribute of this mbean 
 return the set of application parameters for this application 
 return the security constraints for this web application if there are none a zero length array is returned 
 return the error page entry for the specified http error code if any otherwise return null 
 return the error page entry for the specified java exception type if any otherwise return null 
 return the set of defined error pages for all specified error codes and exception types 
 return the filter definition for the specified filter name if any otherwise return null 
 return the set of defined filters for this context 
 return the set of filter mappings for this context 
 obtain and return the value of a specific attribute of this mbean 
 set the value of a specific attribute of this mbean 
 obtain and return the value of a specific attribute of this mbean 
 set the value of a specific attribute of this mbean 
 create the mbeans for the interesting global jndi resources 
 create the mbeans for the interesting global jndi resources in the specified naming context 
 create the mbeans for the specified userdatabase and its contents 
 return the mbean names of all authorized roles for this group 
 return the mbean names of all users that are members of this group 
java rmi remoteexception ioexception 
java rmi alreadyboundexception 
java rmi accessexception remoteexception ioexception 
 the following code to dump mbeans has been copied from jmxproxyservlet 
 get parent containerbase to add its child component from parent s objectname
 get parent containerbase to add its child component from child component s objectname as a string
 create a new accessloggervalve 
 create a new ajpconnector
 create a new datasource realm 
 create a new httpconnector
 create a new connector
 create a new httpsconnector
 create a new jdbc realm 
 create a new jndi realm 
 create a new memory realm 
 create a new remote address filter valve 
 create a new remote host filter valve 
 create a new single sign on valve 
 create a new standardcontext 
 create a new standardcontext 
 create a new standardhost 
 create a new standardmanager 
 create a new userdatabaserealm 
 create a new valve and associate it with a link container 
 create a new web application loader 
 remove an existing connector 
 remove an existing context 
 remove an existing host 
 remove an existing loader 
 remove an existing manager 
 remove an existing realm 
 remove an existing service 
 remove an existing valve 
 create register and return an mbean for this contextenvironment object 
 create register and return an mbean for this contextresource object 
 create register and return an mbean for this contextresourcelink object 
 create register and return an mbean for this group object 
 create register and return an mbean for this loader object 
 create register and return an mbean for this mbeanfactory object 
 create register and return an mbean for this namingresources object 
 create register and return an mbean for this role object 
 create register and return an mbean for this user object 
 create register and return an mbean for this userdatabase object 
 create an objectname for this connector object 
 create an objectname for this context object 
 create an objectname for this service object 
 create an objectname for this contextresource object 
 create an objectname for this contextresourcelink object 
 create an objectname for this engine object 
 create an objectname for this group object 
 create an objectname for this host object 
 create an objectname for this loader object 
 create an objectname for this manager object 
 create an objectname for this server object 
 create an objectname for this mbeanfactory object 
 create an objectname for this realm object 
 create an objectname for this role object 
 create an objectname for this server object 
 create an objectname for this service object 
 create an objectname for this user object 
 create an objectname for this userdatabase object 
 create an objectname for this valve object 
 deregister the mbean for this connector object 
 deregister the mbean for this context object 
 deregister the mbean for this contextenvironment object 
 deregister the mbean for this contextresource object 
 deregister the mbean for this contextresourcelink object 
 deregister the mbean for this engine object 
 deregister the mbean for this group object 
 deregister the mbean for this host object 
 deregister the mbean for this loader object 
 deregister the mbean for this manager object 
 deregister the mbean for this namingresources object 
 deregister the mbean for this realm object 
 deregister the mbean for this role object 
 deregister the mbean for this server object 
 deregister the mbean for this service object 
 deregister the mbean for this user object 
 deregister the mbean for this userdatabase object 
 deregister the mbean for the userdatabase object with this name 
 deregister the mbean for this valve object 
 create a new group and return the corresponding mbean name 
 create a new role and return the corresponding mbean name 
 create a new user and return the corresponding mbean name 
 return the mbean name for the specified group name if any otherwise return null 
 return the mbean name for the specified role name if any otherwise return null 
 return the mbean name for the specified user name if any otherwise return null 
 remove an existing group and destroy the corresponding mbean 
 remove an existing role and destroy the corresponding mbean 
 remove an existing user and destroy the corresponding mbean 
 return the mbean names of the set of defined environment entries for this web application
 return the mbean names of all the defined resource references for this application 
 return the mbean names of all the defined resource link references for this application 
 add an environment entry for this web application 
 add a resource reference for this web application 
 add a resource link reference for this web application 
 add a new connector to the set of defined connectors and associate it with this service s container 
 adds a named executor to the service
 find and return the set of connectors associated with this service 
 retrieves all executors
 retrieves executor by name
 return the mbean names of all groups this user is a member of 
 return the mbean names of all roles assigned to this user 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 gracefully terminate the active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 ensure child realms are destroyed when this realm is destroyed 
 inheritdoc 
 close the specified database connection 
 open the specified database connection 
 return the password associated with the given principal s user name 
 return the roles associated with the given user name
 return a preparedstatement configured to perform the select required to retrieve user credentials for the specified username 
 return a preparedstatement configured to perform the select required to retrieve user roles for the specified username 
 calls logout if necessary on any associated jaaslogincontext may in the future be extended to cover other logout requirements 
 phase  of authenticating a subject when phase  fails this method is called if the logincontext failed somewhere in the overall authentication chain 
 phase  of authenticating a subject 
javax security auth callback unsupportedcallbackexception 
 load the contents of our configuration file 
 parses a comma delimited list of class names and store the class names in the provided list each class must implement java security principal 
 perform the actual jaas authentication
 load custom jaas configuration
 return the principal associated with the specified username and credentials if there is one otherwise return null if there are any errors with the jdbc connection executing the query or anything we return null don t authenticate this event is also logged and the connection will be closed so that a subsequent request will automatically re open it 
 close the specified database connection 
 return a preparedstatement configured to perform the select required to retrieve user credentials for the specified username 
 return the password associated with the given principal s user name 
 return the roles associated with the gven user name 
 open if necessary and return a database connection for use by this realm 
 return a preparedstatement configured to perform the select required to retrieve user roles for the specified username 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 return the link hostnameverifier to use for peer certificateverification when opening connections using starttls 
 return the list of supported ssl protocols by the default link sslcontext 
 return the principal associated with the specified username and credentials if there is one otherwise return null if there are any errors with the jdbc connection executing the query or anything we return null don t authenticate this event is also logged and the connection will be closed so that a subsequent request will automatically re open it 
 return the principal associated with the specified username and credentials if there is one otherwise return null 
 return a user object containing information about the user with the specified username if found in the directory otherwise return null 
 return a user object containing information about the user with the specified username if found in the directory otherwise return null 
 return a user object containing information about the user with the specified username if found in the directory otherwise return null if the userpassword configuration attribute is specified the value of that attribute is retrieved from the user s directory entry if the userrolename configuration attribute is specified all values of that attribute are retrieved from the directory entry 
 use the distinguished name to locate the directory entry for the user with the specified username and return a user object otherwise return null 
 use the userpattern configuration attribute to locate the directory entry for the user with the specified username and return a user object otherwise return null 
 search the directory to return a user object containing information about the user with the specified username if found in the directory otherwise return null 
 check whether the given user can be authenticated with the given credentials if the userpassword configuration attribute is specified the credentials previously retrieved from the directory are compared explicitly with those presented by the user otherwise the presented credentials are checked by binding to the directory as the user 
 check credentials by binding to the directory as the user
 configure the context to use the provided credentials for authentication 
 configure the context to use link connectionname and link connectionpassword if specified or an anonymous connection ifthose attributes are not specified 
 return a list of roles associated with the given user any roles present in the user s directory entry are supplemented by a directory search if no roles are associated with this user a zero length list is returned 
 perform the search on the context as the code dn when code searchasuser is code true otherwise search the context with the default credentials 
 return a string representing the value of the specified attribute 
 add values of a specified attribute to a list
 close any open connection to the directory server for this realm 
 return the principal associated with the given user name 
 open if necessary and return a connection to the configured directory server for this realm 
java security keymanagementexception keyexception generalsecurityexception 
 create a tls enabled ldapcontext and set the starttlsresponse tls instance variable 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 returns the distinguished name of a search result 
javax naming invalidnameexception namingexception 
 inheritdoc 
 return a configured digester to use for processing the xml input file creating a new one if necessary 
org xml sax saxnotrecognizedexception saxexception 
org xml sax saxnotsupportedexception saxexception 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 return the principal associated with the specified username which matches the digest calculated using the given parameters using the method described in rfc  otherwise return null 
 return the principal associated with the specified chain of x client certificates if there is none return null 
java security cert certificateexpiredexception certificateexception generalsecurityexception 
java security cert certificatenotyetvalidexception certificateexception generalsecurityexception 
 inheritdoc 
 perform access control based on the specified authorization constraint return true if this constraint is satisfied and processing should continue or false otherwise 
 enforce any user data constraint required by the security constraint guarding this request uri return true if this constraint was not violated and processing should continue or false if we have created a response already 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 gracefully terminate the active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 digest the password using the specified algorithm and convert the result to a corresponding hexadecimal string if exception the plain credentials string is returned 
 return the digest associated with given principal s user name 
 digest password using the algorithm specified and convert the result to a corresponding hex string if exception the plain credentials string is returned
java lang clonenotsupportedexception 
 prepare for the beginning of active use of the public methods of this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 perform work as a particular subject here the work will be granted to a null subject 
 create the method and cache it for further re use 
 sets instance variables modified from craig r mcclanahan s invokerservlet 
 logs important servlet api and container information copied from snoopallservlet by craig r mcclanahan 
 provides cgi gateway service delegates to link doget httpservletrequest httpservletresponse 
 provides cgi gateway service 
 behaviour depends on the status code status    calls senderror status returns true standard error page mechanism will provide the response body 
 creates a cgienvironment and derives the necessary environment query parameters working directory cgi command etc 
 uses the httpservletrequest to set most cgi variables
 constructs the cgi environment to be supplied to the invoked cgi script relies heavily on servlet api methods and findcgi
 extracts requested resource from web app archive to context work directory to enable cgi script to be executed 
 executes a cgi script with the desired environment current working directory and input output streams this implements the following cgi specification recommedations servers should provide the query component of the script uri as command line arguments to scripts if it does not contain any unencoded   characters and the command line arguments can be generated in an unambiguous manner servers should set the auth type metavariable to the value of the auth scheme token of the authorization if it was supplied as part of the request header see getcgienvironment method where applicable servers should set the current working directory to the directory in which the script is located before invoking it server implementations should define their behavior for the following cases allowed characters in pathinfo this implementation does not allow ascii nul nor any character which cannot be url encoded according to internet standards allowed characters in path segments this implementation does not allow non terminal null segments in the the path ioexceptions may be thrown and path segments this implementation does not allow and in the the path and such characters will result in an ioexception being thrown this should never happen since tomcat normalises the requesturi before determining the contextpath servletpath and pathinfo implementation limitations this implementation does not impose any limitations except as documented above this implementation may be limited by the servlet container used to house this implementation in particular all the primary cgi variable values are derived either directly or indirectly from the container s implementation of the servlet api methods 
 parses the status line and extracts the status code 
 parses the cgi status header value and extracts the status code 
 see java io inputstream read 
 initialize this servlet 
 process a get request for the specified resource 
 process a head request for the specified resource 
 process a post request for the specified resource 
 process a put request for the specified resource 
 handle a partial put new content specified in request is appended to existing content in oldrevisioncontent if present this code does not support simultaneous partial updates to the same resource 
 process a delete request for the specified resource 
 check if the conditions specified in the optional if headers are satisfied 
 serve the specified resource optionally including the data content 
 parse the content range header 
 parse the range header 
 decide which way to render html or xml 
 return an inputstream to an html representation of the contents of this directory 
javax xml transform transformerfactoryconfigurationerror error throwable object 
javax xml transform transformerconfigurationexception transformerexception 
javax xml transform transformerexception 
 return an inputstream to an html representation of the contents of this directory 
 get the readme file as a string 
 return a source for the xsl template if possible 
 check if the if match condition is satisfied 
 check if the if none match condition is satisfied 
 check if the if unmodified since condition is satisfied 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 copy the contents of the specified input stream to the specified output stream and ensure that both streams are closed before returning even in the face of an exception 
 initialize this servlet 
 return jaxp document builder instance 
 handles the special webdav methods 
 propfind method 
 proppatch method 
 mkcol method 
 delete method 
 process a put request for the specified resource 
 copy method 
 move method 
 lock method 
 unlock method 
 copy a resource 
 copy a collection 
 delete a resource 
 delete a resource 
 deletes a collection 
 send a multistatus element containing a complete error report to the client 
 determines the methods normally allowed for the resource 
 return the number of sessions present in this store 
 remove all of the sessions in this store 
 return an array containing the session identifiers of all sessions currently saved in this store if there are no such sessions a zero length array is returned 
 load and return the session associated with the specified session identifier from this store without removing it if there is no such stored session return null 
 remove the session with the specified session identifier from this store if present if no such session is present this method takes no action 
 save the specified session into this store any previously saved information for the associated session identifier is replaced 
 return a file object representing the pathname to our session persistence file if any 
 return an array containing the session identifiers of all sessions currently saved in this store if there are no such sessions a zero length array is returned 
 return an integer containing a count of all sessions currently saved in this store if there are no sessions  is returned 
 load the session associated with the id id if no such session is found null is returned 
 remove the session with the specified session identifier from this store if present if no such session is present this method takes no action 
 remove the session with the specified session identifier from this store if present if no such session is present this method takes no action 
 remove all of the sessions in this store 
 save a session to the store 
 check the connection associated with this store if it s null or closed try to reopen it returns null if the connection could not be established 
 open if necessary and return a database connection for use by this store 
 close the specified database connection 
 clear all sessions from the store 
 inheritdoc this method checks the persistence store if persistence is enabled otherwise just uses the functionality from managerbase 
 load all sessions found in the persistence mechanism assuming they are marked as valid and have not passed their expiration limit if persistence is not supported this method returns without doing anything note that by default this method is not called by the middlemanager class in order to use it a subclass must specifically call it for example in the start and or processpersistencechecks methods 
 remove this session from the active sessions for this manager and from the store 
 look for a session in the store and if found restore it in the manager s list of active sessions if appropriate the session will be removed from the store after swapping in but will not be added to the active session list if it is invalid or past its expiration 
 remove the session from the manager s list of active sessions and write it out to the store if the session is past its expiration or invalid this method does nothing 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 load any currently active sessions that were previously unloaded to the appropriate persistence mechanism if any if persistence is not supported this method returns without doing anything 
 save any currently active sessions in the appropriate persistence mechanism if any if persistence is not supported this method returns without doing anything 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 perform the internal processing required to invalidate this session without triggering an exception if the session has already expired 
 read a serialized version of the contents of this session object from the specified object input stream without requiring that the standardsession itself have been serialized 
 write a serialized version of the contents of this session object to the specified object output stream without requiring that the standardsession itself have been serialized 
 read a serialized version of this session object from the specified object input stream implementation note the reference to the owning manager is not restored by this method and must be set explicitly 
 write a serialized version of this session object to the specified object output stream implementation note the owning manager will not be stored in the serialized representation of this session after calling readobject you must set the associated manager explicitly implementation note any attribute that is not serializable will be unbound from the session with appropriate actions if it implements httpsessionbindinglistener if you do not want any such attributes be sure the distributable property of the associated manager is set to true 
 get only those keys of sessions that are saved in the store and are to be expired 
 called by our background reaper thread to check if sessions saved in our store are subject of being expired if so expire the session and remove it from the store 
 create the object input stream to use to read a session from the store sub classes must have set the thread context class loader before calling this method 
 start this component and implement the requirements of link lifecyclebase startinternal 
 stop this component and implement the requirements of link lifecyclebase stopinternal 
 creates a new parse tree for the specified expression 
 flush the servletoutputstream or printwriter only one will be non null this must be called after a requestdispatcher include since we can t assume that the included servlet flushed its stream 
 return a printwriter throws and exception if a outputstream already been returned 
 see ssicommand
org apache catalina ssi ssistopprocessingexception 
 retrieves the expression from the specified arguments and peforms the necessary evaluation steps 
 see ssicommand
 initialize this servlet 
 see ssicommand
 see ssicommand
 see ssicommand
 process a file with server side commands reading from reader and writing the processed version to writer note we really should be doing this in a streaming way rather than converting it to an array first 
 initialize this servlet 
 process and forward the get request to our requesthandler 
 process and forward the post request to our requesthandler 
 process our request and locate right ssi command 
 initialize daemon 
 load daemon 
 getserver for configtest
 load the catalina daemon 
 start the catalina daemon 
 stop the catalina daemon 
 stop the standalone server 
 stop the standalone server 
 set flag 
 main method and entry point when starting tomcat via the provided scripts 
 set the catalina home system property to the current working directory if it has not been set 
 cluster support is optional the jars may have been removed 
 start a new server instance 爙 conf server xml
 start a new server instance 
 stop an existing server instance 
 load properties 
 create and return a new class loader based on the configuration defaults and the specified directory paths 
 create and return a new class loader based on the configuration defaults and the specified directory paths 
 process the beginning of this element 
 set up an authenticator automatically if required and one has not already been configured 
 process the default configuration file if it exists 
 process a context xml 
 adjust docbase 
 process a before start event for this context 
 scan the web xml files that apply to the web application and merge them using the rules defined in the spec for the global web xml files where there is duplicate configuration the most specific level wins ie an application s web xml takes precedence over the host level or global web xml file 
 scan jars for servletcontainerinitializer implementations 
 scan jars that contain web fragment xml files that will be used to configure this application to see if they also contain static resources if static resources are found add them to the context resources are added in web fragment xml priority order 
 identify the application web xml to be used and obtain an input source for it 
 param filename name of the file possibly with one or more leading pathsegments to read
 parses the given source and stores the parsed data in the given web xml representation the byte stream will be closed at the end of the parse operation 
org apache tomcat util bcel classfile classformatexception runtimeexception 
 handle the beginning of an xml element 
 add a new engine to the set of defined engines 
 remove the specified engine from the set of defined engines along with all of its related hosts and contexts all associated connectors are also removed 
 start nested components link connector s and link engine s and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop nested components link connector s and link engine s and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 expand the war file found at the specified url into an unpacked directory structure and return the absolute pathname to the expanded directory 
 validate the war file found at the specified url 
 copy the specified file or directory to the destination 
 expand the specified input stream into the specified file 
 deploy xml context descriptors 
 param cn
 deploy war files 
 param cn
 deploy directories 
 param cn
 process a start event for this host 
 handle the beginning of an xml element 
 initialize our set of users and home directories 
 process the end of this element 
 get the taglib entries from web xml and add them to the map this is not kept in sync with o a j compiler tldlocationscache as this code needs to scan the tlds listed in web xml whereas jasper only needs the uri to tld mappings 
 initialise the server 
 start the server 
 stop the server 
 destroy the server this object cannot be used once this method has been called 
 create the configured link context for the given host the default constructor of the class that was configured with link standardhost setcontextclass string will be used
 the main program for the bootstrap 
 deploy a web application for any user who has a web application present in a directory with a specified name within their home directory 
 deploy a web application for the specified user if they have such an application in the defined directory within their home directory 
 load the providers for a service type 
 process the end of this element 
 see java io externalizable readexternal
 see java io externalizable writeexternal
 send a message to one or more members in the cluster
 starts up the channel this can be called multiple times for individual services to start the svc parameter can be the logical or value of any constants
 shuts down the channel this can be called multiple times for individual services to shutdown the svc parameter can be the logical or value of any constants
 starts up the channel this can be called multiple times for individual services to start the svc parameter can be the logical or value of any constants
 starts up the channel this can be called multiple times for individual services to start the svc parameter can be the logical or value of any constants
 shuts down the channel this can be called multiple times for individual services to shutdown the svc parameter can be the logical or value of any constants
 send a message to the destinations specified
 param destination member destination length  
 callback from the interceptor stack when a message is received from a remote node this method will be invoked by the previous interceptor this method can also be used to send a message to other components within the same application but its an extreme case and you re probably better off doing that logic between the applications itself 
 sends a norpcchannelreply message to a member this method gets invoked by the channel if a rpc message comes in and no channel listener accepts the message this avoids timeout
 sets up the default implementation interceptor stack if no interceptors have been added
 starts the channel
 param data data to decompress
 send a message and wait for the response 
 creates an objectreader for a tcp socket
 send buffer to cluster listener callback is message complete receiver send message to callback 
 load the local class equivalent of the specified stream class description by using the class loader assigned to this context 
 objectinputstream resolveproxyclass has some funky way of using the incorrect class loader to resolve proxy classes let s do it our way instead
 serializes a message into cluster data
 sets the local member properties for broadcasting
 start broadcasting and listening to membership pings
 stop broadcasting and listening to membership pings
 simple test program
 create a new mcast service impl
 start the service
 stops the service
 receive a datagram packet locking wait
 send a ping
 construct a new member object
 param host string either in byte array string format like     or as a regular hostname     or tomcat mydomain com
 initializes the map by creating the rpc channel registering itself as a channel listener this method is also responsible for initiating the state transfer
 sends a ping out to all the members in the cluster not just map members that this map is alive 
 helper method to broadcast a message to all members in a channel
 replicates any changes to the object since the last time the object has to be primary ie if the object is a proxy or a backup it will not be replicated
 if the reply has already been sent to the requesting thread the rpc callback can handle any data that comes in after the fact 
 apply a diff or an entire object
 deprecated unused will be removed in   x
 deprecated unused will be removed in   x
 publish info about a map pair key value to other nodes in the cluster
 the actual code which drains the channel associated with the given key this method assumes the key has been modified prior to invocation to turn off selection interest in op read when this method completes it re enables op read and calls wakeup on the selector so the selector will resume watching this channel 
 send a reply acknowledgment    
 connect other cluster member receiver 
 send message 
 push messages with only one socket at a time wait for ack is needed and make auto retry when write message is failed after sending error close and reopen socket again after successful sending update stats warning subclasses must be very careful that only one thread call this pushmessage at once 
 acquires the lock by an add thread and sets the add flag if any add thread or the remove thread already acquired the lock this add thread will block until the lock is released 
 acquires the lock by the remove thread and sets the remove flag if any add thread already acquired the lock or the queue is empty the remove thread will block until the lock is released and the queue is not empty 
 start cluster receiver
java nio channels closedchannelexception ioexception 
 get data from channel and store in byte array send it to cluster
 close selector 
 register the given channel with the given selector for the given operations of interest
 start thread and listen
 the actual code which drains the channel associated with the given key this method assumes the key has been modified prior to invocation to turn off selection interest in op read when this method completes it re enables op read and calls wakeup on the selector so the selector will resume watching this channel 
 send a reply acknowledgement    sends it doing a busy write the ack is so small that it should always go to the buffer
 state machine to send data
 connect blocking in this operation
 disconnect todo implement this org apache catalina tribes transport idatasender method
 sendmessage
 return returns the bind 
 same as bind except it does it for the udp port
 send data to one member
 start the sender and register transmitter mbean
 find an idle worker thread if any could return null 
 finalize access to this user database 
 initialize access to this user database 
 save any updated information to the persistent storage location for this user database 
 create and return a new memoryuserdatabase instance that has been configured according to the properties of the specified reference if you instance can be created return null instead 
 construct a new charsetmapper using the specified properties resource 
 load the local class equivalent of the specified stream class description by using the class loader assigned to this context 
 return a proxy class that implements the interfaces named in a proxy class descriptor do this using the class loader assigned to this context 
 deprecated unused will be removed in   x
 return true if the first version number is greater than or equal to the second otherwise return false 
 runtime validation of a web application this method uses jndi to look up the resources located under a dircontext it locates web application manifest mf file in the meta inf directory of the application and all manifest mf files in each jar file located in the web inf lib directory and creates an arraylist of manifestresorce objects these objects are then passed to the validatemanifestresources method for validation 
 checks to see if the given system jar file contains a manifest and adds it to the container s manifest resources 
 return the manifest from a jar file or war file
 add the jars specified to the extension list 
 attempt to load a class using the given container s class loader if the class cannot be loaded a debug level log message will be written to the container s log and null will be returned 
 read input from reader and write it to writer until there is no more input from reader 
 see flow reader writer char 
 read input from input stream and write it to output stream until there is no more input from input stream 
 see flow java io inputstream java io outputstream byte 
 inheritdoc 
 inheritdoc 
 provides a mechanism for sub classes to update the component state calling this method will automatically fire any associated link lifecycle event it will also check that any attempted statetransition is valid for a sub class 
 provides a mechanism for sub classes to update the component state calling this method will automatically fire any associated link lifecycle event it will also check that any attempted statetransition is valid for a sub class 
 utility method to enable sub classes to easily register additional components that don t implement link mbeanregistration withan mbean server note this method should only be used once link initinternal hasbeen called and before link destroyinternal has been called 
 utility method to enable sub classes to easily unregister additional components that don t implement link mbeanregistration withan mbean server note this method should only be used once link initinternal hasbeen called and before link destroyinternal has been called 
 append request parameters from the specified string to the specified map it is presumed that the specified map is not accessed from any other thread so no synchronization is performed implementation note url decoding is performed individually on the parsed name and value elements rather than on the entire query string ahead of time to properly deal with the case where the name or value includes an encoded   or character that would otherwise be interpreted as a delimiter 
 decode and return the specified url encoded string 
 decode and return the specified url encoded byte array 
 append request parameters from the specified string to the specified map it is presumed that the specified map is not accessed from any other thread so no synchronization is performed implementation note url decoding is performed individually on the parsed name and value elements rather than on the entire query string ahead of time to properly deal with the case where the name or value includes an encoded   or character that would otherwise be interpreted as a delimiter note byte array data is modified by this method caller beware 
 create a new random number generator instance we should use for generating session identifiers 
java security nosuchproviderexception generalsecurityexception 
 url encodes the provided path using the given encoding 
 send data and reinitializes buffer 
 log a message summarizing the specified request and response according to the format specified by the pattern property 
 open the new log file for the date specified by datestamp 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 register requests for tracking whenever needed 
 use events to update the connection state 
 invoke the next valve in the sequence when the invoke returns check the response state if the status code is greater than or equal to  or an uncaught exception was thrown then the error handling will be triggered 
 urlencode the given string if null or empty return null 
 this method is invoked by tomcat on each query 
 open if necessary and return a database connection for use by this accesslogvalve 
 close the specified database connection 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 select the appropriate child context to process this request based on the specified request uri if no matching context can be found return an appropriate http error 
 inheritdoc 
 perform the filtering that has been configured for this valve matching against the specified request property 
 reject the request that was denied by this valve if invalidauthenticationwhendeny is true and the context has preemptiveauthentication set set an invalid authorization header to trigger basic auth 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 do concurrency control on the request using the semaphore 
java security cert certificateexception generalsecurityexception 
 inheritdoc 
 process a comet event this method will rarely need to be provided by a subclass unless it needs to reassociate a particular object with the thread that is processing the request 
 start this component and implement the requirements of link org apache catalina util lifecyclebase startinternal 
 stop this component and implement the requirements of link org apache catalina util lifecyclebase stopinternal 
 create the new websocket frame reading data from the processor as necessary 
 read the next websocket frame reading data from the processor as necessary 
 process the next websocket frame 
 adds the data to the buffer for binary data if a textual message is currently in progress that message will be completed and a new binary message started if the buffer for binary data is full the buffer will be flushed and a new binary continuation fragment started 
 adds the data to the buffer for textual data if a binary message is currently in progress that message will be completed and a new textual message started if the buffer for textual data is full the buffer will be flushed and a new textual continuation fragment started 
 flush any message binary or textual that may be buffered and then send a websocket binary message as a single frame with the provided buffer as the payload of the message 
 flush any message binary or textual that may be buffered and then send a websocket text message as a single frame with the provided buffer as the payload of the message 
 flush any message binary or textual that may be buffered 
 respond to a client close by sending a close that echoes the status code and message 
 send a close message to the client
 send a pong message to the client
 send a ping message to the client
 generic function to send either a ping or a pong 
 writes the provided bytes as the payload in a new websocket frame 
java nio charset charactercodingexception ioexception 
 send an action to the connector 
 get more request body data from the web server and store it in the internal buffer 
 parse host 
 when committing the response we have to validate the set of headers as well as setup the response filters 
 callback to write data from the buffer 
 finish ajp response 
 read bytes into the specified chunk 
 write chunk 
 process pipelined http requests using the specified input and output streams 
 receive a chunk of data called to implement the special packet in ajp and to receive the data after we send a get body packet
 read an ajp message 
 process pipelined http requests using the specified input and output streams 
 read the specified amount of bytes and place them in the input buffer 
 receive a chunk of data called to implement the special packet in ajp and to receive the data after we send a get body packet
 read an ajp message 
 process pipelined http requests using the specified input and output streams 
 read at least the specified amount of bytes and place them in the input buffer 
 receive a chunk of data called to implement the special packet in ajp and to receive the data after we send a get body packet
 read an ajp message 
 set compression level 
 send an action to the connector 
 process pipelined http requests using the specified input and output streams 
 end request consumes leftover bytes 
 read some bytes 
 write the contents of a byte chunk 
 end request 
 reads the request body and buffers it 
 read bytes 
 end the current request 
 read bytes from the previous buffer 
 parse the header of a chunk a chunk header can look like one of the following acrlf f chunk extension to be ignoredcrlf the letters before crlf or whatever comes first must be valid hex digits we should not parse fiamgonnamessthisupcrlf as a valid header according to the spec 
 parse crlf at end of chunk 
 parse crlf at end of chunk 
 parse end chunk data 
 write some bytes 
 end the current request it is acceptable to write extra bytes using buffer dowrite during the execution of this method 
 write some bytes 
 added to allow flushing to happen for the gzip ed outputstream
 end the current request it is acceptable to write extra bytes using buffer dowrite during the execution of this method 
 read bytes 
 write some bytes 
 read bytes 
 process pipelined http requests using the specified input and output streams 
 send an action to the connector 
 process pipelined http requests using the specified input and output streams 
 send an action to the connector 
 send an action to the connector 
 deprecated will be removed in tomcat   x 
 read the request line this function is meant to be used during the http request header parsing do not attempt to read the request body using it 
 parse the http headers 
 parse an http header 
 read bytes into the specified chunk 
 flush the response 
 end request 
 write chunk 
 read the request line this function is meant to be used during the http request header parsing do not attempt to read the request body using it 
 parse the http headers 
 parse an http header 
 fill the internal buffer using data from the underlying input stream 
 read bytes into the specified chunk 
 read the request line this function is meant to be used during the http request header parsing do not attempt to read the request body using it 
java io eofexception ioexception 
 perform blocking read with a timeout if desired
 parse the http headers 
 parse an http header 
 fill the internal buffer using data from the underlying input stream 
 flush the response 
 end request 
 send an acknowledgment 
 param bytebuffer bytebuffer
 commit the response 
 callback to write data from the buffer 
 flush the response 
 end request 
 send an acknowledgment 
 commit the response 
 callback to write data from the buffer 
 write chunk 
 new servlet   method 
 must hold writelock to call this method 
 read data from the input buffer and put it into a byte chunk the buffer is owned by the protocol implementation it will be reused on the next read the adapter must either process the data in place or copy it to a separate buffer if it needs to hold it in most cases this is done during byte char conversions or via inputstream unlike inputstream this interface allows the app to process data in place without copy 
 set internal fields for special header names called from set addheader return true if the header is special no need to set the header 
 write a chunk of bytes 
 compare two objects after coercing to the same type if appropriate if the objects are identical or they are equal according to link equals object object then return  if either object is a bigdecimal then coerce both to bigdecimal first similarly for double float biginteger and long integer char short byte otherwise check that the first object is an instance of comparable and compare against the second object if that is null return  otherwise return the result of comparing against the second object similarly if the second object is comparable if the first is null return  else return the result of comparing against the first object a null object is considered as zero when compared with numbers the empty string for string compares otherwise null is considered to be lower than anything else 
 compare two objects for equality after coercing to the same type if appropriate if the objects are identical including both null return true if either object is null return false if either object is boolean coerce both to boolean and check equality similarly for enum string bigdecimal double float long integer short byte character otherwise default to using object equals 
org apache el parser parseexception 
 evaluates the expression relative to the provided context and returns information about the actual referenced method 
 evaluates the expression relative to the provided context invokes the method that was found using the supplied parameters and returns the result of the method invocation 
javax el methodnotfoundexception elexception runtimeexception 
java lang negativearraysizeexception runtimeexception 
 constructor with inputstream and supplied encoding 
 reinitialise 
 get the next token 
 start 
 read a character 
 constructor 
 reinitialise 
 reinitialise 
 reinitialise 
 converts an array of class names to class types
 returns a method based on the criteria 
javax servlet jsp el elexception 
 create an embeddedservletoptions object using data available from servletconfig and servletcontext 
org apache jasper jasperexception servletexception 
 apply command line arguments 
 base dir for the webapp used to generate class names and resolve includes 
 adds servlet declaration and mapping for the jsp page servlet to the generated web xml fragment 
 include the generated web xml inside the webapp s web xml 
 executes the compilation 
 initializes the classloader as if needed for the given compilation context 
 deprecated will be removed in tomcat   x use link getlastmodified string instead 
 write a single character 
 write a portion of an array of characters ordinarily this method stores characters from the given array into this stream s buffer flushing the buffer to the underlying stream as needed if the requested length is at least as large as the buffer however then this method will flush the buffer and write the characters directly to the underlying stream thus redundant discardablebufferedwriters will not copy data unnecessarily 
 write an array of characters this method cannot be inherited from the writer class because it must suppress io exceptions 
 write a portion of a string 
 write a string this method cannot be inherited from the writer class because it must suppress io exceptions 
 write a line separator the line separator string is defined by the system property line separator and is not necessarily a single newline n character 
 print a boolean value the string produced by link java lang string valueof boolean is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a character the character is translated into one or more bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an integer the string produced by link java lang string valueof int is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a long integer the string produced by link java lang string valueof long is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a floating point number the string produced by link java lang string valueof float is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a double precision floating point number the string produced by link java lang string valueof double is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an array of characters the characters are converted into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a string if the argument is null then the string null is printed otherwise the string s characters are converted into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an object the string produced by the link java lang string valueof object method is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 terminate the current line by writing the line separator string the line separator string is defined by the system property line separator and is not necessarily a single newline character n 
 print a boolean value and then terminate the line this method behaves as though it invokes link print boolean and then link println 
 print a character and then terminate the line this method behaves as though it invokes link print char and then link println 
 print an integer and then terminate the line this method behaves as though it invokes link print int and then link println 
 print a long integer and then terminate the line this method behaves as though it invokes link print long and then link println 
 print a floating point number and then terminate the line this method behaves as though it invokes link print float and then link println 
 print a double precision floating point number and then terminate the line this method behaves as though it invokes link print double and then link println 
 print an array of characters and then terminate the line this method behaves as though it invokes link print char and then link println 
 print a string and then terminate the line this method behaves as though it invokes link print string and then link println 
 print an object and then terminate the line this method behaves as though it invokes link print object and then link println 
 clears the current contents of the buffer unlike clear this method will not throw an ioexception if the buffer has already been flushed it merely clears the current content of the buffer and returns 
 close the stream flushing it first once a stream has been closed further write or flush invocations will cause an ioexception to be thrown closing a previously closed stream however has no effect 
 write the contents of this bodyjspwriter into a writer subclasses are likely to do interesting things with the implementation so some things are extra efficient 
 entry point into service 
 variableresolver interface
 create a typed array this is a special case where params are passed through the request and the property is indexed 
 use proprietaryevaluate public static void handlesetpropertyexpression object bean string prop string expression pagecontext pagecontext variableresolver variableresolver functionmapper functionmapper throws jasperexception try method method   getwritemethod bean getclass prop method invoke bean new object pagecontext getexpressionevaluator evaluate expression method getparametertypes  variableresolver functionmapper null catch exception ex throw new jasperexception ex 
 perform a requestdispatcher include operation with optional flushing of the response beforehand 
 url encodes a string based on the supplied character encoding this performs the same function as java next urlencode encode in jsdk  and should be removed if the only platform supported is   or higher 
 flush the output buffer to the underlying character stream without flushing the stream itself this method is non private only so that it may be invoked by printstream 
 discard the output buffer 
 flush the stream 
 close the stream 
 write a single character 
 write a portion of an array of characters ordinarily this method stores characters from the given array into this stream s buffer flushing the buffer to the underlying stream as needed if the requested length is at least as large as the buffer however then this method will flush the buffer and write the characters directly to the underlying stream thus redundant discardablebufferedwriters will not copy data unnecessarily 
 write an array of characters this method cannot be inherited from the writer class because it must suppress io exceptions 
 write a portion of a string 
 write a line separator the line separator string is defined by the system property line separator and is not necessarily a single newline n character 
 print a boolean value the string produced by link java lang string valueof boolean is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a character the character is translated into one or more bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an integer the string produced by link java lang string valueof int is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a long integer the string produced by link java lang string valueof long is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a floating point number the string produced by link java lang string valueof float is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a double precision floating point number the string produced by link java lang string valueof double is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an array of characters the characters are converted into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print a string if the argument is null then the string null is printed otherwise the string s characters are converted into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 print an object the string produced by the link java lang string valueof object method is translated into bytes according to the platform s default character encoding and these bytes are written in exactly the manner of the link write int method 
 terminate the current line by writing the line separator string the line separator string is defined by the system property line separator and is not necessarily a single newline character n need to change this from printwriter because the default println writes to the sink directly instead of through the write method 
 print a boolean value and then terminate the line this method behaves as though it invokes link print boolean and then link println 
 print a character and then terminate the line this method behaves as though it invokes link print char and then link println 
 print an integer and then terminate the line this method behaves as though it invokes link print int and then link println 
 print a long integer and then terminate the line this method behaves as though it invokes link print long and then link println 
 print a floating point number and then terminate the line this method behaves as though it invokes link print float and then link println 
 print a double precision floating point number and then terminate the line this method behaves as though it invokes link print double and then link println 
 print an array of characters and then terminate the line this method behaves as though it invokes link print char and then link println 
 print a string and then terminate the line this method behaves as though it invokes link print string and then link println 
 print an object and then terminate the line this method behaves as though it invokes link print object and then link println 
 proprietary method to evaluate el expressions xxx this method should go away once the el interpreter moves out of jstl and into its own project for now this is necessary because the standard machinery is too slow 
 gets the next available tag handler from this tag handler pool instantiating one if this tag handler pool is empty 
 stores a mapping from the given el function prefix and name to the given java method 
 creates an instance for this class and stores the method for the given el function prefix and name this method is used for the case when there is only one function in the el expression 
 gets the next available tag handler from this tag handler pool instantiating one if this tag handler pool is empty 
 load the class with the specified name this method searches for classes in the same manner as loadclass string boolean with false as the second argument 
 load the class with the specified name searching using the following algorithm until it finds and returns the class if the class cannot be found returns classnotfoundexception call findloadedclass string to check if the class has already been loaded if it has the same class object is returned if the delegate property is set to true call the loadclass method of the parent class loader if any call findclass to find this class in our locally defined repositories call the loadclass method of our parent class loader if any if the class was found using the above steps and the resolve flag is true this method will then call resolveclass class on the resulting class object 
 return the real path for the specified context relative virtual path 
 return a url object of a resource that is mapped to the specified context relative path 
 return an inputstream allowing access to the resource at the specified context relative path 
 compile if needed and load a tag file
 compile and load a prototype for the tag file this is needed when compiling tag files with circular dependencies a prototype skeleton with no dependencies on other other tag files is generated and compiled 
 get a list of files that the current page has source dependency on 
 attempts to construct a jasperexception that contains helpful information about what went wrong uses the jsp compiler system to translate the line number in the generated servlet that originated the exception to a line number in the jsp then constructs an exception containing that information and a snippet of the jsp to help debugging please see http bz apache org bugzilla show bug cgi id  and http www tfenne com jasper for more details 
 read a single character this method will block until a character is available an io error occurs or the end of the stream is reached subclasses that intend to support efficient single character input should override this method 
 read characters into a portion of an array this method will block until some input is available an io error occurs or the end of the stream is reached 
 skip characters this method will block until some characters are available an io error occurs or the end of the stream is reached 
 reset the stream if the stream has been marked then attempt to reposition it at the mark if the stream has not been marked then attempt to reset it in some way appropriate to the particular stream for example by repositioning it to its starting point not all character input streams support the reset operation and some support reset without supporting mark 
 close the stream once a stream has been closed further read ready mark or reset invocations will throw an ioexception closing a previously closed stream however has no effect 
 parse the specified xml document and return a treenode that corresponds to the root node of the document tree 
 parse the specified xml document and return a treenode that corresponds to the root node of the document tree 
 read a single character this method will block until a character is available an io error occurs or the end of the stream is reached subclasses that intend to support efficient single character input should override this method 
 read characters into a portion of an array this method will block until some input is available an io error occurs or the end of the stream is reached 
 skip characters this method will block until some characters are available an io error occurs or the end of the stream is reached 
 read a single character this method will block until a character is available an io error occurs or the end of the stream is reached subclasses that intend to support efficient single character input should override this method 
java io utfdataformatexception ioexception 
 read characters into a portion of an array this method will block until some input is available an io error occurs or the end of the stream is reached 
 skip characters this method will block until some characters are available an io error occurs or the end of the stream is reached 
 autodetects the encoding of the xml document supplied by the given input stream encoding autodetection is done according to the xml   specification appendix f  detection without external encoding information 
 creates a reader capable of reading the given input stream in the specified encoding 
 returns the next character on the input note the character is not consumed 
 returns the next character on the input note the character is consumed 
 returns a string matching the name production appearing immediately on the input as a symbol or null if no name string is present note the name characters are consumed note the string returned must be a symbol the symboltable can be used for this purpose 
 scans a range of attribute value data setting the fields of the xmlstring structure appropriately note the characters are consumed note this method does not guarantee to return the longest run of attribute value data this method may return before the quote character due to reaching the end of the input buffer or any other reason note the fields contained in the xmlstring structure are not guaranteed to remain valid upon subsequent calls to the entity scanner therefore the caller is responsible for immediately using the returned character data or making a copy of the character data 
 scans a range of character data up to the specified delimiter setting the fields of the xmlstring structure appropriately note the characters are consumed note this assumes that the internal buffer is at least the same size or bigger than the length of the delimiter and that the delimiter contains at least one character note this method does not guarantee to return the longest run of character data this method may return before the delimiter due to reaching the end of the input buffer or any other reason note the fields contained in the xmlstring structure are not guaranteed to remain valid upon subsequent calls to the entity scanner therefore the caller is responsible for immediately using the returned character data or making a copy of the character data 
 skips a character appearing immediately on the input note the character is consumed only if it matches the specified character 
 skips space characters appearing immediately on the input note the characters are consumed only if they are space characters 
 skips the specified string appearing immediately on the input note the characters are consumed only if they are space characters 
 loads a chunk of text 
 scans an xml or text declaration  xmldecl   lt xml versioninfo encodingdecl sddecl s   versioninfo   s version eq versionnum  versionnum  encodingdecl   s encoding eq encname  encname  encname   a za z a za z    sddecl   s standalone eq yes  no  yes  no  textdecl   lt xml versioninfo encodingdecl s  
 scans an xml or text declaration  xmldecl    versioninfo   s version eq versionnum  versionnum  encodingdecl   s encoding eq encname  encname  encname   a za z a za z    sddecl   s standalone eq yes  no  yes  no  textdecl   
 scans a pseudo attribute 
 scans a processing data this is needed to handle the situation where a document starts with a processing instruction whose target name starts with xml e g xmlfoo note this method uses fstringbuffer anything in it at the time of calling is lost 
 scans surrogates and append them to the specified buffer note this assumes the current char has already been identified as a high surrogate 
 add the specified logger to the classloader local configuration 
 read configuration for the specified classloader 
 load specified configuration 
 configure from logmanager properties 
 convenience method to derive a name from the specified class and call getinstance string with it 
org apache juli logging logconfigurationexception runtimeexception 
 convenience method to return a named logger without the application having to care about factories 
 convenience method to return a named logger without the application having to care about factories 
 binds a naming context to a thread 
 binds a naming context to a class loader 
 binds a naming context to a thread 
 create a new bean instance 
 create a new datasource instance 
 create a new ejb instance 
 create a new ejb instance using openejb 
 create a new resource env instance 
 crete a new datasource instance 
 create a new datasource instance 
 create a new user transaction instance 
 constructs a new serviceproxy wrapping given service instance 
 see invocationhandler invoke object method object 
org apache naming factory webservices serviceexception object 
 crete a new serviceref instance 
 get a new writable initial context 
 parses a name into its components 
 builds a naming context 
 retrieves the named object if name is empty returns a new instance of this context which represents the same naming context as this context but its environment may be modified independently and it may be accessed concurrently 
 retrieves the named object 
 binds a name to an object all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist 
 binds a name to an object 
 binds a name to an object overwriting any existing binding all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist if the object is a dircontext any existing attributes associated with the name are replaced with those of the object otherwise any existing attributes associated with the name remain unchanged 
 binds a name to an object overwriting any existing binding 
 unbinds the named object removes the terminal atomic name in name from the target context that named by all but the terminal atomic part of name this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist 
 unbinds the named object 
 binds a new name to the object bound to an old name and unbinds the old name both names are relative to this context any attributes associated with the old name become associated with the new name intermediate contexts of the old name are not changed 
 binds a new name to the object bound to an old name and unbinds the old name 
 enumerates the names bound in the named context along with the class names of objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the class names of objects bound to them 
 enumerates the names bound in the named context along with the objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the objects bound to them 
 destroys the named context and removes it from the namespace any attributes associated with the name are also removed intermediate contexts are not destroyed this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist in a federated naming system a context from one naming system may be bound to a name in another one can subsequently look up and perform operations on the foreign context using a composite name however an attempt destroy the context using this composite name will fail with notcontextexception because the foreign context is not a subcontext of the context in which it is bound instead use unbind to remove the binding of the foreign context destroying the foreign context requires that the destroysubcontext be performed on a context from the foreign context s native naming system 
 destroys the named context and removes it from the namespace 
 creates and binds a new context creates a new context with the given name and binds it in the target context that named by all but terminal atomic component of the name all intermediate contexts and the target context must already exist 
 creates and binds a new context 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 retrieves the named object following links except for the terminal atomic component of the name 
 retrieves the parser associated with the named context in a federation of namespaces different naming systems will parse names differently this method allows an application to get a parser for parsing names into their atomic components using the naming convention of a particular naming system within any single naming system nameparser objects returned by this method must be equal using the equals test 
 retrieves the parser associated with the named context 
 composes the name of this context with a name relative to this context given a name name relative to this context and the name prefix of this context relative to one of its ancestors this method returns the composition of the two names using the syntax appropriate for the naming system s involved that is if name names an object relative to this context the result is the name of the same object but relative to the ancestor context none of the names may be null 
 closes this context this method releases this context s resources immediately instead of waiting for them to be released automatically by the garbage collector this method is idempotent invoking it on a context that has already been closed has no effect invoking any other method on a closed context is not allowed and results in undefined behaviour 
 retrieves the named object 
 binds a name to an object all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist 
 retrieves the next element in the enumeration 
 add a resources jar the contents of meta inf resources will be used if a requested resource can not be found in the main context 
 retrieves the named object if name is empty returns a new instance of this context which represents the same naming context as this context but its environment may be modified independently and it may be accessed concurrently 
 retrieves the named object 
 binds a name to an object all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist 
 binds a name to an object 
 binds a name to an object overwriting any existing binding all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist if the object is a dircontext any existing attributes associated with the name are replaced with those of the object otherwise any existing attributes associated with the name remain unchanged 
 binds a name to an object overwriting any existing binding 
 unbinds the named object removes the terminal atomic name in name from the target context that named by all but the terminal atomic part of name this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist 
 binds a new name to the object bound to an old name and unbinds the old name both names are relative to this context any attributes associated with the old name become associated with the new name intermediate contexts of the old name are not changed 
 enumerates the names bound in the named context along with the class names of objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the class names of objects bound to them 
 enumerates the names bound in the named context along with the objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the objects bound to them 
 destroys the named context and removes it from the namespace any attributes associated with the name are also removed intermediate contexts are not destroyed this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist in a federated naming system a context from one naming system may be bound to a name in another one can subsequently look up and perform operations on the foreign context using a composite name however an attempt destroy the context using this composite name will fail with notcontextexception because the foreign context is not a subcontext of the context in which it is bound instead use unbind to remove the binding of the foreign context destroying the foreign context requires that the destroysubcontext be performed on a context from the foreign context s native naming system 
 creates and binds a new context creates a new context with the given name and binds it in the target context that named by all but terminal atomic component of the name all intermediate contexts and the target context must already exist 
 creates and binds a new context 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 composes the name of this context with a name relative to this context given a name name relative to this context and the name prefix of this context relative to one of its ancestors this method returns the composition of the two names using the syntax appropriate for the naming system s involved that is if name names an object relative to this context the result is the name of the same object but relative to the ancestor context none of the names may be null 
 retrieves all of the attributes associated with a named object 
 retrieves all of the attributes associated with a named object 
 retrieves selected attributes associated with a named object see the class description regarding attribute models attribute type names and operational attributes 
 retrieves selected attributes associated with a named object 
 modifies the attributes associated with a named object the order of the modifications is not specified where possible the modifications are performed atomically 
 modifies the attributes associated with a named object using an an ordered list of modifications the modifications are performed in the order specified each modification specifies a modification operation code and an attribute on which to operate where possible the modifications are performed atomically 
 binds a name to an object along with associated attributes if attrs is null the resulting binding will have the attributes associated with obj if obj is a dircontext and no attributes otherwise if attrs is non null the resulting binding will have attrs as its attributes any attributes associated with obj are ignored 
 binds a name to an object along with associated attributes overwriting any existing binding if attrs is null and obj is a dircontext the attributes from obj are used if attrs is null and obj is not a dircontext any existing attributes associated with the object already bound in the directory remain unchanged if attrs is non null any existing attributes associated with the object already bound in the directory are removed and attrs is associated with the named object if obj is a dircontext and attrs is non null the attributes of obj are ignored 
 creates and binds a new context along with associated attributes this method creates a new subcontext with the given name binds it in the target context that named by all but terminal atomic component of the name and associates the supplied attributes with the newly created object all intermediate and target contexts must already exist if attrs is null this method is equivalent to context createsubcontext 
 retrieves the schema associated with the named object the schema describes rules regarding the structure of the namespace and the attributes stored within it the schema specifies what types of objects can be added to the directory and where they can be added what mandatory and optional attributes an object can have the range of support for schemas is directory specific 
 retrieves a context containing the schema objects of the named object s class definitions 
 searches in a single context for objects that contain a specified set of attributes and retrieves selected attributes the search is performed using the default searchcontrols settings 
 searches in a single context for objects that contain a specified set of attributes this method returns all the attributes of such objects it is equivalent to supplying null as the atributestoreturn parameter to the method search name attributes string 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 return the last modified date 
 get object content 
 get object content 
 get input stream 
 list children of this collection the names given are relative to this uri s path the full uri of the children is then path   name 
 destroys the named context and removes it from the namespace any attributes associated with the name are also removed intermediate contexts are not destroyed this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist in a federated naming system a context from one naming system may be bound to a name in another one can subsequently look up and perform operations on the foreign context using a composite name however an attempt destroy the context using this composite name will fail with notcontextexception because the foreign context is not a subcontext of the context in which it is bound instead use unbind to remove the binding of the foreign context destroying the foreign context requires that the destroysubcontext be performed on a context from the foreign context s native naming system 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 binds a name to an object along with associated attributes if attrs is null the resulting binding will have the attributes associated with obj if obj is a dircontext and no attributes otherwise if attrs is non null the resulting binding will have attrs as its attributes any attributes associated with obj are ignored 
 binds a name to an object along with associated attributes overwriting any existing binding if attrs is null and obj is a dircontext the attributes from obj are used if attrs is null and obj is not a dircontext any existing attributes associated with the object already bound in the directory remain unchanged if attrs is non null any existing attributes associated with the object already bound in the directory are removed and attrs is associated with the named object if obj is a dircontext and attrs is non null the attributes of obj are ignored 
 creates and binds a new context along with associated attributes this method creates a new subcontext with the given name binds it in the target context that named by all but terminal atomic component of the name and associates the supplied attributes with the newly created object all intermediate and target contexts must already exist if attrs is null this method is equivalent to context createsubcontext 
 content accessor 
 builds a proxy directory context using the given environment 
 retrieves the named object if name is empty returns a new instance of this context which represents the same naming context as this context but its environment may be modified independently and it may be accessed concurrently 
 retrieves the named object 
 binds a name to an object all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist 
 binds a name to an object 
 binds a name to an object overwriting any existing binding all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist if the object is a dircontext any existing attributes associated with the name are replaced with those of the object otherwise any existing attributes associated with the name remain unchanged 
 binds a name to an object overwriting any existing binding 
 unbinds the named object removes the terminal atomic name in name from the target context that named by all but the terminal atomic part of name this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist 
 unbinds the named object 
 binds a new name to the object bound to an old name and unbinds the old name both names are relative to this context any attributes associated with the old name become associated with the new name intermediate contexts of the old name are not changed 
 binds a new name to the object bound to an old name and unbinds the old name 
 enumerates the names bound in the named context along with the class names of objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the class names of objects bound to them 
 enumerates the names bound in the named context along with the objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the objects bound to them 
 destroys the named context and removes it from the namespace any attributes associated with the name are also removed intermediate contexts are not destroyed this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist in a federated naming system a context from one naming system may be bound to a name in another one can subsequently look up and perform operations on the foreign context using a composite name however an attempt destroy the context using this composite name will fail with notcontextexception because the foreign context is not a subcontext of the context in which it is bound instead use unbind to remove the binding of the foreign context destroying the foreign context requires that the destroysubcontext be performed on a context from the foreign context s native naming system 
 destroys the named context and removes it from the namespace 
 creates and binds a new context creates a new context with the given name and binds it in the target context that named by all but terminal atomic component of the name all intermediate contexts and the target context must already exist 
 creates and binds a new context 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 retrieves the named object following links except for the terminal atomic component of the name 
 retrieves the parser associated with the named context in a federation of namespaces different naming systems will parse names differently this method allows an application to get a parser for parsing names into their atomic components using the naming convention of a particular naming system within any single naming system nameparser objects returned by this method must be equal using the equals test 
 retrieves the parser associated with the named context 
 composes the name of this context with a name relative to this context given a name name relative to this context and the name prefix of this context relative to one of its ancestors this method returns the composition of the two names using the syntax appropriate for the naming system s involved that is if name names an object relative to this context the result is the name of the same object but relative to the ancestor context none of the names may be null 
 adds a new environment property to the environment of this context if the property already exists its value is overwritten 
 removes an environment property from the environment of this context 
 retrieves the environment in effect for this context see class description for more details on environment properties the caller should not make any changes to the object returned their effect on the context is undefined the environment of this context may be changed using addtoenvironment and removefromenvironment 
 closes this context this method releases this context s resources immediately instead of waiting for them to be released automatically by the garbage collector this method is idempotent invoking it on a context that has already been closed has no effect invoking any other method on a closed context is not allowed and results in undefined behaviour 
 retrieves the full name of this context within its own namespace many naming services have a notion of a full name for objects in their respective namespaces for example an ldap entry has a distinguished name and a dns record has a fully qualified name this method allows the client application to retrieve this name the string returned by this method is not a jndi composite name and should not be passed directly to context methods in naming systems for which the notion of full name does not make sense operationnotsupportedexception is thrown 
 retrieves all of the attributes associated with a named object 
 retrieves all of the attributes associated with a named object 
 retrieves selected attributes associated with a named object see the class description regarding attribute models attribute type names and operational attributes 
 retrieves selected attributes associated with a named object 
 modifies the attributes associated with a named object the order of the modifications is not specified where possible the modifications are performed atomically 
 modifies the attributes associated with a named object 
 modifies the attributes associated with a named object using an an ordered list of modifications the modifications are performed in the order specified each modification specifies a modification operation code and an attribute on which to operate where possible the modifications are performed atomically 
 modifies the attributes associated with a named object using an an ordered list of modifications 
 binds a name to an object along with associated attributes if attrs is null the resulting binding will have the attributes associated with obj if obj is a dircontext and no attributes otherwise if attrs is non null the resulting binding will have attrs as its attributes any attributes associated with obj are ignored 
 binds a name to an object along with associated attributes 
 binds a name to an object along with associated attributes overwriting any existing binding if attrs is null and obj is a dircontext the attributes from obj are used if attrs is null and obj is not a dircontext any existing attributes associated with the object already bound in the directory remain unchanged if attrs is non null any existing attributes associated with the object already bound in the directory are removed and attrs is associated with the named object if obj is a dircontext and attrs is non null the attributes of obj are ignored 
 binds a name to an object along with associated attributes overwriting any existing binding 
 creates and binds a new context along with associated attributes this method creates a new subcontext with the given name binds it in the target context that named by all but terminal atomic component of the name and associates the supplied attributes with the newly created object all intermediate and target contexts must already exist if attrs is null this method is equivalent to context createsubcontext 
 creates and binds a new context along with associated attributes 
 retrieves the schema associated with the named object the schema describes rules regarding the structure of the namespace and the attributes stored within it the schema specifies what types of objects can be added to the directory and where they can be added what mandatory and optional attributes an object can have the range of support for schemas is directory specific 
 retrieves the schema associated with the named object 
 retrieves a context containing the schema objects of the named object s class definitions 
 retrieves a context containing the schema objects of the named object s class definitions 
 searches in a single context for objects that contain a specified set of attributes and retrieves selected attributes the search is performed using the default searchcontrols settings 
 searches in a single context for objects that contain a specified set of attributes and retrieves selected attributes 
 searches in a single context for objects that contain a specified set of attributes this method returns all the attributes of such objects it is equivalent to supplying null as the atributestoreturn parameter to the method search name attributes string 
 searches in a single context for objects that contain a specified set of attributes 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 searches in the named context or object for entries that satisfy the given search filter performs the search as specified by the search controls 
 retrieves the named object as a cache entry without any exception 
 revalidate entry 
 load entry into cache 
 set the document root 
java util zip zipexception ioexception 
 release any resources allocated for this directory context 
 retrieves the named object 
 jndi treats and as reserved characters therefore they need to be escaped as part of converting file names to jndi names note that while can be used in windows and unix file names is only valid on unix this method assumes that the string is currently unquoted 
 enumerates the names bound in the named context along with the objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 retrieves selected attributes associated with a named object see the class description regarding attribute models attribute type names and operational attributes 
 constructs a tree of the entries contained in a war file 
 content accessor 
 retrieves the named object if name is empty returns a new instance of this context which represents the same naming context as this context but its environment may be modified independently and it may be accessed concurrently 
 retrieves the named object 
 binds a name to an object all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist 
 binds a name to an object 
 binds a name to an object overwriting any existing binding all intermediate contexts and the target context that named by all but terminal atomic component of the name must already exist if the object is a dircontext any existing attributes associated with the name are replaced with those of the object otherwise any existing attributes associated with the name remain unchanged 
 binds a name to an object overwriting any existing binding 
 unbinds the named object removes the terminal atomic name in name from the target context that named by all but the terminal atomic part of name this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist 
 unbinds the named object 
 binds a new name to the object bound to an old name and unbinds the old name both names are relative to this context any attributes associated with the old name become associated with the new name intermediate contexts of the old name are not changed 
 binds a new name to the object bound to an old name and unbinds the old name 
 enumerates the names bound in the named context along with the class names of objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the class names of objects bound to them 
 enumerates the names bound in the named context along with the objects bound to them the contents of any subcontexts are not included if a binding is added to or removed from this context its effect on an enumeration previously returned is undefined 
 enumerates the names bound in the named context along with the objects bound to them 
 destroys the named context and removes it from the namespace any attributes associated with the name are also removed intermediate contexts are not destroyed this method is idempotent it succeeds even if the terminal atomic name is not bound in the target context but throws namenotfoundexception if any of the intermediate contexts do not exist in a federated naming system a context from one naming system may be bound to a name in another one can subsequently look up and perform operations on the foreign context using a composite name however an attempt destroy the context using this composite name will fail with notcontextexception because the foreign context is not a subcontext of the context in which it is bound instead use unbind to remove the binding of the foreign context destroying the foreign context requires that the destroysubcontext be performed on a context from the foreign context s native naming system 
 destroys the named context and removes it from the namespace 
 creates and binds a new context creates a new context with the given name and binds it in the target context that named by all but terminal atomic component of the name all intermediate contexts and the target context must already exist 
 creates and binds a new context 
 retrieves the named object following links except for the terminal atomic component of the name if the object bound to name is not a link returns the object itself 
 retrieves the named object following links except for the terminal atomic component of the name 
 retrieves the parser associated with the named context in a federation of namespaces different naming systems will parse names differently this method allows an application to get a parser for parsing names into their atomic components using the naming convention of a particular naming system within any single naming system nameparser objects returned by this method must be equal using the equals test 
 retrieves the parser associated with the named context 
 composes the name of this context with a name relative to this context given a name name relative to this context and the name prefix of this context relative to one of its ancestors this method returns the composition of the two names using the syntax appropriate for the naming system s involved that is if name names an object relative to this context the result is the name of the same object but relative to the ancestor context none of the names may be null 
 adds a new environment property to the environment of this context if the property already exists its value is overwritten 
 removes an environment property from the environment of this context 
 retrieves the environment in effect for this context see class description for more details on environment properties the caller should not make any changes to the object returned their effect on the context is undefined the environment of this context may be changed using addtoenvironment and removefromenvironment 
 closes this context this method releases this context s resources immediately instead of waiting for them to be released automatically by the garbage collector this method is idempotent invoking it on a context that has already been closed has no effect invoking any other method on a closed context is not allowed and results in undefined behaviour 
 get the bound context 
 strips the url header 
 perform the check
 perform the conversion
 perform the actual copy and conversion
 param input input stream
 parse the given java class file and return an object that represents the contained data i e constants methods fields and commands a classformatexception is raised if the file is not a valid class file this does not include verification of the byte code as it is performed by the java interpreter 
 read information about the attributes of the class 
 read information about the class and its super class 
 read constant pool entries 
 read information about the fields of the class i e its variables 
 check whether the header of the file is ok of course this has to be the first action on successive file reads 
 read information about the interfaces implemented by this class 
 read information about the methods of the class 
 read major and minor version of compiler which created the file 
 read one constant from the given input the type depends on a tag byte 
 initialize instance from file data 
 initialize instance from file data 
 initialize instance from file data 
 initialize instance from file data 
 initialize instance from file data 
 read constants from given input stream 
 convert the given bytes to characters 
 deprecated unused will be removed in tomcat   x onwards 
 append a char by casting it to byte this is not intended for unicode 
 add data to the buffer
 deprecated unused will be removed in tomcat   x onwards 
 send the buffer to the sink called by append when the limit is reached you can also call it explicitly to force the data to be written 
 convert the given characters to bytes 
 deprecated unused will be removed in tomcat   x onwards 
 add data to the buffer
 add data to the buffer 
 append a string to the buffer
 append a string to the buffer
 deprecated unused will be removed in tomcat   x onwards 
 deprecated unused will be removed in tomcat   x onwards 
 copy the src into this messagebytes allocating more space if needed
 urldecode will modify the source includes converting  to 
 in buffer processing the buffer will be modified includes converting  to 
 urldecode will modify the source includes converting  to 
 urldecode will modify the source
 url encode string using a specified encoding 
 set the associated digester if needed this class loads the parameter classes from their names 
 process the end of this element 
 return the currently mapped namespace uri for the specified prefix if any otherwise return null these mappings come and go dynamically as the document is parsed 
java util emptystackexception runtimeexception 
 return the saxparserfactory we will use creating one if necessary 
 returns a flag indicating whether the requested feature is supported by the underlying implementation of org xml sax xmlreader see for information about the standard sax feature flags 
 sets a flag indicating whether the requested feature is supported by the underlying implementation of org xml sax xmlreader see for information about the standard sax feature flags in order to be effective this method must be called before the getparser method is called for the first time either directly or indirectly 
 return the saxparser we will use to parse the input stream if there is a problem creating the parser return null 
 return the current value of the specified property for the underlying xmlreader implementation see for information about the standard sax properties 
 set the current value of the specified property for the underlying xmlreader implementation see for information about the standard sax properties 
 return the xmlreader to be used for parsing the input document fix me there is a bug in jaxp xerces that prevent the use of a parser that contains a schema with a dtd 
 process notification of the end of an xml element being reached 
 process notification that a namespace prefix is going out of scope 
 forward notification of a parsing error to the application supplied error handler if any 
 forward notification of a fatal parsing error to the application supplied error handler if any 
 forward notification of a parse warning to the application supplied error handler if any 
 parse the content of the specified file using this digester returns the root element from the object stack if any 
 parse the content of the specified input source using this digester returns the root element from the object stack if any 
 parse the content of the specified input stream using this digester returns the root element from the object stack if any 
 parse the content of the specified reader using this digester returns the root element from the object stack if any 
 parse the content of the specified uri using this digester returns the root element from the object stack if any 
 return the top object on the stack without removing it if there are no objects on the stack return null 
 return the n th object down the stack where  is the top element and getcount  is the bottom element if the specified index is out of range return null 
 pop the top object off of the stack and return it if there are no objects on the stack return null 
 pops gets and removes the top object from the stack with the given name note a stack is considered empty if no objects have been pushed onto it yet 
 gets the top object from the stack with the given name this method does not remove the object from the stack note a stack is considered empty if no objects have been pushed onto it yet 
 return the top object on the parameters stack without removing it if there are no objects on the stack return null the parameters stack is used to store callmethodrule parameters see link params 
 return the n th object down the parameters stack where  is the top element and getcount  is the bottom element if the specified index is out of range return null the parameters stack is used to store callmethodrule parameters see link params 
 pop the top object off of the parameters stack and return it if there are no objects on the stack return null the parameters stack is used to store callmethodrule parameters see link params 
 process the beginning of this element 
 process the end of this element 
 return an instance of our associated object creation factory creating one if necessary 
 create a saxparser configured to support xml schema and dtd
 constructor stores the content handler currently used by digester so it can be reset when done and initializes the dom objects needed to build the node 
 appends a link org wc dom text text node to the current node 
 checks whether control needs to be returned to digester 
 adds a new link org wc dom processinginstruction processinginstruction to the current node 
 adds a new child link org wc dom element element to the currentnode 
 constructor creates an instance of this rule that will create either a dom link org wc dom element element or a dom link org wc dom documentfragment documentfragment depending on the value of the nodetype parameter 
 implemented to replace the content handler currently in use by a nodebuilder 
 process the beginning of this element 
 create a new saxparser
 this method is called when the beginning of a matching xml element is encountered the default implementation delegates to the deprecated method link begin attributes begin without the namespace and name parameters to retain backwards compatibility 
 this method is called when the body of a matching xml element is encountered if the element has no body this method is not called at all the default implementation delegates to the deprecated method link body string body without the namespace andname parameters to retain backwards compatibility 
 this method is called when the end of a matching xml element is encountered the default implementation delegates to the deprecated method link end end without the namespace and name parameters to retain backwards compatibility 
 process the end of this element 
 process the end of this element 
 process the end of this element 
 return the current xerces version 
 create a saxparser based on the underlying xerces version 
 configure schema validation as recommended by the jaxp   spec the properties object may contains information about the schema local and language 
 configure schema validation as recommended by the xerces spec both dtd and schema validation will be enabled simultaneously 
 read xml as dom 
 load the resource from the specified location 
 writes the entire contents of the specified input stream to this byte stream bytes from the input stream are read directly into the internal buffers of this streams 
 writes the entire contents of this byte stream to the specified output stream 
 switches the underlying output stream from a memory based stream to one that is backed by disk this is the point at which we realise that too much data is being written to keep in memory so we elect to switch to disk based storage 
 returns an link java io inputstream inputstream that can beused to retrieve the contents of the file 
 returns the contents of the file as an array of bytes if the contents of the file were not yet cached in memory they will be loaded from the disk storage and cached 
 returns the contents of the file as a string using the specified encoding this method uses link get to retrieve thecontents of the file 
 returns the contents of the file as a string using the default character encoding this method uses link get to retrieve thecontents of the file todo consider making this method throw unsupportedencodingexception 
 a convenience method to write an uploaded item to disk the client code is not concerned with whether or not the item is stored in memory or on disk in a temporary location they just want to write the uploaded item to a file this implementation first attempts to rename the uploaded item to the specified destination file if the item was originally written to disk otherwise the data will be copied to the specified file this method is only guaranteed to work once the first time it is invoked for a particular item this is because in the event that the method renames a temporary file that file will no longer be available to copy or rename again at a later time 
 processes an rfc  compliant multipart form data stream 
 processes an rfc  compliant multipart form data stream 
 processes an rfc  compliant multipart form data stream 
 creates a new instance 
 returns an input stream which may be used to read the items contents 
 closes the file item 
 creates a new instance 
 called for finding the next item if any 
org apache tomcat util http fileupload fileuploadioexception ioexception 
org apache tomcat util http fileupload malformedstreamexception ioexception 
org apache tomcat util http fileupload illegalboundaryexception ioexception 
 returns whether another instance of link fileitemstream is available 
 returns the next available link fileitemstream 
 deletes a directory recursively 
 cleans a directory without deleting it 
 deletes a file if file is a directory delete it and all sub directories the difference between file delete and this method are a directory to be deleted does not have to be empty you get exceptions when a file or directory cannot be deleted java io file methods returns a boolean 
 schedules a file to be deleted when jvm exits if file is directory delete it and all sub directories 
 schedules a directory recursively for deletion on jvm exit 
 cleans a directory without deleting it 
 determines whether the specified file is a symbolic link rather than an actual file will not return true if there is a symbolic link anywhere in the path only if the specific file is note the current implementation always returns code false ifthe system is detected as windows using link file separatorchar    
 copy bytes from an inputstream to an outputstream this method buffers the input internally so there is no need to use a bufferedinputstream large streams over gb will return a bytes copied value of  after the copy has completed since the correct number of bytes cannot be returned as an int for large streams use the copylarge inputstream outputstream method 
 copy bytes from a large over gb inputstream to an outputstream this method buffers the input internally so there is no need to use a bufferedinputstream the buffer size is given by link default buffer size 
 reads bytes from an input stream this implementation guarantees that it will read as many bytes as possible before giving up this may not always be the case for subclasses of link inputstream 
 reads the requested number of bytes or fail if there are not enough left this allows for the possibility that link inputstream read byte int int maynot read as many bytes as requested most likely because of reaching eof 
 reads the requested number of bytes or fail if there are not enough left this allows for the possibility that link inputstream read byte int int maynot read as many bytes as requested most likely because of reaching eof 
 reads a byte from the buffer and refills it as necessary 
 skips a boundary token and checks whether more encapsulations are contained in the stream 
 reads the header part of the current encapsulation headers are returned verbatim to the input stream including the trailing crlf marker parsing is left to the application todo allow limiting maximum header size to protect against abuse 
 reads body data from the current encapsulation and writes its contents into the output stream arbitrary large amounts of data can be processed by this method using a constant size buffer see link multipartstream inputstream byte int multipartstream progressnotifier constructor 
 reads body data from the current encapsulation and discards it use this method to skip encapsulations you don t need or don t understand 
 finds the beginning of the first encapsulation 
 returns the next byte in the stream 
 reads bytes into the given buffer 
 closes the input stream 
 closes the input stream 
 skips the given number of bytes 
 attempts to read more data 
 processes an rfc  compliant multipart form data stream 
 retrieve the content length of the request 
 retrieve the input stream for the request 
 writes the specified byte to this output stream 
 writes b length bytes from the specified byte array to this output stream 
 writes len bytes from the specified byte array starting at offset off to this output stream 
 flushes this output stream and forces any buffered output bytes to be written out 
 closes this output stream and releases any system resources associated with this stream 
 checks to see if writing the specified number of bytes would cause the configured threshold to be exceeded if so triggers an event to allow a concrete implementation to take action on this 
 called to check whether the input streams limit is reached 
 reads the next byte of data from this input stream the value byte is returned as an int in the range  to  if no byte is available because the end of the stream has been reached the value  is returned this method blocks until input data is available the end of the stream is detected or an exception is thrown this method simply performs in read and returns the result 
 reads up to len bytes of data from this input stream into an array of bytes if len is not zero the method blocks until some input is available otherwise no bytes are read and  is returned this method simply performs in read b off len and returns the result 
 decode a string of text obtained from a mail header into its proper form the text generally will consist of a string of tokens some of which may be encoded using base encoding 
 parse a string using the rfc  rules for an encoded word type this encoding has the syntax encoded word     charset encoding encoded text   
 decode the encoded byte data writing it to the given output stream 
 copies the contents of the given link inputstream to the given link outputstream shortcut for copy pinputstream poutputstream new byte  
 copies the contents of the given link inputstream to the given link outputstream 
 this convenience method allows to read a link org apache tomcat util http fileupload fileitemstream s content into a string the platform s default character encoding is used for converting bytes into characters 
 this convenience method allows to read a link org apache tomcat util http fileupload fileitemstream s content into a string using the given character encoding 
 map the specified host name and uri mutating the given mapping data 
 map the specified uri relative to the context mutating the given mapping data 
 map the specified uri 
 wrapper mapping (eM鶖剆ervlet
 process the query string into parameters
 parses an http authorization header for digest authentication as per rfc  section    
 return the token if one was found the empty string if no data wasavailable to read or null if data other than a token was found
 return the quoted string if one was found null if data other than aquoted string was found or null if the end of data was reached before the quoted string was terminated
 token can be read unambiguously with or without surrounding quotes so this parsing method for token permits optional surrounding double quotes this is not defined in any rfc it is a special case to handle data from buggy clients known buggy clients for digest auth include microsoft ie  amp  apple safari for osx and ios that add quotes to values that should be tokens 
 lhex can be read unambiguously with or without surrounding quotes so this parsing method for lhex permits optional surrounding double quotes some buggy clients libwww perl for digest auth are known to send quoted lhex when the specification requires just lhex lhex are literally lower case hexadecimal digits this implementation allows for upper case digits as well converting the returned value to lower case 
 call execute any ant like task should work
 call void setattribute string object 
 call void getattribute string 
 construct a urlclassloader will compile and work in jdk  too 
 guess a product install home by analyzing the class path it works for product using the pattern lib executable jar or if executable jar is included in classpath by a shell script java jar also works insures both install and home system properties are set if either or both system properties are unset install and home will be set to the same value this value will be the other system property that is set or the guessed value if neither is set 
 deprecated not used
 deprecated is used only by deprecated method
 construct a file url from a file using a base dir
 add elements from the classpath cp to a vector jars as file urls we use vector for jdk   compat 
 construct a url classpath from files in a directory a cpath property and tools jar 
 deprecated not used
 deprecated not used though compliments callmethod and callmethodn here
 obtain and return the value of a specific attribute of this mbean 
 obtain and return the values of several attributes of this mbean 
 invoke a particular method on this mbean and return any returned value implementation note this implementation will attempt to invoke this method on the mbean itself or if not available on the managed resource object associated with this mbean 
 set the value of a specific attribute of this mbean 
 set the values of several attributes of this mbean 
 add an attribute change notification event listener to this mbean 
 remove an attribute change notification event listener from this mbean 
 remove an attribute change notification event listener from this mbean 
 send an attributechangenotification to all registered listeners 
 send a notification which contains the specified string as a jmx modelmbean generic notification 
 add a notification event listener to this mbean 
 remove a notification event listener from this mbean 
 remove a notification event listener from this mbean 
 remove a notification event listener from this mbean 
 remove a notification event listener from this mbean 
 remove a notification event listener from this mbean 
 create and return a modelmbean that has been preconfigured with the modelmbeaninfo information for this managed bean but is not associated with any particular managed resource the returned modelmbean will not have been registered with our mbeanserver 
 create and return a modelmbean that has been preconfigured with the modelmbeaninfo information for this managed bean and is associated with the specified managed object instance the returned modelmbean will not have been registered with our mbeanserver 
 check if this class conforms to javabeans specifications if the class is conformant returns true 
 xxx find if the classname is the name of the mbean or the real class i suppose first xxx read optional descriptions from a properties generated from source xxx deal with constructors
 store the mbeans xxx add a background thread to store it periodically 
 load data returns a list of items 
 deprecated unused will be removed in tomcat   x
 load an extended mlet file the source can be an url file or inputstream all mbeans will be instantiated registered and the attributes will be set the result is a list of objectnames 
 load descriptors the source can be a file or url or inputstream for the descriptors file in the case of file and url if the extension is ser a serialized version will be loaded this method should be used to explicitly load metadata but this is not required in most cases the registercomponent method will find metadata in the same package 
 register a bean by creating a modeler mbean and adding it to the mbeanserver if metadata is not loaded we ll look up and read a file named mbeans descriptors ser or mbeans descriptors xml in the same package or parent if the bean is an instance of dynamicmbean it s metadata will be converted to a model mbean and we ll wrap it so modeler services will be supported if the metadata is still not found introspection will be used to extract it automatically if an mbean is already registered under this name it ll be first unregistered if the component implements mbeanregistration the methods will be called if the method has a method setregistry that takes a registrymbean as parameter it ll be called with the current registry 
 unregister a component we ll first check if it is registered and mask all errors this is mostly a helper 
 invoke a operation on a list of mbeans can be used to implement lifecycle operations 
 get the type of an attribute of the object from the metadata 
 find the operation info for a method
 find or load metadata 
 experimental convert a string to object based on type used by several components we could provide some pluggability it is here to keep things consistent and avoid duplication in other tasks 
 experimental 
 register a component xxx make it private 
 lookup the component descriptor in the package and in the parent packages 
 param sourcetype
 deprecated
 unlock the server socket accept using a bogus connection 
 port in use 
 initialize the endpoint 
 deallocate apr memory pools and close server socket 
 process the specified connection 
 add the sendfile data to the sendfile poller note that in most cases the initial non blocking calls to sendfile will return right away and will be handled asynchronously inside the kernel as a result the poller will never be used 
org apache tomcat jni error 
 deallocate apr memory pools and close server socket 
 configure the socket 
java security keystoreexception generalsecurityexception 
 reads the keystore and initializes the ssl socket factory 
 gets the initialized key managers 
java security unrecoverablekeyexception unrecoverableentryexception generalsecurityexception 
 gets the initialized trust managers 
java security invalidalgorithmparameterexception generalsecurityexception 
 return the initialization parameters for the trustmanager currently only the default pkix is supported 
java security cert crlexception generalsecurityexception 
 load the collection of crls 
 checks that the certificate is compatible with the enabled cipher suites if we don t check now the jioendpoint can enter a nasty logging loop see bug  
javax net ssl sslpeerunverifiedexception sslexception ioexception 
java security cert certificateencodingexception certificateexception generalsecurityexception 
 performs a blocking write using the bytebuffer for data to be written if the selector parameter is null then it will perform a busy write that could take up a lot of cpu cycles 
 performs a blocking read using the bytebuffer for data to be read if the selector parameter is null then it will perform a busy read that could take up a lot of cpu cycles 
 closes this channel 
 writes a sequence of bytes to this channel from the given buffer 
 reads a sequence of bytes from this channel into the given buffer 
 initialize the endpoint 
 start the nio endpoint creating acceptor poller threads 
 deallocate nio memory pools and close server socket 
 process the specified connection 
 the background thread that listens for incoming tcp ip connections and hands them off to an appropriate processor 
 performs a blocking write using the bytebuffer for data to be written and a selector to block if the selector parameter is null then it will perform a busy write that could take up a lot of cpu cycles 
 performs a blocking read using the bytebuffer for data to be read and a selector to block if the selector parameter is null then it will perform a busy read that could take up a lot of cpu cycles 
 performs a read using the bytebuffer for data to be read and a selector to register for events should you have the block true if the selector parameter is null then it will perform a busy read that could take up a lot of cpu cycles 
javax net ssl sslexception ioexception 
 flush the channel 
 flushes the buffer to the network non blocking
 performs ssl handshake non blocking but performs need task on the same thread hence you should never call this method using your acceptor thread as you would slow down your system significantly the return for this operation is  if the handshake is complete and a positive value if it is not complete in the event of a positive value coming back reregister the selection key for the return values interestops 
 force a blocking handshake to take place for this key this requires that both network and application buffers have been emptied out prior to this call taking place or a ioexception will be thrown 
 performs the wrap function
 perform handshake unwrap
 sends a ssl close message will not physically close the connection here to close the connection you could do something like close while isopen mytimeoutfunction thread sleep  if isopen close true forces a close if you timed out 
 force a close can throw an ioexception
 reads a sequence of bytes from this channel into the given buffer 
 writes a sequence of bytes to this channel from the given buffer 
 create a url object by parsing a string representation relative to a specified context based on logic from jdk    s java net url 
 parse the specified portion of the string representation of a url assuming that it has a format similar to that for http fixme this algorithm can undoubtedly be optimized for performance however that needs to wait until after sufficient unit tests are implemented to guarantee correct behavior with no regressions 
 scan the provided servletcontext and classloader for jar files each jar file found will be passed to the callback handler to be processed 
 ensures that link digest string byte will support the specifiedalgorithm this method must be called and return successfully before using link digest string byte 
 acquires a shared latch if one is available or waits for one if no shared latch is current available 
 executes the given command at some time in the future the command may execute in a new thread in a pooled thread or in the calling thread at the discretion of the executor implementation if no threads are available it will be added to the work queue if the work queue is full the system will wait for the specified time and it throw a rejectedexecutionexception if the queue is still full after that 
java util concurrent timeoutexception 
java util zip dataformatexception 
javax websocket decodeexception 
javax websocket deploymentexception 
javax websocket encodeexception 
 called when there is data in the servletinputstream to process 
 published the provided endpoint implementation at the specified path with the specified configuration link wsservercontainer servletcontext must be called before calling this method 
 provides the equivalent of link addendpoint serverendpointconfig for publishing plain old java objects pojos that have been annotated as websocket endpoints 
 until the websocket specification provides such a mechanism this tomcat proprietary method is provided to enable applications to programmatically determine whether or not to upgrade an individual request to websocket note this method is not used by tomcat but is used directly by third party code and must not be removed 
 return true if sufficient data was present to complete theprocessing of the header
org apache tomcat websocket wsioexception ioexception 
 creates a new websocket session for communication between the two provided end points the result of link thread getcontextclassloader at the time this constructor is called will be used when calling link endpoint onclose session closereason 
 websocket   section    need internal close method as spec requires that the local endpoint receives a  on timeout 
 called when a close message is received should only ever happen once also called after a protocol error when the protocolhandler needs to force the closing of the connection 
 process response blocking until http response has been fully received 
 cleans up the resources still in use by websocket sessions created from this container this includes closing sessions and cancelling link future s associated with blocking read writes 
 validate a jsp page this will get invoked once per directive in the jsp page this method will return null if the page is valid otherwise the method should return an array of validationmessage objects an array of length zero is also interpreted as no errors 

 installs a cookie handler for the http request to manage session cookies 
 creates an authenticatedurl 
 returns an authenticated link httpurlconnection 
 helper method that extracts an authentication token received from a connection this method is used by link authenticator implementations 
 performs spnego authentication against the specified url if a token is given it does a nop and returns the given token if no token is given it will perform the spnego authentication sequence using an http options request 
 implements the spnego authentication sequence interaction using the current default principal in the kerberos cache normally set via kinit 
 performs simple authentication against the specified url if a token is given it does a nop and returns the given token if no token is given it will perform an http options request injecting an additional parameter link user name in the query string with the value returned by the link getusername method if the response is successful it will update the authentication token 
 it enforces the the kerberos spnego authentication sequence returning an link authenticationtoken only after the kerberos spnego sequence hascompleted successfully in the case of java access and only after the custom authentication implemented by the subclass in alternateauthenticate has completed successfully in the case of browser access 
 initializes the authentication filter and signer secret provider it instantiates and initializes the specified link authenticationhandler 
 if the request has a valid authentication token it allows the request to continue to the target resource otherwise it triggers an authentication sequence using the configured link authenticationhandler 
 initializes the authentication handler instance it creates a kerberos context using the principal and keytab specified in the configuration this method is invoked by the link authenticationfilter init method 
 it enforces the the kerberos spnego authentication sequence returning an link authenticationtoken only after the kerberos spnego sequence hascompleted successfully 
 gets an rsapublickey from the provided pem encoding 
 replace the numbered parameters of the form n where n is from  to the length of params normal text is copied directly and n is replaced by the corresponding parameter 
 try to apply this rule to the given name represented as a parameter array 
 get the translation of the principal name into an operating system user name 
 create kerberos principal for a given service and hostname inferring realm from the fqdn of the hostname it converts hostname to lower case if hostname is null or     it uses dynamically looked up fqdn of the current host instead if domain realm mappings are inadequately specified it will use default realm per usual kerberos behavior if default realm also gives a null value then a principal without realm will be returned which by kerberos definitions is just another way to specify default realm 
 get all the unique principals from keytabfile which matches a pattern 
 verifies a signed string and extracts the original string 
 returns then signature of a string 
 this method creates the curator client and connects to zookeeper 
 get the value of the name property as an int if no such property exists the provided default value is returned or if the specified value is not a valid int then an error is thrown 
 get the value of the name property as a long if no such property exists the provided default value is returned or if the specified value is not a valid long then an error is thrown 
 get the value of the name property as a float if no such property exists the provided default value is returned or if the specified value is not a valid float then an error is thrown 
 get the value of the name property as a double if no such property exists the provided default value is returned or if the specified value is not a valid double then an error is thrown 
 get the value for a known password configuration element in order to enable the elimination of clear text passwords in config this method attempts to resolve the property name as an alias through the credentialprovider api and conditionally fallsback to config 
 try and resolve the provided element name as a credential provider alias 
 load a class by name returning null rather than throwing an exception if it couldn t be loaded this is to avoid the overhead of creating an exception 
 get the value of the name property as an array of class the value of the property specifies a list of comma separated class names if no such property is specified then defaultvalue is returned 
 get the value of the name property as a class if no such property is specified then defaultvalue is returned 
 get an input stream attached to the configuration resource with the given name 
 get a link reader attached to the configuration resource with thegiven name 
 write out the non default properties in this configuration to the given link outputstream using utf  encoding 
 write out the non default properties in this configuration to the given link writer when property name is not empty and the property exists in the configuration this method writes the property and its attributes to the link writer when property name is null or empty this method writes all the configuration properties and their attributes to the link writer when property name is not empty but the property doesn t exist in the configuration this method throws an link illegalargumentexception 
 return the xml dom corresponding to this configuration 
 writes properties and their attributes final and resource to the given link writer when propertyname is not empty and the property exists in the configuration the format of the output would be property key key value value isfinal key isfinal resource key resource when propertyname is null or empty it behaves same as link dumpconfiguration configuration writer the output would be properties key key value value isfinal key isfinal resource key resource key key value value isfinal ke isfinal resource key resource when propertyname is not empty and the property is not found in the configuration this method will throw an link illegalargumentexception 
 writes out all properties and their attributes final and resource to the given link writer the format of the output would be properties key key value value isfinal key isfinal resource key resource key key value value isfinal ke isfinal resource key resource it does not output the properties of the configuration object which is loaded from an input stream 
 for debugging list non default properties to the terminal and exit 
 inheritdoc this method makes the change to this objects link configuration and calls reconfigurepropertyimpl to update internal data structures this method cannot be overridden subclasses should instead override reconfigurepropertyimpl 
 decryption is buffer based if there is data in link outbuffer then read it out of this buffer if there is no data in link outbuffer then read more from the underlying stream and do the decryption 
 read data from underlying stream 
 this method is executed immediately after decryption check whether decryptor should be updated and recalculate padding if needed 
 reset the underlying stream offset clear link inbuffer and link outbuffer this typically happens during link seek long or link skip long 
 positioned read it is thread safe 
 decrypt length bytes in buffer starting at offset output is also put into buffer starting at offset it is thread safe 
 positioned read fully it is thread safe 
 seek to a position 
 skip n bytes 
 get underlying stream position 
 bytebuffer read 
 decrypt all data in buf total n bytes from given start position output is also buf and same start position buf position and buf limit should be unchanged after decryption 
 encryption is buffer based if there is enough room in link inbuffer then write to this buffer if link inbuffer is full then do encryption and write data to theunderlying stream 
 do the encryption input is link inbuffer and output is link outbuffer 
 to flush we need to encrypt the data in the buffer and write to the underlying stream then do the flush 
 aes ctr will consume all of the input data it requires enough space in the destination buffer to encrypt entire input buffer 
 aes ctr will consume all of the input data it requires enough space in the destination buffer to decrypt entire input buffer 
 open up and initialize the keystore 
 try loading from the user specified path else load from the backup path in case exception is not due to bad wrong password 
 the keystore might have gone down during a flush in which case either the new or old files might exists this method tries to load the keystore from one of these intermediate files 
 get key metadata in bulk 
 get the current version of the key which should be used for encrypting new data 
 generates a key material 
 create a new key generating the material for it the given key must not already exist this implementation generates the key material and calls the link createkey string byte options method 
 roll a new version of the given key generating the material for it this implementation generates the key material and calls the link rollnewversion string byte method 
 find the provider with the given key 
 create a keyprovider based on a provided uri 
 parse the command line arguments and initialize the data hadoop key create keyname size size cipher algorithm provider providerpath hadoop key roll keyname provider providerpath hadoop key list provider providerpath hadoop key delete keyname provider providerpath i hadoop key invalidatecache keyname provider providerpath 
 this provider expects uris in the following form kms where proto   http or https authority   hosts   hostname   string port   integer this will always create a link loadbalancingkmsclientprovider if the uri is correct 
 constructor takes the following tunable configuration parameters
 this removes the value currently at the head of the queue for the provided key will immediately fire the queue filler function if key does not exist if queue exists but all values are drained it will ask the generator function to add  value to queue and then drain it 
 this removes the num values currently at the head of the queue for the provided key will immediately fire the queue filler function if key does not exist how many values are actually returned is governed by the syncgenerationpolicy specified by the user 
 aes ctr will consume all of the input data it requires enough space in the destination buffer to encrypt entire input buffer 
 aes ctr will consume all of the input data it requires enough space in the destination buffer to decrypt entire input buffer 
 return an opensslcipher object that implements the specified transformation 
 create an object for the given class and initialize it from conf 
 the main factory method for creating a file system get a file system for the uri s scheme and authority the scheme of the uri determines a configuration property name fs abstractfilesystem scheme impl whose value names the abstractfilesystem class the entire uri and conf is passed to the abstractfilesystem factory method 
 constructor to be called by subclasses 
 get the uri for the file system based on the given uri the path query part of the given uri is stripped out and default file system port is used to form the uri 
 return a set of server default configuration values based on path 
 return the fully qualified path of path f resolving the path through any internal symlinks or mount point
 the specification of this method matches that of link filecontext create path enumset options createopts exceptthat the path f must be fully qualified and the permission is absolute i e umask has been applied 
 the specification of this method matches that of link filecontext open path except that path f must be for thisfile system 
 the specification of this method matches that of link filecontext rename path path options rename except that pathf must be for this file system 
 the specification of this method matches that of link filecontext rename path path options rename except that pathf must be for this file system 
 the specification of this method matches that of link filecontext access path fsaction except that an unresolvedlinkexception may be thrown if a symlink is encountered in the path 
 the specification of this method matches that of link filecontext getfilelinkstatus path except that an unresolvedlinkexception may be thrown if a symlink is encountered in the path leading up to the final path component if the file system does not support symlinks then the behavior is equivalent to link abstractfilesystem getfilestatus path 
 the specification of this method matches that of link filecontext getfsstatus path except that path f must be for thisfile system 
 the specification of this method matches that of link filecontext liststatus path except that path f must be for thisfile system 
 the specification of this method matches that of link filecontext listlocatedstatus path except that path f must be for this file system in hdfs implementation the blocklocation of returned locatedfilestatus will have different formats for replicated and erasure coded file please refer to link filesystem getfileblocklocations filestatus long long for more details 
 set an xattr of a file or directory the name must be prefixed with the namespace followed by for example user attr refer to the hdfs extended attributes user documentation for details 
 perform a fallback read 
 keeps track of disk usage 
 skips over and discards n bytes of data from the input stream the skip method skips over some smaller number of bytes when reaching end of file before n bytes have been skipped the actual number of bytes skipped is returned if n is negative no bytes are skipped 
 seek to the given position in the stream the next read will be from that position this method does not allow seek past the end of the file this produces ioexception 
 opens an fsdatainputstream at the indicated path 
 set replication for an existing file implement the abstract setreplication of filesystem
 the src file is under fs and the dst is on the local disk copy it from fs control to the local dst name if src and dst are directories the copycrc parameter determines whether to copy crc files 
 skips over and discards n bytes of data from the input stream the skip method skips over some smaller number of bytes when reaching end of file before n bytes have been skipped the actual number of bytes skipped is returned if n is negative no bytes are skipped 
 seek to the given position in the stream the next read will be from that position this method does not allow seek past the end of the file this produces ioexception 
 opens an fsdatainputstream at the indicated path 
 set replication for an existing file implement the abstract setreplication of filesystem
 rename files dirs 
 implement the delete path boolean in checksum file system 
 remove the associated renew action from the queue
 return a string indicating which filesystem volume we re checking 
 return the filesystem mount point for the indicated volume 
 delete all the paths that were marked as delete on exit 
 get the file system of supplied path 
 create a filecontext for specified uri using the default config 
 create a filecontext for specified default uri using the specified config 
 create a filecontext using the passed config generally it is better to use link getfilecontext uri configuration instead of this one 
 set the working directory for wd relative names such a foo bar working directory feature is provided by simply prefixing relative names with the working dir note this is different from unix where the wd is actually set to the inode hence setworkingdir does not follow symlinks etc this works better in a distributed environment that has multiple independent roots link getworkingdirectory should return what setworkingdir set 
 resolve the path following any symlinks or mount points
 renames path src to path dst fails if src is a file and dst is a directory fails if src is a directory and dst is a file fails if the parent of dst does not exist or is a file if overwrite option is not passed as an argument rename fails if the dst already exists if overwrite option is passed as an argument rename overwrites the dst if it is a file or an empty directory rename fails if dst is a non empty directory note that atomicity of rename is dependent on the file system implementation please refer to the file system documentation for details 
 set the verify checksum flag for the file system denoted by the path this is only applicable if the corresponding filesystem supports checksum by default doesn t do anything 
 mark a path to be deleted on jvm shutdown 
 does the file exist note avoid using this method if you already have filestatus in hand instead reuse the filestatus 
 return the link contentsummary of path f 
 see link liststatus path pathfilter 
 filter files directories in the given path using the user supplied path filter 
 filter files directories in the given list of paths using user supplied path filter 
 list the statuses and block locations of the files in the given path if the path is a directory if recursive is false returns files in the directory if recursive is true return files in the subtree rooted at the path the subtree is traversed in the depth first order if the path is a file return the file s status and block locations files across symbolic links are also returned 
 returns true if the iterator has more files 
 process the input stat if it is a file return the file stat if it is a directory traverse the directory if recursive is true ignore it if recursive is false if it is a symlink resolve the symlink first and then process it depending on if it is a file or directory 
 returns the next file s status with its block locations
 copy file from src to dest see link copy path path boolean boolean 
 copy from src to dst optionally deleting src and overwriting dst 
 check if copying srcname to dst would overwrite an existing file or directory 
 get delegation tokens for the file systems accessed for a given path 
 set an xattr of a file or directory the name must be prefixed with the namespace followed by for example user attr refer to the hdfs extended attributes user documentation for details 
 create a snapshot with a default name 
 copy constructor 
 read instance encoded as protobuf from stream 
 write instance encoded as protobuf to stream 
 this method adds a filesystem instance to the cache so that it can be retrieved later it is only for testing 
 get a filesystem instance based on the uri the passed in configuration and the user 
 returns the configured filesystem implementation 
 canonicalize the given uri this is implementation dependent and may for example consist of canonicalizing the hostname using dns and adding the default port if not specified the default implementation simply fills in the default port if not specified and if link getdefaultport returns adefault port 
 deprecated call link get uri configuration instead 
 get the local filesystem 
 get a filesystem for this uri s scheme and authority if the configuration has the property code fs scheme impl disable cache set to true a new instance will be created initialized with the supplied uri and configuration then returned without being cached if the there is a cached fs instance matching the same uri it will be returned otherwise a new fs instance will be created initialized with the configuration and uri cached and returned to the caller 
 returns the filesystem for this uri s scheme and authority and the given user internally invokes link newinstance uri configuration 
 returns the filesystem for this uri s scheme and authority the entire uri is passed to the filesystem instance s initialize method this always returns a new filesystem object 
 returns a unique configured filesystem implementation for the default filesystem of the supplied configuration this always returns a new filesystem object 
 get a unique local filesystem object 
 close all cached filesystem instances after this operation they may not be used in any operations 
 close all cached filesystem instances for a given ugi be sure those filesystems are not used anymore 
 obtain all delegation tokens used by this filesystem that are not already present in the given credentials existing tokens will neither be verified as valid nor having the given renewer missing tokens will be acquired and added to the given credentials default impl works for simple fs with its own token and also for an embedded fs whose tokens are those of its child filesystems i e the embedded fs has no tokens of its own 
 recursively obtain the tokens for this filesystem and all descendant filesystems as determined by link getchildfilesystems 
 create a file with the provided permission the permission of the file is set to be the provided permission as in setpermission not permission umask the hdfs implementation is implemented using two rpcs it is understood that it is inefficient but the implementation is thread safe the other option is to change the value of umask in configuration to be  but it is not thread safe 
 create a directory with the provided permission the permission of the directory is set to be the provided permission as in setpermission not permission umask
 return an array containing hostnames offset and size of portions of the given file for a nonexistent file or regions code null is returned this call is most helpful with location aware distributed filesystems where it returns hostnames of machines that contain the given file a filesystem will normally return the equivalent result of passing the code filestatus of the path to link getfileblocklocations filestatus long long 
 return a set of server default configuration values 
 return the fully qualified path of path resolving the path through any symlinks or mount point 
 opens an fsdatainputstream at the indicated path 
 open an fsdatainputstream matching the pathhandle instance the implementation may encode metadata in pathhandle to address the resource directly and verify that the resource referenced satisfies constraints specified at its construciton 
 create an fsdataoutputstream at the indicated path files are overwritten by default 
 create an fsdataoutputstream at the indicated path 
 create an fsdataoutputstream at the indicated path with write progress reporting files are overwritten by default 
 create an fsdataoutputstream at the indicated path files are overwritten by default 
 create an fsdataoutputstream at the indicated path with write progress reporting files are overwritten by default 
 create an fsdataoutputstream at the indicated path 
 create an link fsdataoutputstream at the indicated pathwith write progress reporting the frequency of callbacks is implementation specific it may be none 
 create an fsdataoutputstream at the indicated path 
 create an fsdataoutputstream at the indicated path with write progress reporting 
 create an fsdataoutputstream at the indicated path with write progress reporting 
 create an fsdataoutputstream at the indicated path with a custom checksum option 
 this create has been added to support the filecontext that processes the permission with umask before calling this method this a temporary method added to support the transition from filesystem to filecontext for user applications 
 this version of the mkdirs method assumes that the permission is absolute it has been added to support the filecontext that processes the permission with umask before calling this method this a temporary method added to support the transition from filesystem to filecontext for user applications 
 this version of the mkdirs method assumes that the permission is absolute it has been added to support the filecontext that processes the permission with umask before calling this method this a temporary method added to support the transition from filesystem to filecontext for user applications 
 opens an fsdataoutputstream at the indicated path with write progress reporting same as create except fails if parent directory doesn t already exist 
 opens an fsdataoutputstream at the indicated path with write progress reporting same as create except fails if parent directory doesn t already exist 
 creates the given path as a brand new zero length file if create fails or if it already existed return false important the default implementation is not atomic
 append to an existing file optional operation same as code append f getconf getint io file buffer size key io file buffer size default null 
 append to an existing file optional operation same as append f buffersize null 
 get the replication factor 
 renames path src to path dst fails if src is a file and dst is a directory fails if src is a directory and dst is a file fails if the parent of dst does not exist or is a file if overwrite option is not passed as an argument rename fails if the dst already exists if overwrite option is passed as an argument rename overwrites the dst if it is a file or an empty directory rename fails if dst is a non empty directory note that atomicity of rename is dependent on the file system implementation please refer to the file system documentation for details this default implementation is non atomic this method is deprecated since it is a temporary method added to support the transition from filesystem to filecontext for user applications 
 delete a file directory 
 mark a path to be deleted when its filesystem is closed when the jvm shuts down cleanly all cached filesystem objects will be closed automatically these the marked paths will be deleted as a result if a filesystem instance is not cached i e has been created with link createfilesystem uri configuration then the paths will be deleted in when link close is called on that instance the path must exist in the filesystem at the time of the method call it does not have to exist at the time of jvm shutdown notes clean shutdown of the jvm cannot be guaranteed the time to shut down a filesystem will depends on the number of files to delete for filesystems where the cost of checking for the existence of a file directory and the actual delete operation for example object stores is high the time to shutdown the jvm can be significantly extended by over use of this feature connectivity problems with a remote filesystem may delay shutdown further and may cause the files to not be deleted 
 delete all paths that were marked as delete on exit this recursively deletes all files and directories in the specified paths the time to process this operation is code o paths with the actual time dependent on the time for existence and deletion operations to complete successfully or not 
 check if a path exists it is highly discouraged to call this method back to back with other link getfilestatus path calls as this will involve multiple redundantrpc calls in hdfs 
 true iff the named path is a directory note avoid using this method instead reuse the filestatus returned by getfilestatus or liststatus methods 
 true iff the named path is a regular file note avoid using this method instead reuse the filestatus returned by link getfilestatus path or liststatus methods 
 the number of bytes in a file 
 return the link contentsummary of a given link path 
 return the link quotausage of a given link path 
 given an opaque iteration token return the next batch of entries in a directory this is a private api not meant for use by end users this method should be overridden by filesystem subclasses that want to use the generic link filesystem liststatusiterator path implementation 
 filter files directories in the given path using the user supplied path filter results are added to the given array results 
 filter files directories in the given path using the user supplied path filter does not guarantee to return the list of files directories status in a sorted order 
 filter files directories in the given list of paths using default path filter does not guarantee to return the list of files directories status in a sorted order 
 filter files directories in the given list of paths using user supplied path filter does not guarantee to return the list of files directories status in a sorted order 
 list the statuses of the files directories in the given path if the path is a directory return the file s status and block locations if the path is a file if a returned status is a file it contains the file s block locations 
 list a directory the returned results include its block location if it is a file the results are filtered by the given path filter
 list the statuses and block locations of the files in the given path does not guarantee to return the iterator that traverses statuses of the files in a sorted order if the path is a directory if recursive is false returns files in the directory if recursive is true return files in the subtree rooted at the path if the path is a file return the file s status and block locations 
 call link mkdirs path fspermission with default permission 
 the src file is on the local disk add it to filesystem at the given dst name and the source is kept intact afterwards
 the src files is on the local disk add it to filesystem at the given dst name removing the source afterwards 
 the src file is on the local disk add it to the filesystem at the given dst name removing the source afterwards 
 the src file is on the local disk add it to the filesystem at the given dst name delsrc indicates if the source should be removed
 the src files are on the local disk add it to the filesystem at the given dst name delsrc indicates if the source should be removed
 the src file is on the local disk add it to the filesystem at the given dst name delsrc indicates if the source should be removed
 copy it a file from the remote filesystem to the local one 
 copy a file to the local filesystem then delete it from the remote filesystem if successfully copied 
 copy it a file from a remote filesystem to the local one delsrc indicates if the src will be removed or not 
 the src file is under this filesystem and the dst is on the local disk copy it from the remote filesystem to the local dst name delsrc indicates if the src will be removed or not userawlocalfilesystem indicates whether to use rawlocalfilesystem as the local file system or not rawlocalfilesystem is non checksumming so it will not create any crc files at local 
 called when we re all done writing to the target a local fs will do nothing because we ve written to exactly the right place a remote fs will copy the contents of tmplocalfile to the correct target at fsoutputfile 
 return the total size of all files in the filesystem 
 return the total size of all files from a specified path 
 get the block size for a particular file 
 checks if the user can access a path the mode specifies which access checks to perform if the requested permissions are granted then the method returns normally if access is denied then the method throws an link accesscontrolexception the default implementation calls link getfilestatus path and checks the returned permissions against the requested permissions note that the link getfilestatus path call will be subject toauthorization checks typically this requires search execute permissions on each directory in the path s prefix but this is implementation defined any file system that provides a richer authorization model such as acls may override the default implementation so that it checks against that model instead in general applications should avoid using this method due to the risk of time of check time of use race conditions the permissions on a file may change immediately after the access call returns most applications should prefer running specific file system actions as the desired user represented by a link usergroupinformation 
 see link filecontext getfilelinkstatus path 
 get the checksum of a file if the fs supports checksums 
 returns a status object describing the use and capacity of the filesystem if the filesystem has multiple partitions the use and capacity of the root partition is reflected 
 create a snapshot with a default name 
 set an xattr of a file or directory the name must be prefixed with the namespace followed by for example user attr refer to the hdfs extended attributes user documentation for details 
 get all the trash roots for current user or all users 
 create and initialize a new instance of a filesystem 
 the objects inserted into the cache using this method are all unique 
 get the fs instance if the key maps to an instance creating and initializing the fs if it is not found if this is the first entry in the map and the jvm is not shutting down this registers a shutdown hook to close filesystems and adds this fs to the code toautoclose set if code fs automatic close is set in the configuration default true 
 close all filesystems in the cache whether they are marked for automatic closing or not 
 close all filesystem instances in the cache 
 attempt calling overridden link docall path method withspecified link filesystem and link path if the call fails with an unresolvedlinkexception it will try to resolve the path and retry the call by calling link next filesystem path 
 copy files between filesystems 
 copy files between filesystems 
 copy files between filesystems 
 copy local files to a filesystem 
 copy filesystem files to local files 
 copy filesystem files to local files 
 convert a os native filename to a path that works for the shell 
 convert a os native filename to a path that works for the shell and avoids script injection attacks 
 convert a os native filename to a path that works for the shell 
 given a stream input it will unzip the it in the unzip directory passed as the second parameter
 given a file input it will unzip it in the unzip directory passed as the second parameter
 run a command and send the contents of an input stream to it 
 given a tar file as input it will untar the file in a the untar directory passed as the second parameter this utility will untar tar files and tar gz tgz files 
 given a tar file as input it will untar the file in a the untar directory passed as the second parameter this utility will untar tar files and tar gz tgz files 
 change the permissions on a filename 
 set the ownership on a file directory user name and group name cannot both be null 
 platform independent implementation for link file setreadable boolean file setreadable does not work as expected on windows 
 platform independent implementation for link file setwritable boolean file setwritable does not work as expected on windows 
 platform independent implementation for link file setexecutable boolean file setexecutable does not work as expected on windows note revoking execute permission on folders does not have the same behavior on windows as on unix platforms creating deleting or renaming a file within that folder will still succeed on windows 
 platform independent implementation for link file canread 
 platform independent implementation for link file canwrite 
 platform independent implementation for link file canexecute 
 set permissions to the required value uses the java primitives instead of forking if group    other 
 create a tmp file for a base file 
 move the src file to the name specified by target 
 create a jar file at the given path containing a manifest with a classpath that references all specified entries some platforms may have an upper limit on command line length for example the maximum command line length on windows is  characters but the length of the classpath may exceed this to work around this limitation use this method to create a small intermediate jar with a manifest that contains the full classpath it returns the absolute path to the new jar which the caller may set as the classpath for a new process environment variable evaluation is not supported within a jar manifest so this method expands environment variables before inserting classpath entries to the manifest the method parses environment variables according to platform specific syntax var on windows or var otherwise on windows environment variables are case insensitive for example var and var evaluate to the same value specifying the classpath in a jar manifest does not support wildcards so this method expands wildcards internally any classpath entry that ends with is translated to all files at that path with extension jar or jar 
 make sure that a path specifies a filesystem 
 close the underlying output stream 
 read one checksum verified byte
 read checksum verified bytes from this byte input stream into the specified byte array starting at the given offset this method implements the general contract of the corresponding link inputstream read byte int int read method of the link inputstream class as an additional convenience it attempts to read as many bytes as possible by repeatedly invoking the read method of the underlying stream this iterated read continues until one of the following conditions becomes true the specified number of bytes have been read the read method of the underlying stream returns  indicating end of file if the first read on the underlying stream returns  to indicate end of file then this method returns  otherwise this method returns the number of bytes actually read 
 fills the buffer with a chunk data no mark is supported this method assumes that all data in the buffer has already been read in hence pos  count 
 like read byte int int but does not provide a dest buffer so the read data is discarded 
 skips over and discards n bytes of data from the input stream this method may skip more bytes than are remaining in the backing file this produces no exception and the number of bytes skipped may include some number of bytes that were beyond the eof of the backing file attempting to read from the stream after skipping past the end will result in  indicating the end of the file if n is negative no bytes are skipped 
 seek to the given position in the stream the next read will be from that position this method may seek past the end of the file this produces no exception and an attempt to read from the stream will result in  indicating the end of the file 
 a utility function that tries to read up to len bytes from stm
 performs the operation specified by the next function calling it repeatedly until all symlinks in the given path are resolved 
 write one byte 
 writes len bytes from the specified byte array starting at offset off and generate a checksum for each data chunk this method stores bytes from the given array into this stream s buffer before it gets checksumed the buffer gets checksumed and flushed to the underlying output stream when all data in a checksum chunk are in the buffer if the buffer is empty and requested length is at least as large as the size of next checksum chunk size this method will checksum and write the chunk directly to the underlying output stream thus it avoids unnecessary data copy 
 write a portion of an array flushing to the underlying stream at most once if necessary 
 checksums all complete data chunks and flushes them to the underlying stream if there is a trailing partial chunk it is not flushed and is maintained in the buffer 
 generate checksums for the given data chunks and output chunks checksums to the underlying output stream 
 returns the trash object associated with this shell 
 returns the current trash location for the path specified
 run
 main has some simple utility methods
 connect to the ftp server using configuration parameters 
 a stream obtained via this call must be closed before using other apis of this class or else the invocation will block 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 expand globs in the given filepattern into a collection of file patterns so that in the expanded set no file pattern has a slash character in a curly bracket pair some examples of how the filepattern is expanded filepattern expanded file pattern a b a b a b a b p a b c d s pa bs pc ds a b c d e f a b c d e f a b c d e f a b e f c d e f a b b c d e f a b b a b c d a b e f a b c d a b c d 
 expand the leftmost outer curly bracket pair containing a slash character in filepattern 
 creates a glob filter with the specified file pattern 
 creates a glob filter with the specified file pattern and an user filter 
 creates a hardlink 
 creates hardlinks from multiple existing files within one parent directory into one target directory 
 retrieves the number of links to the specified file 
 initialize a har filesystem per har archive the archive home directory is the top level directory in the filesystem that contains the har archive be careful with this method you do not want to go on creating new filesystem instances per call to path getfilesystem the uri of har is har underlyingfsscheme host port archivepath or har archivepath this assumes the underlying filesystem to be used in case not specified 
 decode the raw uri to get the underlying uri
 get block locations from the underlying fs and fix their offsets and lengths 
 get filestatuses of all the children of a given directory this just reads through index file and reads line by line to get all statuses for children of a directory its a brute force way of getting all such filestatuses
 return the filestatus of files in har archive the permission returned are that of the archive index files the permissions are not persisted while creating a hadoop archive 
 returns a har input stream which fakes end of file it reads the index files to get the part file name and the size and start of the file 
 liststatus returns the children of a directory after looking up the index files 
 constructors for har input stream 
 get a path from the local fs this method should be used if the size of the file is not known apriori we go round robin over the set of disks via the configured dirs and return the first complete path where we could create the parent directory of the passed path 
 get a path from the local fs pass size as size unknown if not known apriori we round robin over the set of disks via the configured dirs and return the first complete path which has enough space 
 get a path from the local fs pass size as size unknown if not known apriori we round robin over the set of disks via the configured dirs and return the first complete path which has enough space 
 get a path from the local fs for reading we search through all the configured dirs for the file s existence and return the complete path to the file when we find one 
 get all of the paths that currently exist in the working directories 
 creates a temporary file in the local fs pass size as  if not known apriori we round robin over the set of disks via the configured dirs and select the first complete path which has enough space a file is created on this directory the file is guaranteed to go away when the jvm exits 
 get a path from the local fs if size is known we go round robin over the set of disks via the configured dirs and return the first complete path which has enough space if size is not known use roulette selection pick directories with probability proportional to their available space 
 creates a file on the local fs pass size as link localdirallocator size unknown if not known apriori weround robin over the set of disks via the configured dirs and return a file on the first path which has enough space the file is guaranteed to go away when the jvm exits 
 get a path from the local fs for reading we search through all the configured dirs for the file s existence and return the complete path to the file when we find one 
 get all of the paths that currently exist in the working directories 
 create a new path based on the child path resolved against the parent path 
 construct a path from a string path strings are uris but with unescaped elements and some additional normalization 
 returns a qualified path object 
 create and initialize a link fspermission from link datainput 
 create and initialize a link permissionstatus from link datainput 
 inheritdoc note returned list is not sorted in any given order due to reliance on java s link file list api 
 creates the specified directory hierarchy does not treat existence as an error 
 load file permission information unix symbol rwxrwxrwx sticky bit info to improve peformance give priority to native stat call first try get permission information by using native jni call then fall back to use non native processbuilder call in case native lib is not loaded or native call is not successful
 sets the link path s last modified time and last access time to the given valid times 
 return a filestatus representing the given path if the path refers to a symlink return a filestatus representing the link rather than the object the link refers to 
 public link filestatus methods delegate to this function which in turneither call the new link stat based implementation or the deprecatedmethods based on platform support 
 deprecated remains for legacy support should be removed when link stat gains support for windows and other operating systems 
 shutdown the connection pool and close all open connections 
 set configuration from ui 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convert the file information in lsentry to a link filestatus object 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 convenience method so that we don t open a new connection when using this method from within another method otherwise every api invocation incurs the overhead of opening closing a tcp connection 
 a stream obtained via this call must be closed before using other apis of this class or else the invocation will block 
 execute the command on the input path data commands can override to make use of the resolved filesystem 
 for each source path execute the command
 invokes the command handler the default behavior is to process options expand arguments and then process each argument run   link processoptions linkedlist  link processrawarguments linkedlist   link expandarguments linkedlist   link expandargument string  link processarguments linkedlist   link processargument pathdata    link processpathargument pathdata   link processpaths pathdata pathdata   link processpath pathdata  link processnonexistentpath pathdata most commands will chose to implement just link processoptions linkedlist and link processpath pathdata 
 allows commands that don t use paths to handle the raw arguments default behavior is to expand the arguments via link expandarguments linkedlist and pass the resulting list to link processarguments linkedlist 
 expands a list of arguments into link pathdata objects the defaultbehavior is to call link expandargument string on each elementwhich by default globs the argument the loop catches ioexceptions increments the error count and displays the exception 
 processes the command s list of expanded arguments link processargument pathdata will be invoked with each itemin the list the loop catches ioexceptions increments the error count and displays the exception 
 processes a link pathdata item calling link processpathargument pathdata or link processnonexistentpath pathdata on each item 
 this is the last chance to modify an argument before going into the possibly recursive link processpaths pathdata pathdata  link processpath pathdata loop ex ls and du use this toexpand out directories 
 iterates over the given expanded paths and invokes link processpath pathdata on each element if recursive is true will do a post visit dfs on directories 
 gets the directory listing for a path and invokes link processpaths pathdata pathdata 
 get a public static class field
 invokes static void registercommands commandfactory on the given class this method abstracts the contract between the factory and the command class do not assume that directly invoking registercommands on the given class will have the same effect 
 the last arg is expected to be a local path if only one argument is given then the destination will be the current directory 
 called with a source and target destination pair
 copies the source file to the target 
 if direct write is disabled copies the stream contents to a temporary file copying if the copy is successful the temporary file will be renamed to the real path else the temporary file will be deleted if direct write is enabled then creation temporary file is skipped 
 read a single byte from the stream 
 close the stream 
 returns the link filestatus from the link pathdata item if thecurrent options require links to be followed then the returned file status is that of the linked file 
 invokes static void registerexpression findexpressionfactory on the given class this method abstracts the contract between the factory and the expression class do not assume that directly invoking registerexpression on the given class will have the same effect 
 creates an instance of the requested link expression class 
 parse a list of arguments to to extract the link expression elements the input deque will be modified to remove the used elements 
 creates an object to wrap the given parameters as fields the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it
 looks up the file status for a path if the path doesn t exist then the status will be null
 creates an object to wrap the given parameters as fields the string used to create the path will be recorded since the path object does not return exactly the same string used to initialize it 
 updates the paths s file status
 returns a new pathdata with the given extension 
 returns a list of pathdata objects of the items contained in the given directory 
 creates a new object for a child entry in this directory
 expand the given path as a glob pattern non existent paths do not throw an exception because creation commands like touch and mkdir need to create them the stat field will be null if the path does not exist 
 construct a uri from a string with unescaped special characters that have non standard semantics e g a custom parsing is needed to prevent misbehavior 
 in case of the symlinks or mount points one has to move the appropriate trashbin in the actual volume of the path p being deleted hence we get the file system of the fully qualified resolved path and then move the path p to the trashbin in that volume 
 create inode tree from the specified mount table specified in config
 creates a new nfly instance 
 category read 
 iterate all available nodes in the proximity order to attempt repair of all filenotfound nodes 
 returns the closest non failing destination s result 
 initializes an nfly mountpoint in viewfs 
 called after a new filesystem instance is constructed 
 convenience constructor for apps to call directly
 get fsstatus for all viewfsmountpoints matching path for the given viewfilesystem say viewfilesystem has following mount points configured  hdfs nn host port sales mounted on dept sales  hdfs nn host port marketing mounted on dept marketing  hdfs nn host port eng usa mounted on dept eng usa  hdfs nn host port eng asia mounted on dept eng asia for the above config here is a sample list of paths and their matching mount points while getting fsstatus path description matching mountpoint root viewfilesystem lists all     mount points dept not a mount point but a valid     internal dir in the mount tree and resolved down to path dept sales matches a mount point  dept sales india path is over a valid mount point  and resolved down to dept sales dept eng not a mount point but a valid     internal dir in the mount tree and resolved down to path erp doesn t match or leads to or over any valid mount points none
 decode string representation of a value and check whether it s encoded if the given string begins with x or x it expresses a hexadecimal number if the given string begins with s or s base encoding is expected if the given string is enclosed in double quotes the inner string is treated as text otherwise the given string is treated as text 
 encode byte value to string representation with encoding values encoded as text strings are enclosed in double quotes while strings encoded as hexadecimal and base are prefixed with x and s respectively 
 create a new activestandbyelector object the elector is created by providing to it the zookeeper configuration the parent znode under which to create the znode and a reference to the callback interface the parent znode name must be the same for all service instances and different across services after the leader has been lost a new leader will be elected after the session timeout expires hence the app must set this parameter based on its needs for failure response time the session timeout must be greater than the zookeeper disconnect timeout and is recommended to be x that value to enable zookeeper to retry transient disconnections setting a very short session timeout may result in frequent transitions between active and standby states during issues like network outages gs pauses 
 utility function to ensure that the configured base znode exists this recursively creates the znode as well as all of its parents 
 clear all of the state held within the parent znode this recursively deletes everything within the znode as well as the parent znode itself it should only be used when it s certain that no electors are currently participating in the election 
 get data set by the active leader
 get a new zookeeper client instance protected so that test class can inherit and mock out the zookeeper instance
 sleep for the given number of milliseconds this is non static and separated out so that unit tests can override the behavior not to sleep 
 write the activebreadcrumb node indicating that this node may need to be fenced on failover 
 try to delete the activebreadcrumb node when gracefully giving up active status if this fails it will simply warn since the graceful release behavior is only an optimization 
 if there is a breadcrumb node indicating that another node may need fencing try to fence that node 
 waits for the next event from zookeeper to arrive 
 failover from service  to service  if the failover fails then try to failback 
 return a proxy to connect to the target ha service 
 returns a proxy to connect to the target ha service for health monitoring if link gethealthmonitoraddress is implemented to return a non nulladdress then this proxy will connect to that address otherwise the returned proxy defaults to using link getaddress which means this method s behavior is identical to link getproxy configuration int 
 build a powershell script to kill a java exe process in a remote machine 
 verify that the argument if given in the conf is parseable 
 wait until one of the following events another thread publishes the results of an attempt to become active using link recordactiveattempt activeattemptrecord the node enters bad health status the specified timeout elapses 
 request from graceful failover to cede active role causes this zkfc to transition its local node to standby then quit the election for the specified period of time after which it will rejoin iff it is healthy 
 coordinate a graceful failover to this node 
 coordinate a graceful failover this proceeds in several phases  pre flight checks ensure that the local node is healthy and thus a candidate for failover a determine the current active node if it is the local node no need to failover return success b get the other nodes a ask the other nodes to yield from election for a number of seconds b ask the active node to yield from the election for a number of seconds  allow the normal election path to run in other threads wait until we either become unhealthy or we see an election attempt recorded by the normal code path  allow the old active to rejoin the election so a future failback is possible 
 quote all of the active html characters in the given string as they are added to the buffer 
 return an output stream that quotes all of the output 
 load ssl properties from the ssl configuration 
 get the pathname to the webapps files 
 start the server does not wait for the server to start 
 bind using single configured port if findport is true we will try to bind after incrementing port till a free port is found 
 bind using port ranges keep on looking for a free port in the port range and throw a bind exception if no port in the configured range binds 
 open the main listener for the server
 checks the user has privileges to access to instrumentation servlets if hadoop security instrumentation requires admin is set to false default value it always returns true if hadoop security instrumentation requires admin is set to true it will check that if the current user is in the admin acls if the user is in the admin acls it returns true otherwise it returns false 
 used by child copy constructors 
 positions the reader before its nth value 
 read and return the next value in the file 
 return the nth value in the file 
 fast version of the link mapfile reader get writablecomparable writable method firstit checks the bloom filter for the existence of the key and only if present it performs the real get operation this yields significant performance improvements for get operations on sparsely populated files 
 
 
 write the data provided to the compression codec compressing no more than the buffer size less the compression overhead as specified during construction for each block each block contains the uncompressed length for the block followed by one or more length prefixed blocks of compressed data 
 this method reads a byte from the compressed stream whenever we need to read from the underlying compressed stream this method should be called instead of directly calling the read method of the underlying compressed stream this method does important record keeping to have the statistic that how many bytes have been read off the compressed stream 
 this method tries to find the marker passed to it as the first parameter in the stream it can find bit patterns of length    bits specifically this method is used in cbzipinputstream to find the end of block eob delimiter in the stream starting from the current position of the stream if marker is found the stream position will be at the byte containing the starting bit of the marker 
 returns the number of bytes between the current stream position and the immediate next bzip block marker 
 in continous reading mode this read method starts from the start of the compressed stream and end at the end of file by emitting un compressed data in this mode stream positioning is not announced and should be ignored in byblock reading mode this read method informs about the end of a bzip block by returning eob at this event the compressed stream position is also announced this announcement tells that how much of the compressed stream has been de compressed and read out of this class in between eob events the stream position is not updated 
 constructs a new cbzipoutputstream with specified blocksize attention the caller is resonsible to write the two bzip magic bytes bz to the specified stream prior to calling this constructor 
 create a link compressionoutputstream that will write to the given link outputstream with the given link compressor 
 create a link compressioninputstream that will read from the given link inputstream with the given link decompressor and return a stream for uncompressed data 
 creates compressioninputstream to be used to read off uncompressed data in one of the two reading modes i e continuous or blocked reading modes
 this method updates compressed stream position exactly when the client of this code has read off at least one byte passed any bzip end of block marker this mechanism is very helpful to deal with data level record boundaries please see constructor and next methods of org apache hadoop mapred linerecordreader as an example usage of this feature we elaborate it with an example in the following assume two different scenarios of the bzip compressed stream where m represent end of block n is line delimiter and represent compressed data m n n m n assume that end is right after m in the first case the reading will stop at n and there is no need to read one more line to see the reason of reading one more line in the next method is explained in linerecordreader while in the second example linerecordreader needs to read one more line till the second n now since bzipcodecs only update position at least one byte passed a maker so it is straight forward to differentiate between the two cases mentioned 
 create an output stream with a codec taken from the global codecpool 
 create an input stream with a codec taken from the global codecpool 
 a little test program 
 create a compression input stream that reads the decompressed bytes from the given stream 
 this method returns the current position in the stream 
 decompress the data gzip header deflate stream gzip trailer in the provided buffer 
 parse the gzip header assuming we re in the appropriate state in order to deal with degenerate cases e g user buffer is one byte long we copy some header bytes to another buffer filename comment and extra field bytes are simply skipped see http www ietf org rfc rfc txt for the gzip spec note that no version of gzip to date at least through       supports the fhcrc header crc flagbit instead the implementation treats it as a multi file continuation flag which it also doesn t support sun s jdk v   supports the header crc however and so do we 
 must be called by all methods which access fields to ensure that the data has been uncompressed 
 writes bytes from a datainput directly into the buffer 
 write to a file stream 
 overwrite an integer into the internal buffer note that this call can only be used to overwrite existing data in the buffer i e buffer count cannot be increased and dataoutputstream written cannot be increased 
 stores the item in the configuration with the given keyname 
 restores the object from the configuration 
 stores the array of items in the configuration with the given keyname 
 restores the array of objects from the configuration 
 decode with inputs and erasedindexes generates outputs how to prepare for inputs  create an array containing data units  parity units please note the data units should be first or before the parity units  set null in the array locations specified via erasedindexes to indicate they re erased and no data are to read from  set null in the array locations for extra redundant items as they re not necessary to read when decoding for example in rs   if only  unit is really erased then we have  extra items as redundant they can be set as null to indicate no data will be used from them for an example using rs   assuming sources d d d d d d and parities p p p d being erased we can and may want to use only  units like d d d d p p to recover d we will have inputs   null d d null d d d d p null p p erasedindexes    index of d into inputs array outputs   a writable buffer note for both inputs and outputs no mixing of on heap buffers and direct buffers are allowed if the coder option allow change inputs is set true false by default the content of input buffers may change after the call subject to concrete implementation 
 decode with inputs and erasedindexes generates outputs more see above 
 decode with inputs and erasedindexes generates outputs more see above note for both input and output ecchunks no mixing of on heap buffers and direct buffers are allowed 
 encode with inputs and generates outputs note for both inputs and outputs no mixing of on heap buffers and direct buffers are allowed if the coder option allow change inputs is set true false by default the content of input buffers may change after the call subject to concrete implementation anyway the positions of input buffers will move forward 
 encode with inputs and generates outputs more see above 
 encode with inputs and generates outputs more see above 
 current size of compressed data 
 finishing up the current block 
 get the compressed size of the block in progress 
 signaling the end of write to the block the block register will be called for registering the finished block 
 constructor
 close the bcfile writer attempting to use the writer after calling close is not allowed and may lead to undetermined results 
 create a meta block and obtain an output stream for adding data into the block there can only be one blockappender stream active at any time regular blocks may not be created after the first meta blocks the caller must call blockappender close to conclude the block creation 
 create a meta block and obtain an output stream for adding data into the block the meta block will be compressed with the same compression algorithm as data blocks there can only be one blockappender stream active at any time regular blocks may not be created after the first meta blocks the caller must call blockappender close to conclude the block creation 
 create a data block and obtain an output stream for adding data into the block there can only be one blockappender stream active at any time data blocks may not be created after the first meta blocks the caller must call blockappender close to conclude the block creation 
 finishing reading the block release all resources 
 constructor
 stream access to a meta block 
 stream access to a data block 
 have we reached the last chunk 
 how many bytes remain in the current chunk 
 check whether we reach the end of the stream 
 write out a chunk 
 write out a chunk that is a concatenation of the internal buffer plus user supplied data this will never be the last block 
 flush the internal buffer is this the last call to flushbuffer 
 adding a new key value pair to the tfile this is synonymous to append key  key length value  value length 
 adding a new key value pair to tfile 
 obtain an output stream for writing a key into tfile this may only be called when there is no active key appending stream or value appending stream 
 lazily loading the tfile index 
 get the first key in the tfile 
 get the last key in the tfile 
 if greater is true then returns the beginning location of the block containing the key strictly greater than input key if greater is false then returns the beginning location of the block greater than equal to the input key
 get the recordnum for the first key value pair in a compressed block whose byte offset in the tfile is greater than or equal to the specified offset 
 get a sample key that is within a block whose starting offset is greater than or equal to the specified offset 
 get a scanner than can scan the whole tfile 
 get a scanner that covers a portion of tfile based on byte offsets 
 get a scanner that covers a portion of tfile based on keys 
 get a scanner that covers a portion of tfile based on keys 
 get a scanner that covers a specific key range 
 get a scanner that covers a specific key range 
 create a scanner that covers a range of records 
 constructor
 constructor
 move the cursor to the first entry whose key is greater than or equal to the input key synonymous to seekto key  key length the entry returned by the previous entry call will be invalid 
 move the cursor to the first entry whose key is greater than or equal to the input key the entry returned by the previous entry call will be invalid 
 move the cursor to the new location the entry returned by the previous entry call will be invalid 
 rewind to the first entry in the scanner the entry returned by the previous entry call will be invalid 
 seek to the end of the scanner the entry returned by the previous entry call will be invalid 
 move the cursor to the first entry whose key is greater than or equal to the input key synonymous to lowerbound key  key length the entry returned by the previous entry call will be invalid 
 move the cursor to the first entry whose key is greater than or equal to the input key the entry returned by the previous entry call will be invalid 
 move the cursor to the first entry whose key is strictly greater than the input key synonymous to upperbound key  key length the entry returned by the previous entry call will be invalid 
 move the cursor to the first entry whose key is strictly greater than the input key the entry returned by the previous entry call will be invalid 
 move the cursor to the next key value pair the entry returned by the previous entry call will be invalid 
 load a compressed block for reading expecting blockindex is valid 
 close the scanner release all resources the behavior of using the scanner after calling close is not defined the entry returned by the previous entry call will be invalid 
 get an entry to access the key and value 
 get the recordnum corresponding to the entry pointed by the cursor 
 internal api comparing the key at cursor to user specified key 
 copy the key and value in one shot into byteswritables this is equivalent to getkey key getvalue value 
 copy the key into byteswritable the input byteswritable will be automatically resized to the actual key size 
 copy the value into byteswritable the input byteswritable will be automatically resized to the actual value size the implementation directly uses the buffer inside byteswritable for storing the value the call does not require the value length to be known 
 writing the key to the output stream this method avoids copying key buffer from scanner into user buffer then writing to the output stream 
 writing the value to the output stream this method avoids copying value data from scanner into user buffer then writing to the output stream it does not require the value length to be known 
 copy the key into user supplied buffer 
 copy value into user supplied buffer user supplied buffer must be large enough to hold the whole value the value part of the key value pair pointed by the current cursor is not cached and can only be examined once calling any of the following functions more than once without moving the cursor will result in exception link getvalue byte link getvalue byte int link getvaluestream 
 copy value into user supplied buffer user supplied buffer must be large enough to hold the whole value starting from the offset the value part of the key value pair pointed by the current cursor is not cached and can only be examined once calling any of the following functions more than once without moving the cursor will result in exception link getvalue byte link getvalue byte int link getvaluestream 
 advance cursor by n positions within the block 
 advance cursor in block until we find a key that is greater than or equal to the input key 
 for reading from file 
 encoding an integer into a variable length encoding format synonymous to utils writevlong out n 
 encoding a long integer into a variable length encoding format if n in   encode in one byte with the actual value otherwise if n in  ^  ^ encode in two bytes byte    n   byte   n  otherwise if n in  ^  ^ encode in three bytes byte   n ^  byte    n  byte   n  otherwise if n in  ^  ^ encode in four bytes byte   n ^  byte    n  byte    n  byte   n  otherwise if n in ^ ^ encode in five bytes byte     byte    n  byte    n  byte    n  byte   n  if n in ^ ^ encode in six bytes byte     byte    n  byte    n  byte    n  byte    n  byte   n  if n in ^ ^ encode in seven bytes byte     byte    n  byte    n  byte    n  byte    n  byte    n  byte   n  if n in ^ ^ encode in eight bytes byte     byte    n  byte    n  byte    n  byte    n  byte    n  byte    n  byte   n  if n in ^ ^ encode in nine bytes byte     byte    n  byte    n  byte    n  byte    n  byte    n  byte    n  byte    n  byte   n  
 decoding the variable length integer synonymous to int utils readvlong in 
 decoding the variable length integer suppose the value of the first byte is fb and the following bytes are nb if fb    return long fb if fb in   return fb if fb in   return fb if fb in   return fb if fb in   return interpret nb fb as a signed big endian integer 
 write a string as a vint n followed by n bytes as in text format 
 read a string as a vint n followed by n bytes in text format 
 construct the version object by reading from the input stream 
 write the objec to a dataoutput the serialized format of the version is major version followed by minor version both as big endian short integers 
 copies from one stream to another 
 copies from one stream to another 
 copies from one stream to another closes the input and output streams at the end 
 copies from one stream to another 
 copies count bytes from one stream to another 
 utility wrapper for reading from link inputstream it catches any errors thrown by the underlying stream either io or decompression related and re throws as an ioexception 
 reads len bytes in a loop 
 similar to readfully skips bytes in a loop 
 close the closeable objects and ignore any link throwable ornull pointers must only be used for cleanup in exception handlers 
 close the closeable objects and ignore any link throwable ornull pointers must only be used for cleanup in exception handlers 
 closes the socket ignoring link ioexception 
 write a bytebuffer to a writablebytechannel handling short writes 
 write a bytebuffer to a filechannel at a given offset handling short writes 
 return the complete list of files in a directory as strings this is better than file listdir because it does not ignore ioexceptions 
 ensure that any writes to the given file is written to the storage device that contains it this method opens channel on given file and closes it once the sync is done borrowed from uwe schindler in lucene 
 ensure that any writes to the given file is written to the storage device that contains it this method opens channel on given file and closes it once the sync is done borrowed from uwe schindler in lucene 
 takes an ioexception file directory path and method name and returns an ioexception with the input exception as the cause and also include the file method details the new exception provides the stack trace of the place where the exception is thrown and some extra diagnostics information return instance of same exception if exception class has a public string constructor otherwise return an pathioexception interruptedioexception and pathioexception are returned unwrapped 
 append a key value pair to the map the key must be greater or equal to the previous key added to the map 
 get the key at approximately the middle of the file or null if the file is empty 
 reads the final key from the file 
 positions the reader at the named key or if none such exists at the first entry after the named key returns true iff the named key exists in this map 
 positions the reader at the named key or if none such exists at the first entry after the named key 
 positions the reader at the named key or if none such exists at the key that falls just before or just after dependent on how the before parameter is set 
 finds the record that is the closest match to the specified key returns key or if it does not exist at the first entry after the named key param key key that we re trying to find param val data value if key is found return the key that was the closest match or null if eof 
 finds the record that is the closest match to the specified key 
 merge multiple mapfiles to one mapfile
 merge all input files to output map file  read first key value from all input files to keys values array  select the least key and corresponding value  write the selected key and value to output file  replace the already written key value in keys values arrays with the next key value from the selected input  repeat step   till all keys are read 
 constructs reads and returns an instance 
 construct a hash value for the content from the inputstream 
 call posix fadvise on the given file descriptor see the manpage for this syscall for more information on systems where this call is not available does nothing 
 call sync file range on the given file descriptor see the manpage for this syscall for more information on systems where this call is not available does nothing 
 locks the provided direct bytebuffer into memory preventing it from swapping out after a buffer is locked future accesses will not incur a page fault see the mlock  man page for more information 
 returns the file stat for a file descriptor 
 return the file stat for a file path 
 create a directory with permissions set to the specified mode by setting permissions at creation time we avoid issues related to the user lacking write dac rights on subsequent chmod calls one example where this can occur is writing to an smb share where the user does not have full control rights and therefore write dac is denied 
 create a file for write with permissions set to the specified mode by setting permissions at creation time we avoid issues related to the user lacking write dac rights on subsequent chmod calls one example where this can occur is writing to an smb share where the user does not have full control rights and therefore write dac is denied this method mimics the semantics implemented by the jdk in link java io fileoutputstream the file is opened for truncate or append the sharing mode allows other readers and writers and paths longer than max path are supported see io util md c in the jdk 
 create a filedescriptor that shares delete permission on the file opened at a given offset i e other process can delete the file the filedescriptor is reading only windows implementation uses the native interface 
 create the specified file for write access ensuring that it does not exist 
 a version of renameto that throws a descriptive exception when it fails 
 creates a hardlink dst that points to src this is deprecated since jdk nio can create hardlinks via the link java nio file files api 
 unbuffered file copy from src to dst without tainting os buffer cache in posix platform it uses filechannel transferto which internally attempts unbuffered io on os with native sendfile support and falls back to buffered io otherwise it minimizes the number of filechannel transferto call by passing the the src file size directly instead of a smaller size as the rd parameter this saves the number of sendfile system call when native sendfile is supported in the two fall back cases where sendfile is not supported filechannle transferto already has its own batching of size  mb and  kb respectively in windows platform it uses its own native wrapper of copyfileex with copy file no buffering flag which is supported on windows server  and above ideally we should use filechannel transferto across both posix and windows platform unfortunately the wrapper java sun nio ch filechannelimpl transferto used by filechannel transferto for unbuffered io is not implemented on windows based on openjdk    source code java sun nio ch filechannelimpl transferto on windows simply returns ios unsupported note this simple native wrapper does minimal parameter checking before copy and consistency check e g size after copy it is recommended to use wrapper function like the storage nativecopyfileunbuffered function in hadoop hdfs with pre post copy checks 
 create a new sharedfiledescriptorfactory 
 create a shared file descriptor which will be both readable and writable 
 write a link writable link string primitive type or an array of the preceding 
 write a link writable link string primitive type or an array of the preceding 
 read a link writable link string primitive type or an array of the preceding 
 read a link writable link string primitive type or an array of the preceding 
 try to instantiate a protocol buffer of the given message class from the given input stream 
 find and load the class with given name classname by first finding it in the specified conf if the specified conf is null try load it directly 
 writes bytes from a inputstream directly into the buffer 
 invoke the call once without retrying 
 parse the i th element as an integer 
 open the given file for random read access verifying the expected user group constraints if security is enabled note that this function provides no additional security checks if hadoop security is disabled since doing the checks would be too expensive when native libraries are not available 
 same as openforrandomread except that it will run even if security is off this is used by unit tests 
 opens the link fsdatainputstream on the requested file on local filesystem verifying the expected user group constraints if security is enabled 
 same as openfsdatainputstream except that it will run even if security is off this is used by unit tests 
 open the given file for read access verifying the expected user group constraints if security is enabled note that this function provides no additional checks if hadoop security is disabled since doing the checks would be too expensive when native libraries are not available 
 same as openforread except that it will run even if security is off this is used by unit tests 
 open the specified file for write access ensuring that it does not exist 
 create a new writer with the given options 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of sequencefile writer 
 construct the preferred type of raw sequencefile writer 
 construct the preferred type of raw sequencefile writer 
 construct a uncompressed writer from a set of options 
 create the named file 
 create the named file with write progress reporter 
 create the named file with write progress reporter 
 write and flush the file header 
 initialize 
 append a key value pair 
 append a key value pair 
 append a key value pair 
 append a key value pair 
 workhorse to check and write out compressed data lengths 
 compress and flush contents to dfs 
 close the file 
 append a key value pair 
 append a key value pair 
 common work of the constructors 
 initialize the link reader 
 read the next compressed block 
 position vallenin valin to the value corresponding to the current key 
 get the value corresponding to the last read key 
 get the value corresponding to the last read key 
 read the next key value pair in the file into key and val returns true if such a pair exists and false when at end of file 
 deprecated call link nextraw dataoutputbuffer sequencefile valuebytes 
 read raw records 
 read raw keys 
 read the next key in the file skipping its value return null at end of file 
 read raw values 
 seek to the next sync mark past a given position 
 perform a file sort from a set of input files into an output file 
 perform a file sort from a set of input files and return an iterator 
 the backwards compatible interface to sort 
 merges the list of segments of type segmentdescriptor
 merges the contents of files passed in path using a max factor value that is already set
 merges the contents of files passed in path 
 merges the contents of files passed in path 
 clones the attributes like compression of the input file and creates a corresponding writer
 writes records from rawkeyvalueiterator into a file represented by the passed writer
 merge the provided files 
 sort calls this to generate the final merged output 
 used by mergepass to merge the output of the sort
 this is the single level merge that is called multiple times depending on the factor size and the number of segments
 fills up the rawkey object with the key returned by the reader
 fills up the passed rawvalue with the value corresponding to the key read earlier
 closes the underlying reader 
 the default cleanup subclasses can override this with a custom cleanup 
 the default cleanup subclasses can override this with a custom cleanup 
 append a key to a set the key must be strictly greater than the previous key added to the set 
 read the next key in a set into key returns true if such a key exists and false when at the end of the set 
 read the matching key from a set into key returns key or null if no match exists 
 read the short value 
 write short value 
 finds any occurrence of what in the backing buffer starting as position start the starting position is measured in bytes and the return value is in terms of byte position in the buffer the backing buffer is not converted to a string for this operation 
 set to contain the contents of a string 
 convert text back to string
 deserialize 
 read a text object whose length is already known this allows creating text from a stream which uses a different serialization format 
 serialize write this object to out length uses zero compressed encoding
 converts the provided byte array to a string using the utf  encoding if the input is malformed replace by a default value 
 converts the provided byte array to a string using the utf  encoding if replace is true then malformed input is replaced with the substitution character which is ufffd otherwise the method throws a malformedinputexception 
 converts the provided string to bytes using the utf  encoding if the input is malformed invalid chars are replaced by a default value 
 converts the provided string to bytes using the utf  encoding if replace is true then malformed input is replaced with the substitution character which is ufffd otherwise the method throws a malformedinputexception 
 read a utf encoded string from in
 read a utf encoded string with a maximum size
 write a utf encoded string to out
 write a utf encoded string with a maximum size to out
 check if a byte array contains valid utf 
 set to contain the contents of a string 
 skips over one utf in the input 
 convert to a string 
 convert to a string checking for valid utf 
 convert a string to a utf  encoded byte array 
 convert a utf  encoded byte array back into a string 
 read a utf  encoded string 
 write a utf  encoded string 
 force initialization of the static members as of java  referencing a class doesn t force it to initialize since this class requires that the classes be initialized to declare their comparators we force that initialization to happen 
 reads a zero compressed encoded integer from a byte array and returns it 
 serializes an integer to a binary stream with zero compressed encoding for    i    only one byte is used with the actual value for other values of i the first byte value indicates whether the integer is positive or negative and the number of bytes that follow if the first byte value v is between  and  the following integer is positive with number of bytes that follow are v if the first byte value v is between  and  the following integer is negative with number of bytes that follow are v bytes are stored in the high non zero byte first order 
 serializes a long to a binary stream with zero compressed encoding for    i    only one byte is used with the actual value for other values of i the first byte value indicates whether the long is positive or negative and the number of bytes that follow if the first byte value v is between  and  the following long is positive with number of bytes that follow are v if the first byte value v is between  and  the following long is negative with number of bytes that follow are v bytes are stored in the high non zero byte first order 
 reads a zero compressed encoded long from input stream and returns it 
 reads a zero compressed encoded integer from input stream and returns it 
 reads an integer from the input stream and returns it this function validates that the integer is between lower upper inclusive 
 skip len number of bytes in input streamin
 read a string but check it for sanity the format consists of a vint followed by the given number of bytes 
 insert e into the backing queue or block until we can if client backoff is enabled this method behaves like add which throws if the queue overflows if we block and the queue changes on us we will insert while the queue is drained 
 retrieve an e from the backing queue or block until we can guaranteed to return an element from the current queue 
 checks if queue is empty by checking at checkpoint num points with checkpoint interval ms interval this doesn t mean the queue might not fill up at some point later but it should decrease the probability that we lose a call this way 
 cleanup executor on which ipc calls parameters are sent if reference counter is zero this method discards the instance of the executor if not this method just decrements the internal reference counter 
 read a byte from the stream send a ping if timeout on read retries if no failure is detected until a byte is read 
 read bytes into a buffer starting from offset off send a ping if timeout on read retries if no failure is detected until a byte is read 
 if multiple clients with the same principal try to connect to the same server at the same time the server assumes a replay attack is in progress this is a feature of kerberos in order to work around this what is done is that the client backs off randomly and tries to initiate the connection again the other problem is to do with ticket expiry to handle that a relogin is attempted 
 connect to the server and set up the io streams it then sends a header to the server and starts the connection thread that waits for responses 
 write the connection header this is sent when connection is established    hrpc  bytes     version  byte     service class  byte     authprotocol  byte   
 initiates a rpc call by sending the rpc request to the remote server note this is not called from the connection thread but by other threads 
 make a call passing rpcrequest to the ipc server defined by remoteid returning the rpc respond 
 make a call passing rpcrequest to the ipc server defined by remoteid returning the rpc response 
 get a connection from the pool or create a new one and add it to the pool connections to a given connectionid are reused 
 returns the priority level for a given identity by first trying the cache then computing it 
 put the element in a queue of a specific priority 
 this is the client side invoker of rpc method it only throws serviceexception since the invocation proxy expects only serviceexception to be thrown by the method in case protobuf service serviceexception has the following causes exceptions encountered on the client side in this method are set as cause in serviceexception as is exceptions from the server are wrapped in remoteexception and are set as cause in serviceexception note that the client calling protobuf rpc methods must handle serviceexception by getting the cause from the serviceexception if the cause is remoteexception then unwrap it to get the exception thrown by the server 
 check if a method is supported by the server or not
 get a server protocol s signature
 if this remote exception wraps up one of the lookuptypes then return this exception unwraps any ioexception 
 instantiate and return the exception wrapped up by this remote exception this unwraps any throwable that has a constructor taking a string as a parameter otherwise it returns this 
 this method handles the following conditions if retry is not to be processed return null if there is no cache entry add a new entry code newentry and returnit if there is an existing entry wait for its completion if the completion state is link cacheentry failed the expectation is that the thread that waited for completion retries the request the link cacheentry state is set to link cacheentry inprogress again if the completion state is link cacheentry success the entry is returned so that the thread that waits for it can can return previous response 
 get the protocol version from protocol class if the protocol class has a protocolannotation then get the protocol version from the annotation otherwise get it from the versionid field of the protocol class 
 get a proxy connection to a remote server
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 get a proxy connection to a remote server
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 get a proxy connection to a remote server
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 construct a client side proxy that implements the named protocol talking to a server at the named address 
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 construct a client side proxy object with the default socketfactory
 get a protocol proxy that contains a proxy connection to a remote server and a set of methods that are supported by the server
 stop the proxy proxy must either implement link closeable or must haveassociated link rpcinvocationhandler 
 returns whether the given method is supported or not the protocol signatures are fetched and cached the connection id for the proxy provided is re used 
 a convenience method to bind to a given address and report better exceptions if the address is not a valid host 
 send a deferred response ignoring errors 
 updating the readselector while it s being used is not thread safe so the connection must be queued the reader will drain the queue and update its readselector before performing the next select
 process saslmessage and send saslresponse back
 process a saslmessge 
 this method reads in a non blocking fashion from the channel this method is called repeatedly when data is present in the channel when it has enough data to process one rpc it processes that rpc on the first pass it processes the connectionheader connectioncontext an outofband rpc and at most one rpc request that follows that on future passes it will process at most one rpc request quirky things datalengthbuffer  bytes is used to read hrpc or rpc request length 
 process the sasl s negotiate request including the optimization of accelerating token negotiation 
 try to set up the response to indicate that the client version is incompatible with the server this can contain special case code to speak enough of past ipc protocols to pass back an exception to the caller 
 reads the connection context following the connection header
 process a wrapped rpc request unwrap the sasl packet and process each embedded rpc request 
 process one rpc request from buffer read from socket stream decode rpc in a rpc call handle out of band rpc requests such as the initial connectioncontext a successfully decoded rpccall will be deposited in rpc q and its response will be sent later when the request is processed prior to this call the connectionheader hrpc has been handled and if sasl then sasl has been established and the buf we are passed has been unwrapped from sasl 
 process an rpc request the connection headers and context must have been already read based on the rpckind decode the rpcrequest a successfully decoded rpccall will be deposited in rpc q and its response will be sent later when the request is processed 
 establish rpc connection setup by negotiating sasl if required then reading and authorizing the connection header
 authorize proxy users to access this server
 constructs a server listening on the named port and address parameters passed must be of the named class the handlercount determines the number of handler threads that will be used to process calls if queuesizeperhandler or numreaders are not  they will be used instead of parameters from configuration otherwise the configuration will be picked up if rpcrequestclass is null then the rpcrequestclass must have been registered via link registerprotocolengine rpc rpckind class rpc rpcinvoker this parameter has been retained for compatibility with existing tests and usage 
 setup response for the ipc call 
 setup response for the ipc call on fatal error from a client that is using old version of hadoop the response is serialized using the previous protocol s response layout 
 wait for the server to be stopped does not wait for all subthreads to finish see link stop 
 called for each call 
 authorize the incoming client connection 
 this is a wrapper around link writablebytechannel write bytebuffer if the amount of data is large it writes to channel in smaller chunks this is to avoid jdk from creating many direct buffers as the size of buffer increases this also minimizes extra copies in nio layer as a result of multiple write operations required to write a large buffer 
 this is a wrapper around link readablebytechannel read bytebuffer if the amount of data is large it writes to channel in smaller chunks this is to avoid jdk from creating many direct buffers as the size of bytebuffer increases there should not be any performance degredation 
 helper for link channelread readablebytechannel bytebuffer and link channelwrite writablebytechannel bytebuffer only one of readch or writech should be non null 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 process a get request for the specified resource 
 convert an event to json
 convert an event to json
 send http https request to the daemon 
 send http https request to get log level 
 send http https request to set log level 
 connect to the url supports http https and supports spnego authentication it falls back to simple authentication if it fails to initiate spnego 
 configures the client to send http https request to the url supports spengo for authentication 
 consume one element will block if queue is empty only one consumer at a time is allowed
 consume all the elements will block if queue is empty
 dequeue one element from head of the queue will block if queue is empty
 change the declared field code field in code source object to link mutablemetric 
 initialize the registry with all the methods in a protocol so they all show up in the first snapshot convenient for jmx implementations 
 sends ganglia metrics to the configured hosts
 initialize the connection to hdfs and create the base directory also launch the flush thread 
 extract the roll interval from the configuration and return it in milliseconds 
 return the supplied file system for testing or otherwise get a new file system 
 create a new directory based on the current interval and a new log file in that directory 
 create a new log file and return the link fsdataoutputstream if a file with the specified path already exists add a suffix starting with  and try again keep incrementing the suffix until a nonexistent target path is found once the file is open update link currentfsoutstream link currentoutstream and link currentfilepath are setappropriately 
 create a new log file and return the link fsdataoutputstream if a file with the specified path already exists open the file for append instead once the file is open update link currentfsoutstream link currentoutstream and link currentfilepath 
 register the mbean using our standard mbeanname format hadoop service  name  where the and are the supplied parameters
 returns the hostname associated with the specified ip address by the provided nameserver loopback addresses 
 return networkinterface for the given subinterface name eg eth  or null if no interface with the given name can be found 
 like link dns getips string boolean but returns allips associated with the given interface and its subinterfaces 
 returns all the ips associated with the provided interface if any in textual form 
 returns the first available ip address associated with the provided network interface or the local host ip if default is given 
 returns all the host names associated by the provided nameserver with the address bound to the specified network interface
 determine the local hostname retrieving it from cache if it is known if we cannot determine our host name return localhost 
 get the ipaddress of the local host as a string this will be a loop back value if the local host address cannot be determined if the loopback address of localhost does not resolve then the system s network is in such a state that nothing is going to work a message is logged at the error level and a null pointer returned a pointer which will trigger failures later on the application
 returns all the host names associated by the default nameserver with the address bound to the specified network interface
 returns the default first host name associated by the provided nameserver with the address bound to the specified network interface
 returns the default first host name associated by the default nameserver with the address bound to the specified network interface
 returns the default first host name associated by the provided nameserver with the address bound to the specified network interface 
 returns all the ips associated with the provided interface if any as a list of inetaddress objects 
 create a socket address with the given host and port the hostname might be replaced with another host that was set via link addstaticresolution string string the value of hadoop security token service use ip will determine whether the standard java host resolver is used or if the fully qualified resolver is used 
 resolve the uri s hostname and add the default port if not in the uri
 returns an inetsocketaddress that a client can use to connect to the given listening address 
 same as getinputstream socket socket getsotimeout 
 return a link socketinputwrapper for the socket and set the giventimeout if the socket does not have an associated channel then its socket timeout will be set to the specified value otherwise a link socketinputstream will be created which reads with the configuredtimeout any socket created using socket factories returned by link netutils must use this interface instead of link socket getinputstream in general this should be called only once on each socket see the note in link socketinputwrapper settimeout long for more information 
 same as getoutputstream socket  timeout of zero implies write will wait until data is available from documentation for link getoutputstream socket long returns outputstream for the socket if the socket has an associated socketchannel then it returns a link socketoutputstream with the given timeout if the socket does nothave a channel link socket getoutputstream is returned in the latercase the timeout argument is ignored and the write will wait until data is available any socket created using socket factories returned by link netutils must use this interface instead of link socket getoutputstream 
 returns outputstream for the socket if the socket has an associated socketchannel then it returns a link socketoutputstream with the given timeout if the socket does nothave a channel link socket getoutputstream is returned in the latercase the timeout argument is ignored and the write will wait until data is available any socket created using socket factories returned by link netutils must use this interface instead of link socket getoutputstream 
 this is a drop in replacement for link socket connect socketaddress int in the case of normal sockets that don t have associated channels this just invokes socket connect endpoint timeout if socket getchannel returns a non null channel connect is implemented using hadoop s selectors this is done mainly to avoid sun s connect implementation from creating thread local selectors since hadoop does not have control on when these are closed and could end up taking all the available file descriptors 
 like link netutils connect socket socketaddress int butalso takes a local address and port to bind the socket to 
 given a string representation of a host return its ip address in textual presentation 
 performs a sanity check on the list of hostnames ips to verify they at least appear to be valid 
 return hostname without throwing exception the returned hostname string format is hostname 
 return hostname without throwing exception the returned hostname string format is hostname ip address 
 checks if code host is a local host name and return link inetaddress corresponding to that address 
 given an inetaddress checks to see if the address is a local address by comparing the address with all the interfaces on the node 
 return an inetaddress for each interface that matches the given subnet specified using cidr notation 
 create a new input stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 same as socketinputstream socket getchannel socket getsotimeout create a new input stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 set the timeout for reads from this stream note the behavior here can differ subtly depending on whether the underlying socket has an associated channel in particular if there is no channel then this call will affect the socket timeout for all readers of this socket if there is a channel then this call will affect the timeout only for this stream as such it is recommended to only create one link socketinputwrapper instance per socket 
 performs one io and returns number of bytes read or written it waits up to the specified timeout if the channel is not read before the timeout sockettimeoutexception is thrown 
 the contract is similar to link socketchannel connect socketaddress with a timeout 
 this is similar to link doio bytebuffer int except that itdoes not perform any io it just waits for the channel to be ready for io as specified in ops 
 waits on the channel with the given timeout using one of the cached selectors it also removes any cached selectors that are idle for a few seconds 
 takes one selector from end of lru list of free selectors if there are no selectors awailable it creates a new selector also invokes trimidleselectors 
 create a new ouput stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 transfers data from filechannel using link filechannel transferto long long writablebytechannel updates waitforwritabletime and transfertotime with the time spent blocked on the network and the time spent transferring data from disk to network respectively similar to readfully this waits till requested amount of data is transfered 
 call link transfertofully filechannel long int mutablerate mutablerate with null waitforwritabletime and transfertotime
 set the proxy of this socket factory as described in the string parameter
 create a new domainsocket listening on the given path 
 create a pair of unix domain sockets which are connected to each other by calling socketpair  
 accept a new unix domain connection this method can only be used on sockets that were bound with bind 
 create a new domainsocket connected to the given path 
 close the socket 
 call shutdown shut rdwr on the unix domain socket 
 send some filedescriptor objects to the process on the other side of this socket 
 receive some filedescriptor objects from the process on the other side of this socket and wrap them in fileinputstream objects 
 open up and initialize the keystore 
 this method provide the ability to do pre post tasks in filter chain override this method to authorize proxy user between authenticationfilter and next filter 
 this function is kept to provide backward compatibility 
 convenience method for reading a token storage file and loading its tokens 
 convenience method for reading a token storage file and loading its tokens 
 convenience method for reading a token from a datainputstream 
 stores all the keys to dataoutput 
 loads all the keys 
 this method will block if a cache entry doesn t exist and any subsequent requests for the same user will wait on this request to return if a user already exists in the cache and when the key expires the first call to reload the key will block but subsequent requests will return the old value until the blocking thread returns if reloadgroupsinbackground is true then the thread that needs to refresh an expired key will not block either instead it will return the old cache value and schedule a background refresh
 override the reload method to provide an asynchronous implementation if reloadgroupsinbackground is false then this method defers to the super implementation otherwise is arranges for the cache to be updated later
 handles an link httpinteraction by applying the filtering logic 
 refresh the netgroup cache
 execute diagnostics things it would be nice if ugi made accessible a way to enable jaas debug programatically access to the tgt 
 fail fast on a jvm without jce installed this is a recurrent problem that is it keeps creeping back with jvm updates a fast failure is the best tactic 
 locate the code krb conf file and dump it no op on windows 
 dump a keytab list all principals 
 log in from a keytab dump the ugi validate it then try and log in again that second time login catches jvm hadoop compatibility problems 
 validate any jaas entry referenced in the link sun security jaas file property 
 dump any file to standard out 
 main entry point 
 returns list of groups for a user the ldapctx which underlies the dircontext object is not thread safe so we need to block around this whole method the caching infrastructure will ensure that performance stays in an acceptable range 
 a helper method to get the relative distinguished name rdn from distinguished name dn according to active directory documentation a group object s rdn is a cn 
 look up groups using posixgroups semantics use posix gid uid to find groups of the user 
 perform the second query to get the groups of the user if posixgroups is enabled use use posix gid uid to find otherwise use the general group member attribute to find it 
 perform ldap queries to get group names of a user perform the first ldap query to get the user object using the user s name if one query is enabled retrieve the group names from the user object if one query is disabled or if it failed perform the second query to get the groups 
 mangle given local java keystore file uri to allow use as a localjavakeystoreprovider 
 there are certain integrations of the credential provider api in which a recursive dependency between the provider and the hadoop filesystem abstraction causes a problem these integration points need to leverage this utility method to remove problematic provider types from the existing provider path within the configuration 
 the password is either found in the environment or in a file this routine implements the logic for locating the password in these locations 
 read more data and get them processed entry condition ostart   ofinish exit condition ostart return ofinish ostart we have this many bytes for you  no data now but could have more later or  absolutely no more data 
 disposes of any system resources or security sensitive information sasl might be using 
 reads the next byte of data from this input stream the value byte is returned as an int in the range  to  if no byte is available because the end of the stream has been reached the value  is returned this method blocks until input data is available the end of the stream is detected or an exception is thrown 
 reads up to b length bytes of data from this input stream into an array of bytes the read method of inputstream calls the read method of three arguments with the arguments b  and b length 
 reads up to len bytes of data from this input stream into an array of bytes this method blocks until some input is available if the first argument is null up to len bytes are read and discarded 
 skips n bytes of input from the bytes that can be read from this input stream without blocking fewer bytes than requested might be skipped the actual number of bytes skipped is equal to n or the result of a call to link available available whichever is smaller if n is less than zero no bytes are skipped the actual number of bytes skipped is returned 
 returns the number of bytes that can be read from this input stream without blocking the available method of inputstream returns  this method should be overridden by subclasses 
 closes this input stream and releases any system resources associated with the stream the close method of saslinputstream calls the close method of its underlying input stream 
 writes the specified byte to this output stream 
 writes b length bytes from the specified byte array to this output stream the write method of sasloutputstream calls the write method of three arguments with the three arguments b  and b length 
 writes len bytes from the specified byte array starting at offset off to this output stream 
 flushes this output stream
 closes this output stream and releases any system resources associated with this stream 
 instantiate a sasl client for the first supported auth type in the given list the auth type must be defined enabled and the user must possess the required credentials else the next auth is tried 
 try to create a saslclient for an authentication type may return null if the type isn t supported or the client lacks the required credentials 
 do client side sasl authentication with server via the given inputstream and outputstream
 evaluate the server provided challenge the server must send a token if it s not done if the server is done the challenge token is optional because not all mechanisms send a final token for the client to update its internal state the client must also be done after evaluating the optional token to ensure a malicious server doesn t prematurely end the negotiation with a phony success 
 get sasl wrapped inputstream if sasl qop requires unwrapping otherwise return original stream can be called only after saslconnect has been called 
 get sasl wrapped outputstream if sasl qop requires wrapping otherwise return original stream can be called only after saslconnect has been called 
 release resources used by wrapped saslclient 
 read from in 
 write to out 
 convert kerberos principal name pattern to valid kerberos principal names it replaces hostname pattern with hostname which should be fully qualified domain name if hostname is null or     it uses dynamically looked up fqdn of the current host instead 
 convert kerberos principal name pattern to valid kerberos principal names this method is similar to link getserverprincipal string string except  the reverse dns lookup from addr to hostname is done only when necessary  param addr can t be null no default behavior of using local hostname when addr is null 
 retrieve the name of the current host multihomed hosts may restrict the hostname lookup to a specific interface and nameserver with link org apache hadoop fs commonconfigurationkeyspublic hadoop security dns interface key and link org apache hadoop fs commonconfigurationkeyspublic hadoop security dns nameserver key 
 login as a principal specified in config substitute host in user s kerberos principal name with a dynamically looked up fully qualified domain name of the current host 
 login as a principal specified in config substitute host in user s kerberos principal name with hostname if non secure mode return if no keytab available bail out with an exception
 perform the given action as the daemon s login user if an interruptedexception is thrown it is converted to an ioexception 
 perform the given action as the daemon s current user if an interruptedexception is thrown it is converted to an ioexception 
 resolves a host subject to the security requirements determined by hadoop security token service use ip optionally logs slow resolutions 
 create an inetaddress with a fully qualified hostname of the given hostname inetaddress does not qualify an incomplete hostname that is resolved via the domain search list link inetaddress getcanonicalhostname will fully qualify thehostname but it always return the a record whereas the given hostname may be a cname 
 get the list of users or groups returned by the specified command and save them in the corresponding map 
 returns list of groups for a user
 get the current user s group list from unix by running the command groups note for non existing user it will return empty list 
 attempt to partially resolve group names 
 refresh the netgroup cache
 add a group to cache only netgroups are cached
 gets users for a netgroup
 initializes the keystores of the factory 
 creates a reloadable trustmanager the trustmanager reloads itself if the underlying trustore file has changed 
 initializes the factory 
 if the given link httpurlconnection is an link httpsurlconnection configures the connection with the link sslsocketfactory and link hostnameverifier of this sslfactory otherwise does nothing 
 the javax net ssl hostnameverifier contract 
 extracts the array of subjectalt dns names from an xcertificate returns null if there aren t any note java doesn t appear able to extract international characters from the subjectalts it can only extract international characters from the cn field or maybe the version of openssl i m using to test isn t storing the international characters correctly in the subjectalts 
 should be called before this object is used 
 for subclasses externalizing the storage for example zookeeper based implementations
 for subclasses externalizing the storage for example zookeeper based implementations
 for subclasses externalizing the storage for example zookeeper based implementations
 update the current master key this is called once by startthreads before tokenremoverthread is created and only by tokenremoverthread afterwards 
 update the current master key for generating delegation tokens it should be called only by tokenremoverthread 
 renew a delegation token 
 cancel a token by removing it from cache 
 remove expired delegation tokens from cache 
 
 
 returns an authenticated link httpurlconnection it uses a delegation token only if the given auth token is an instance of link token andit contains a delegation token otherwise use the configured link delegationtokenauthenticator to authenticate the connection 
 returns an authenticated link httpurlconnection if the delegation token is present it will be used taking precedence over the configured authenticator 
 returns an authenticated link httpurlconnection if the delegation token is present it will be used taking precedence over the configured authenticator if the doas parameter is not null the request will be done on behalf of the specified doas user 
 renews a delegation token from the server end point using the configured authenticator for authentication 
 cancels a delegation token from the server end point it does not require being authenticated by the configured authenticator 
 it delegates to link authenticationfilter getconfiguration string filterconfig andthen overrides the link authenticationhandler to use if authenticationtype is set to simple or kerberos in order to use the corresponding implementation with delegation token support 
 authenticates a request looking for the delegation query string parameter and verifying it is a valid token if there is not delegation query string parameter it delegates the authentication to the link kerberosauthenticationhandler unless it isdisabled 
 requests a delegation token using the configured authenticator for authentication 
 requests a delegation token using the configured authenticator for authentication 
 renews a delegation token from the server end point using the configured authenticator for authentication 
 renews a delegation token from the server end point using the configured authenticator for authentication 
 cancels a delegation token from the server end point it does not require being authenticated by the configured authenticator 
 cancels a delegation token from the server end point it does not require being authenticated by the configured authenticator 
 load the pathchildrencache into the in memory map possible caches to be loaded are keycache and tokencache 
 this method synchronizes the state of a delegation token information in local cache with its actual value in zookeeper 
 print out a credentials file from the local filesystem 
 fetch a token from a service and save to file in the local filesystem 
 alias a token from a file and save back to file in the local filesystem 
 append tokens from list of files in local filesystem saving to last file 
 remove a token from a file in the local filesystem matching alias 
 renew a token from a file in the local filesystem matching alias 
 parse the command line arguments and initialize subcommand also will attempt to perform kerberos login if both principal and keytab flags are passed in args array 
 the same functionality with link retrievepassword except that this method can throw a link retriableexception or a link standbyexception to indicate that client can retry failover the same operation because of temporary issue on the server side 
 compute hmac of the identifier using the secret key and return the output as password
 get the token identifier object or null if it could not be constructed because the class could not be loaded for example 
 encode this token as a url safe string 
 decode the given url safe string into this token 
 is this token managed so that it can be renewed or cancelled 
 renew this delegation token 
 cancel this delegation token 
 return the current user including any doas in the current stack 
 find the most appropriate usergroupinformation to use
 create a usergroupinformation from a kerberos ticket cache 
 create a usergroupinformation from a subject with kerberos principal 
 get the currently logged in user if no explicit login has occurred the user will automatically be logged in with either kerberos credentials if available or as the local os user based on security settings 
 log in a user using the given subject
 log a user in from a keytab file loads a user identity from a keytab file and logs them in they become the currently logged in user 
 re login a user from keytab if tgt is expired or is close to expiry 
 re login a user in from a keytab file loads a user identity from a keytab file and logs them in they become the currently logged in user this method assumes that link loginuserfromkeytab string string hadhappened already the subject field of this usergroupinformation object is updated to have the new credentials 
 re login a user in from the ticket cache this method assumes that login had happened already the subject field of this usergroupinformation object is updated to have the new credentials 
 log a user in from a keytab file loads a user identity from a keytab file and login them in this new user does not affect the currently logged in user 
 did the login happen via keytab
 did the login happen via ticket cache
 run the given action as the user potentially throwing an exception 
 log all current real login ugi and token info into specified log 
 log all current real login ugi and token info into ugi debug log 
 a test method to print out the current user s ugi 
 inheritdoc 
 bind to the interrupt handler 
 this creates all the configurations defined by link getconfigurationstocreate ensuring thatthe resources have been pushed in if one cannot be loaded it is logged and the operation continues except in the case that the class does load but it isn t actually a subclass of link configuration 
 launch a service catching all exceptions and downgrading them to exit codes after logging sets link serviceexception to this value 
 instantiate the service defined in code serviceclassname sets the code configuration fieldto the the value of code conf and the code service field to the service created 
 parse the command arguments extracting the service class as the last element of the list after extracting all the rest the field link commandoptions field must already have been set 
 override point create a generic options parser or subclass thereof 
 get the groups for the users given and print formatted output to the link printstream configured earlier 
 get a client of the link getusermappingsprotocol 
 wait for the termination of the thread pools 
 find a jar that contains a class of the same name if any it will return a jar file even if that is not the first thing on the class path that has a class with the same name 
 use link get long timeunit timeout parameters to wait 
 start the connection to the zookeeper ensemble 
 start the connection to the zookeeper ensemble 
 get the data in a znode 
 get the data in a znode 
 set data into a znode 
 create a znode 
 create a znode 
 utility function to ensure that the configured base znode exists this recursively creates the znode as well as all of its parents 
 delete a znode 
 deletes the path checks for existence of path as well 
 this constructs a datachecksum by reading header len bytes from input stream in
 writes the checksum header to the output stream out 
 writes the current checksum to the stream if reset is true then resets the checksum 
 verify that the given checksums match the given data the mark of the bytebuffer parameters may be modified by this function but the position is maintained 
 implementation of chunked verification specifically on byte arrays this is to avoid the copy when dealing with bytebuffers that have array backing 
 create the directory if it doesn t exist and check that dir is readable writable and executable
 create the local directory if necessary check permissions and also ensure it can be read from and written into 
 the semantics of mkdirswithexistscheck method is different from the mkdirs method provided in the sun s java io file class in the following way while creating the non existent parent directories this method checks for the existence of those directories if the mkdir fails at any point since that directory might have just been created by some other process if both mkdir and the exists check fails for any seemingly non existent directory then we signal an error sun s mkdir would signal an error return false if a directory it is attempting to create already exists or the mkdir fails 
 performs some disk io by writing to a new file in the given directory and sync ing file contents to disk this increases the likelihood of catching catastrophic disk controller failures sooner 
 try to perform some disk io by writing to the given file without using native io 
 see link fileoutputstream fileoutputstream file 
 see link fileoutputstream write byte 
 returns link diskvalidator instance corresponding to its name the diskvalidator parameter can be basic for link basicdiskvalidator or read write for link readwritediskvalidator 
 like link terminate int string but uses the given throwable tobuild the message to display or throw as an link exitexception 
 forcibly terminates the currently running java virtual machine 
 like link terminate int throwable without a message 
 terminate the current process note that terminate is the only method that should be used to terminate the daemon processes 
 forcibly terminates the currently running java virtual machine 
 forcibly terminates the currently running java virtual machine 
 reads the lines in a file 
 dump a resource to out
 loads the class of the given name
 create an instance of a class
 create a genericoptionsparser to parse given options as well as generic hadoop options the resulting commandline object can be obtained by link getcommandline 
 modify configuration according user specified generic options 
 takes input as a comma separated list of files and verifies if they exist it defaults for file if the files specified do not have a scheme it returns the paths uri converted defaulting to file so an input of home user file home user file would return file home user file file home user file this method does not recognize wildcards 
 takes input as a comma separated list of files and verifies if they exist it defaults for file if the files specified do not have a scheme it returns the paths uri converted defaulting to file so an input of home user file home user file would return file home user file file home user file 
 parse the user specified options get the generic options and modify configuration accordingly 
 compute the hash of the specified file
 creates a http servlet response serializing the exception in it as json 
 validates the status of an httpurlconnection against an expected http status code if the current status code is not the expected one it throws an exception with a detail message using server side error messages if available note this method will throw the deserialized exception even if not declared in the throws of the method signature 
 save to a local file any existing file is overwritten unless the os blocks that 
 clone by converting to json and back again this is much less efficient than any java clone process 
 load from a hadoop filesystem there s a check for data availability after the file is open by raising an eofexception if stream available     this allows for a meaningful exception without the round trip overhead of a getfilestatus call before opening the file it may be brittle against an fs stream which doesn t return a value here but the standard filesystems all do json parsing and mapping problems are converted to ioes 
 save to a hadoop filesystem 
 write the json as bytes then close the file 
 deserialize from a byte array 
 convert an instance to a string form for output this is a robust operation which will convert any json generating exceptions into error text 
 creates a new keyprovider from the given configuration and configuration key name 
 close the underlying stream 
 read one line from the inputstream into the given text 
 read a line terminated by one of cr lf or crlf 
 read a line terminated by a custom delimiter 
 read from the inputstream into the given text 
 read from the inputstream into the given text 
 accepts an ip address and return true if ipaddress is in the list
 create a description of an example program 
 invoke the example application with the given arguments
 this is the method that adds the classed to the repository
 this is a driver for the example programs it looks at the first command line argument and tries to find an example program with that name if it is found it calls the main method in that class with the rest of the command line arguments 
 read a variable length integer in the same format that protobufs encodes 
 this code is to support backward compatibility and break the compile time dependency of core on mapred this should be made deprecated along with the mapred package hadoop  should be removed when mapred package is removed 
 create an object for the given class and initialize it from conf
 unpack a jar file into a directory this version unpacks all files inside the jar regardless of filename 
 unpack matching files from a jar entries inside the jar that do not match the given pattern will be skipped 
 unpack matching files from a jar entries inside the jar that do not match the given pattern will be skipped keep also a copy of the entire jar in the same directory for backward compatibility todo remove this feature in a new release and do only unjar
 unpack matching files from a jar entries inside the jar that do not match the given pattern will be skipped 
 creates a classloader based on the environment that was specified by the user if hadoop use client classloader is specified it creates an application classloader that provides the isolation of the user class space from the hadoop classes and their dependencies it forms a class space for the user jar as well as the hadoop classpath otherwise it creates a classloader that simply adds the user jar to the classpath 
 return a long value as passed in the given parameter throwingan exception if it is not present or if it is not a valid number 
 centralized logic to discover and validate the sanity of the hadoop home directory this does a lot of work so it should only be called privately for initialization once per process 
 get the hadoop home directory raises an exception if not found
 fully qualify the path to a binary that should be in a known hadoop bin location this is primarily useful for disambiguating call outs to executable sub components of hadoop to avoid clashes with other executables that may be in the path caveat this call doesn t just format the path to the bin directory it also checks for file existence of the composed path the output of this call should be cached by callers 
 inner logic of link getqualifiedbin string accessible for tests 
 fully qualify the path to a binary that should be in a known hadoop bin location this is primarily useful for disambiguating call outs to executable sub components of hadoop to avoid clashes with other executables that may be in the path caveat this call doesn t just format the path to the bin directory it also checks for file existence of the composed path the output of this call should be cached by callers 
 look for setsid 
 check to see if a command needs to be executed and execute if needed 
 run the command 
 execute the shell command 
 static method to execute a shell command covers most of the simple cases without requiring the user to implement the shell interface 
 static method to execute a shell command covers most of the simple cases without requiring the user to implement the shell interface 
 static method to execute a shell command covers most of the simple cases without requiring the user to implement the shell interface 
 param thread link thread to be shutdown 
 param service link executorservice to be shutdown 
 param service link executorservice to be shutdown 
 param str the string array to be parsed into an uri array 
 convert a string to long the input string is first be trimmed and then it is parsed with traditional binary prefix for example k will be converted to      g will be converted to  ^    
 read proc meminfo parse and compute memory information 
 read proc cpuinfo parse and calculate cpu information 
 read proc stat file parse and calculate cumulative cpu 
 read proc net dev file parse and calculate amount of bytes read and written through the network 
 read proc diskstats file parse and calculate amount of bytes read and written from to disks 
 read sys block diskname queue hw sector size file parse and calculate sector size for a specific disk 
 cause the current thread to sleep as close as possible to the provided number of milliseconds this method will log and ignore any link interruptedexception encountered 
 runs the tool with its configuration equivalent to run tool getconf tool args 
 print out a prompt to the user and return true if the user responds with y or yes case insensitive 
 transform input xml given a stylesheet 
 parse a comma separated list of authentication mechanisms each such mechanism should be of the form scheme auth the same syntax used for the addauth command in the zk cli 
 read the test config file testconf xml
 compare the actual output with the expected output
 tests runner
 tests use of multi byte characters in property names and values this test round trips multi byte string literals through saving and loading of config and asserts that the same values were read 
 a test to check whether this thread goes into infinite loop because of destruction of data structure by resize of map this problem was reported by spark  
 this test checks the correctness of loading setting the properties in terms of occurrence of deprecated keys 
 this test is to ensure the correctness of loading of keys with respect to being marked as final and that are related to deprecation 
 run a set of threads making changes to the deprecations concurrently with another set of threads calling get and set on configuration objects 
 utility function to extract quot public static final quot member variables from a configuration type class 
 utility function to extract quot public static final quot default member variables from a configuration type class 
 initialize the four variables corresponding the configuration class and the xml properties file 
 test crypto reading with different buffer size 
 test crypto writing with different buffer size 
 test crypto with different iv 
 test hflush hsync of crypto output stream and with different buffer size 
 test positioned read 
 test read fully 
 test seek to different position 
 test get position 
 test skip 
 test byte buffer read with different buffer size 
 test unbuffer 
 verifies that queue is initially filled to numinitvalues 
 verifies that queue is initialized warmed up for provided keys
 verifies that the refill task is executed after checkinterval if num values below lowwatermark 
 verifies that the no refill happens after checkinterval if num values above lowwatermark 
 verify getatmost when syncgeneration policy   all
 verify getatmost when syncgeneration policy   all
 verify getatmost when syncgeneration policy   low watermark
 test will timeout if secure random implementation always returns a constant value 
 test will timeout if secure random implementation always returns a constant value 
 test will timeout if secure random implementation always returns a constant value 
 test will timeout if secure random implementation always returns a constant value 
 this tests link streamcapabilities hascapability string for thethe underlying streams 
 call listfiles on an directory with an empty subdir 
 execute link filesystem listlocatedstatus path pathfilter generate a list from the iterator verify the length of the list returned and then return it 
 execute link filesystem listlocatedstatus path pathfilter generate a list from the iterator verify the length of the list returned and then return it uses link contracttestutils tolistthroughnextcallsalone remoteiterator to stress the iteration process 
 test that after renaming the nested subdirectory is moved along with all its ancestors 
 test that after renaming the nested file is moved along with all its ancestors it is similar to link testrenamepopulatesdirectoryancestors 
 create a uri off the scheme
 skip a test if a feature is unsupported in this fs 
 setup create the contract then init it 
 teardown 
 write a file and read it in validating the result optional flags control whether file overwrite operations should be enabled and whether the file should be deleted afterwards if there is a mismatch between what was written and what was expected a small range of bytes either side of the first error are logged to aid diagnosing what problem occurred whether it was a previous file or a corrupting of the current file this assumes that two sequential runs to the same path use datasets with different character moduli 
 write a file optional flags control whether file overwrite operations should be enabled
 write a file optional flags control whether file overwrite operations should be enabled optional using link org apache hadoop fs fsdataoutputstreambuilder 
 read a file verify its length and contents match the expected array 
 cleanup at the end of a test run 
 delete a directory there s a safety check for operations against the root directory these are intercepted and rejected with an ioexception unless the allowrootdelete flag is true
 rename operation safety check for attempts to rename the root directory verifies that src no longer exists after rename 
 block any operation on the root path this is a safety check
 list then delete the children of a path but not the path itself this can be used to delete the entries under a root path when that fs does not support code delete 
 write the text to a file returning the converted byte array for use in validating the round trip 
 touch a file 
 delete a file dir and assert that delete returned true and that the path no longer exists this variant rejects all operations on root directories 
 delete a file dir and assert that delete returned true and that the path no longer exists this variant rejects all operations on root directories
 read in length bytes convert to utf string 
 assert that a file exists and whose link filestatus entrydeclares that this is a file and not a symlink or directory 
 assert that a file exists and whose link filestatus entrydeclares that this is a file and not a symlink or directory 
 assert that a path exists but make no assertions as to the type of that entry 
 verify that a path exists returning the file status of the path 
 receives test data from the given input file and checks the size of the data as well as the pattern inside the received data 
 generates test data of the given size according to some specific pattern and writes it to the provided output file 
 creates and reads a file with the given size the test file is generated according to a specific pattern so it can be easily verified even if it s a multi gb one during the read phase the incoming data stream is also checked against this pattern 
 recursively create a directory tree return the details about the created tree the files and directories are those created under the path not the base directory created that is retrievable via link treescanresults getbasepath 
 recursively create a directory tree 
 get the status of a path eventually even if the fs doesn t have create consistency if the path is not there by the time the timeout completes an assertion is raised 
 recursively list all entries with a depth first traversal of the directory tree 
 write a dataset read it back in and verify that they match afterwards the file is deleted 
 verify that if you take an existing file and overwrite it the new values get picked up this is a test for the behavior of eventually consistent filesystems 
 asserts that a zero byte file has a status of file and not directory or symlink
 asserts that a zero byte file has a status of file and not directory or symlink
 assert that root directory renames are not allowed
 assert that renaming a parent directory to be a child of itself is forbidden
 main function it first parses the command line arguments it then reads the directory structure from the input directory structure file and creates directory structure in the file system namespace afterwards it reads the file attributes and creates files in the file all file content is filled with a 
 read directory structure file under the input directory create each directory under the specified root the directory names are relative to the specified root 
 read file structure file under the input directory create each file under the specified root the file names are relative to the root 
 constructor 
 main loop for each thread each iteration decides what s the next operation and then pauses 
 let the thread pause for a random amount of time in the range of  maxdelaybetweenops if the delay is not zero otherwise no pause 
 perform the next operation depending on the read and write probabilities the next operation could be either read write or list 
 read operation randomly picks a file in the test space and reads the entire file 
 the write operation randomly picks a directory in the test space and creates a file whose name consists of the current machine s host name and the thread id the length of the file follows gaussian distribution with an average size of  blocks and the standard deviation of  block the new file is filled with a immediately after the file creation completes the file is deleted from the test space 
 main function called by tool runner it first initializes data by parsing the command line arguments it then calls the loadgenerator
 this is the main function run threads to generate load on nn it starts the number of dfsclient threads as specified by the user it stops all the threads when the specified elapsed time is passed 
 parse the command line arguments and initialize the data 
 read a script file of the form lines of text with duration in seconds read probability and write probability separated by white space 
 create a table that contains all directories under root and another table that contains all files under root 
 create a table that contains all directories under the specified path and another table that contains all files under the specified path and whose name starts with file 
 main program
 the main function first parses the command line arguments then generates in memory directory structure and outputs to a file last generates in memory files and outputs them to a file 
 parse the command line arguments and initialize the data 
 output directory structure to a file each line of the file contains the directory name only empty directory names are printed 
 output all files attributes to a file each line of the output file contains a file name and its length 
 main program
 creates a file and deletes it 
 checks if a new created file exists 
 test writing to a file and reading its value 
 test getting the status of a file 
 test deleting a non empty directory 
 test renaming a file 
 test renaming a file onto an existing file 
 tests whether binary avro data files are displayed correctly 
 tests that a zero length file is displayed correctly 
 tests that a one byte file is displayed correctly 
 tests that a one byte file is displayed correctly 
 call all the constructors and verify the delegation is working properly
 call each of the setters and verify
 verify that du returns expected used space for a file we assume here that if a file system crates a file of size that is a multiple of the block size in this file system then the used size for the file will be exactly that size this is true for most file systems 
 check that the full parameter constructor works correctly 
 check that the non symlink constructor works correctly 
 check that the constructor without owner group and permissions works correctly 
 check that the no parameter constructor works correctly 
 check that tostring produces the expected output for a file 
 check that tostring produces the expected output for a directory 
 check that tostring produces the expected output for a symlink 
 get the cached filesystem for cachedfile a for the supplied user
 creates multiple directories for testing contents of them are dir tmp file x dir del file x dir dir file x dir dir file x link y to tmp x link tmpdir to tmp dir partitioned file part r  contents foo file part r  contents bar 
 creates a new file in the specified directory with the specified name and the specified file contents this method will add a newline terminator to the end of the contents string in the destination file 
 tests if fullydelete deletes a symlink to file only and not the file pointed to by symlink b symlink to dir only and not the dir pointed to by symlink 
 tests if fullydelete deletes a dangling symlink to file properly b dangling symlink to directory properly
 creates a directory which can not be deleted completely directory structure the naming is important in that link myfile is used to return them in alphabetical order when listed del w      file w xsubdir rwx ysubdir w zlink     file rwx file  xsubsubdir rwx  file rwx 
 test that getdu is able to handle cycles caused due to symbolic links and that directory sizes are not added to the final calculated size
 test that rename on a symlink works as expected 
 test that deletion of a symlink works as expected 
 test that length on a symlink works as expected 
 test copy to a path with non existent parent directory 
 test chmod  create and write file on fs  verify that exit code for chmod on existing file is   verify that exit code for chmod on non existing file is   verify that exit code for chmod with glob input on non existing file is   verify that exit code for chmod with glob input on existing file in 
 test chown  create and write file on fs  verify that exit code for chown on existing file is   verify that exit code for chown on non existing file is   verify that exit code for chown with glob input on non existing file is   verify that exit code for chown with glob input on existing file in 
 test chgrp  create and write file on fs  verify that exit code for chgrp on existing file is   verify that exit code for chgrp on non existing file is   verify that exit code for chgrp with glob input on non existing file is   verify that exit code for chgrp with glob input on existing file in 
 test that the builder can create a class specified through the class 
 initialize clean environment for start of each test
 validate that link setupdirs produced the expected result
 validate that single file link operations produced the expected results
 validate that multi file link operations produced the expected results
 test the single file method hardlink createhardlink also tests getlinkcount with values greater than one 
 test createhardlinkmult with empty list of files we use an extended version of the method call that returns the number of system exec calls made which should be zero in this case 
 test when input path is a file 
 test when input path is a directory 
 two buffer dirs the first dir does not exist is on a read only disk the second dir exists is rw
 two buffer dirs the first dir exists is on a read only disk the second dir exists is rw
 two buffer dirs both do not exist but on a rw disk check if tmp dirs are allocated in a round robin
 two buffer dirs both exists and on a r w disk later disk becomes read only 
 two buffer dirs the first dir does not exist is on a read only disk the second dir exists is rw getlocalpathforwrite with checkaccess set to false should create a parent directory with checkaccess true the directory should not be created 
 test no side effect files are left over after creating a temp temp file remove both the temp file and its parent verify that no files or directories are left over as can happen when file objects are mistakenly created from fully qualified path strings 
 test getlocalpathtoread returns correct filename and file schema 
 test that link localdirallocator getalllocalpathstoread string configuration returns correct filenames and file schema 
 test the capability of setting the working directory 
 test syncable interface on raw local file system
 regression test for hadoop  bufferedfsinputstream returning wrong results after certain sequences of seeks and reads 
 test localfilesystem setpermission 
 test localfilesystem setowner 
 steps  create a directory with default permissions  with umask   check the directory has good permissions   set the umask to   create a new directory with default permissions  for this directory we expect  as permission not 
 test uris created from path objects 
 test reserved characters in uris and therefore paths 
 test trash for the shell s delete command for the file system fs
 test trash for the shell s delete command for the default file system specified in the paramter conf
 simulate the carrier process of the trash emptier restarts verify it honors the fs trash interval before and after restart 
 test same file deletion multiple time this is more of a performance test shouldn t be run as a unit test
 when mark is used on bufferedinputstream the request size on the checksum file system can be small however checksum file system currently depends on the request size   bytespersum to work properly 
 tests that viewfilesystem dispatches calls for every acl method through the mount table to the correct underlying filesystem with all path arguments translated as required 
 regression test for hadoop  
 test modify operations create mkdir delete etc on the mount file system where the pathname references through the mount points hence these operation will modify the target file system verify the operation via mountfs ie fsys and also via the target file system ie fsyslocal that the mount link points to 
 test readops e g list liststatus on internal dirs of mount table these operations should succeed 
 send the letterword
 set up two services and their failover controllers svc is started first so that it enters active state and then svc is started which enters standby
 set up the specified number of services and their failover controllers svc is started first so that it enters active state and then svc svcn is started which enters standby adds any extra svc needed beyond the first two before starting the rest of the cluster 
 wait for the given ha service to enter the given ha state this is based on the state of zkfc not the state of ha service there could be difference between the two for example when the service becomes unhealthy zkfc will quit zk election and transition to haservicestate initializing and remain in that state until the service becomes healthy 
 expires the zk session associated with service fromidx and waits until service toidx takes over 
 verify that successful znode create result becomes active and monitoring is started
 verify that when the callback fails to enter active state the elector rejoins the election after sleeping for a short period 
 verify that when the callback fails to enter active state after a zk disconnect i e from the statcallback that the elector rejoins the election after sleeping for a short period 
 verify that if there is a record of a prior active node the elector asks the application to fence it before becoming active 
 verify that retry of network errors verifies master by session id and becomes active if they match monitoring is started 
 verify behavior of watcher process callback with non node event
 verify behavior of watcher process with node event
 joinelection should happen only after service healthy 
 the test creates  electors which try to become active using a real zookeeper server it verifies that  becomes active and  becomes standby upon becoming active the leader quits election and the test verifies that the standby now becomes active 
 test that the proper state is propagated when the health monitor sees an uncaught exception in its thread 
 test that if the callback throws an rte this will terminate the health monitor and thus change its state to failed
 test that the various command lines for formatting the zk directory function correctly 
 test that if zookeeper is not running the correct error code is returned 
 test that automatic failover won t run against a target that hasn t explicitly enabled the feature 
 test that if acls are specified in the configuration that it sets the acls when formatting the parent node 
 test that the zkfc won t run if fencing is not configured for the local service 
 test that if the standby node is unhealthy it doesn t try to become active
 randomly expire the zk sessions of the two zkfcs this differs from the above test in that it is not a controlled failover we just do random expirations and expect neither one to ever generate fatal exceptions 
 have the services fail their health checks half the time causing the master role to bounce back and forth in the cluster meanwhile causes zk to disconnect clients every ms to trigger the retry code and failures to become active 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create and start a server with the test webapp
 pass in a server return a url bound to localhost and its port
 read in the content from a url
 test that verifies headers can be up to k long the test adds a k header leaving k for other headers this is because the header buffer setting is for all headers names and values included 
 access a url ignoring some ioexception such as the page does not exist 
 test the maximum number of threads cannot be exceeded 
 test the echo map servlet that uses getparametermap 
 access a url and get the corresponding return http status code the url will be accessed as the passed user by sending user name request parameter 
 verify the access for logs stacks conf and loglevel servlets when authentication filters are set but authorization is not enabled 
 verify the administrator access for logs stacks conf and loglevel servlets 
 groupa usera groupb usera userb groupc userc spnego filter has been enabled usera has the privilege to impersonate users in groupb usera has admin access to all default servlets but userb and userc don t have so logs can only be accessed by usera 
 test that verifies headers can be up to k long the test adds a k header leaving k for other headers this is because the header buffer setting is for all headers names and values included 
 test that verifies that excluded ciphers ssl rsa with rc  sha tls ecdh ecdsa with rc  sha tls ecdh rsa with rc  sha tls ecdhe ecdsa with rc  sha tls ecdhe rsa with rc  sha are not available for negotiation during ssl connection 
 test that verified that additionally included cipher tls ecdhe ecdsa with aes  cbc sha is only available cipher for working tls connection from client to server disabled for all other common ciphers 
 test verifies that mutually exclusive server s disabled cipher suites and client s enabled cipher suites can successfully establish tls connection 
 write inflen bytes deflated to file in test dir using codec records are of the form lt i gt lt b rand gt lt ii gt lt b rand gt 
 regression test for hadoop  seeking in a block compressed stream would not properly reset the block decompressor state 
 create the raw erasure encoder to test
 create the raw erasure decoder to test
 performs benchmark 
 create the raw erasure encoder to test
 create the raw erasure decoder to test
 test none codecs
 fill up exactly one block 
 one block plus one record 
 verify that the compressed data size is less than raw data size 
 test for races in fstat usage note this test is likely to fail on rhel   which has a non threadsafe implementation of getpwuid r 
 validate access checks on windows 
 test that opens and closes a file  times this would crash with too many open files if we leaked fds using this access pattern 
 test basic chmod operation
 test that concurrent failed method invocations only result in a single failover 
 for debugging and testing 
 test code mapfile reader getclosest method 
 test code mapfile reader midkey method 
 test code mapfile writer rename method 
 test code mapfile rename method with throwing code ioexception 
 test code mapfile reader finalkey method
 test code mapfile writer constructor with key valueand validate it with code keyclass valueclass methods 
 test code mapfile reader next key value for iteration 
 test code mapfile writer testfix method
 test getclosest feature 
 assert mapwritable does not grow across calls to readfields 
 unit tests for sequencefile 
 unit tests for sequencefile metadata 
 for debugging and testing 
 test code setfile reader methods next get in combination 
 for debugging and testing 
 test encoding and decoding of utf outside the basic multilingual plane this is a regression test for hadoop  
 test  check that simpleversionedwritable throws an exception 
 test  check that simplewritable 
 utility method for testing writables 
 run minibenchmark with miniserver as the rpc server 
 run minibenchmark using delegation token authentication 
 test if  the rpc server uses the call id retry provided by the rpc client and  the rpc client receives the same call id retry from the rpc server 
 test if the rpc server gets the retry count from client 
 test if the rpc server gets the default retry count  from client 
 tests that client generates a unique sequential call id for each rpc call even if multiple threads are using the same client 
 generic test case for exceptions thrown at some point in the ipc process 
 test case that fails a write but only after taking enough time that a ping should have been sent this is a reproducer for a deadlock seen in one iteration of hadoop  
 test that if the socket factory throws an ioe it properly propagates to the client 
 test that if a runtimeexception is thrown after creating a socket but before successfully connecting to the ipc server that the failure is handled properly this is a regression test for hadoop  
 check service class byte in ipc header is correct on wire 
 check that reader queueing works
 make a call from a client and verify if header info is changed in server side
 check that file descriptors aren t leaked by starting and stopping ipc servers 
 test if  the rpc server uses the call id retry provided by the rpc client and  the rpc client receives the same call id retry from the rpc server 
 test the retry count while used in a retry proxy 
 test that there is no retry when invalid token exception is thrown verfies fix for hadoop 
 test if the rpc server gets the default retry count  from client 
 test if the rpc server gets the retry count from client 
 tests that client generates a unique sequential call id for each rpc call even if multiple threads are using the same client 
 a server method implemented using link retrycache 
 this simlulates a long server retried operations multiple threads start an operation that takes long time and finally succeeds the retries in this case end up waiting for the current operation to complete all the retries then complete based on the entry in the retry cache 
 this simlulates a long server operation multiple threads start an operation that takes long time and finally fails the retries in this case end up waiting for the current operation to complete all the retries end up performing the operation again 
 this simlulates a short server operation multiple threads start an operation that takes very short time and finally succeeds the retries in this case do not wait long for the current operation to complete all the retries then complete based on the entry in the retry cache 
 this simlulates a short server operation multiple threads start an operation that takes short time and finally fails the retries in this case do not wait for the current operation to complete all the retries end up performing the operation again 
 switch off setting sockettimeout values on rpc sockets verify that rpc calls still work ok 
 test that server stop properly stops all threads
 test rpc backoff by response time of each priority level 
 verify the rpc server can shutdown properly when callqueue is full 
 this tests that the time bounded wait for a proxy operation works and times out 
 this test sets off a blocking thread and then interrupts it before checking that the thread was interrupted
 test sockssocketfactory 
 test standardsocketfactory 
 test client command line options does not validate server behavior 
 creates and starts a jetty server binding at an ephemeral port to run loglevel servlet 
 run both client and server using the given protocol 
 test setting log level to info 
 test setting log level to error 
 server runs http no spnego 
 server runs http  spnego 
 server runs https no spnego 
 server runs https  spnego 
 save the config to a file
 common use cases
 test a race condition when updating the jmx cache hadoop   thread a reads the jmx metric every  jmx cache ttl it marks the jmx cache to be updated by marking lastrecs to null after this it adds a new key to the metrics the next call to read should pick up this new key  thread b triggers jmx metric update every  jmx cache ttl it assigns lastrecs to a new object not null any more  thread a tries to read jmx metric again sees lastrecs is not null and does not update jmx cache as a result the read does not pickup the new metric 
 hadoop 
 test blocking when queue is empty
 test consumers that take their time 
 test concurrent consumer access which is illegal
 ensure that quantile estimates from link mutablequantiles are withinspecified error bounds 
 test that link mutablequantiles rolls the window over at the specifiedinterval 
 test that link mutablequantiles rolls over correctly even if no itemshave been added to the window
 tests if the results are correct if no samples are inserted dry run of empty roll over 
 tests the case s interval and  sliding windows sample stream  times   and  respectively e g       and    
 helper method that writes metrics files to a target path reads those files and returns the contents of all files as a single string this method will assert that the correct number of files is found 
 read the log files at the target path and return the contents as a single string this method will assert that the correct number of files is found 
 read the target log file and append its contents to the stringbuilder 
 return the name of the log file for this host 
 call link dowritetest after pre creating the log file and filling itwith junk data 
 create a file at the target path with some known data in it quot extra stuff quot if the test run is happening within  seconds of the top of the hour this method will sleep until the top of the hour 
 create files at the target path with some known data in them each file will have the same content quot extra stuff quot if the test run is happening within  seconds of the top of the hour this method will sleep until the top of the hour 
 return a calendar based on the current time if the current time is very near the top of the hour less than  seconds sleep until the new hour before returning a new calendar instance 
 check whether port is available or not 
 wait till the port available 
 find the specified number of unique ports available the ports are all closed afterwards so other network services started may grab those same ports 
 test that our local ip address is not null
 test that null dns server gives the same result as if no dns server was passed 
 test that default dns server gives the same result as if no dns server was passed 
 test the default ip addresses is the local ip addr
 testcase get our local address and reverse look it up
 test that when using an invalid dns server with hosts file fallback we are able to get the hostname from the hosts file this test may fail on some misconfigured test machines that don t have an entry for localhost in their hosts file this entry is correctly configured out of the box on common linux distributions and os x windows refuses to resolve     to localhost despite the presence of this entry in the hosts file we skip the test on windows to avoid reporting a spurious failure 
 test that when using an invalid dns server without hosts file fallback we get back the cached host name 
 change dns cachedhostname to something which cannot be a real host name uses reflection since it is a private final field 
 test that the name localhost resolves to something if this fails your machine s network is in a mess go edit etc hosts
 test that we can t accidentally connect back to the connecting socket due to a quirk in the tcp spec this is a regression test for hadoop  
 test for link netutils islocaladdress java net inetaddress 
 test for link netutils normalizehostnames 
 test that we get a read result of  on eof 
 test that if one thread is blocking in a read or write operation another thread can close the socket and stop the accept 
 test that we get an asynchronouscloseexception when the domainsocket we re using is closed during a read or write operation 
 test a simple client server interaction 
 test file descriptor passing 
 test file descriptor path security 
 test that we can create a domainsocketwatcher and then shut it down 
 test that we can get notifications out a domainsocketwatcher 
 test that a java interruption can stop the watcher thread
 test that domain sockets are closed when the watcher is closed 
 test the netgroups groups in acl rules that start with this is a manual test because it requires host setup native code compiled specify the group mapping class host setup etc nsswitch conf should have a line like this netgroup files etc netgroup should be the whole file lasvegas elvis memphis elvis jerryleelewis to run this test export java home  path to java ant dtestcase testaccesscontrollist dtest output yes dtestaccesscontrollistgroupmapping classname compile native test where classname is one of the classes that provide group mapping services i e classes that implement groupmappingserviceprovider interface at this time org apache hadoop security jnibasedunixgroupsnetgroupmapping org apache hadoop security shellbasedunixgroupsnetgroupmapping
 creates a keystore with a single key and saves it to a file 
 performs complete setup of ssl configuration in preparation for testing an sslfactory this includes keys certs keystores truststores the server ssl configuration file the client ssl configuration file and the master configuration file read by the sslfactory 
 performs complete setup of ssl configuration in preparation for testing an sslfactory this includes keys certs keystores truststores the server ssl configuration file the client ssl configuration file and the master configuration file read by the sslfactory 
 performs complete setup of ssl configuration in preparation for testing an sslfactory this includes keys certs keystores truststores the server ssl configuration file the client ssl configuration file and the master configuration file read by the sslfactory 
 saves configuration to a file 
 checks that sslfactory initialization is successful with the given arguments this is a helper method for writing test cases that cover different combinations of settings for the store password and key password it takes care of bootstrapping a keystore a truststore and ssl client or server configuration then it initializes an sslfactory if no exception is thrown then initialization was successful 
 delay returning on a latch or a specific amount of time 
 dump any file to standard out 
 test that if the link ldapgroupsmapping connection timeout is set in theconfiguration the ldapgroupsmapping connection will timeout by this value if it does not get a ldap response from the server 
 test that if the link ldapgroupsmapping read timeout is set in theconfiguration the ldapgroupsmapping query will timeout by this value if it does not get a ldap response from the server 
 test hadoop proxy user for impersonation 
 test login method 
 given user name get all the groups needs to happen before creating the test users
 test for the case that usergroupinformation getcurrentuser is called when the accesscontrolcontext has a subject associated with it but that subject was not created by hadoop ie it has no associated user principal 
 test hassufficienttimeelapsed method 
 this test checks a race condition between getting and adding tokens for the current user calling usergroupinformation getcurrentuser returns a new object each time so simply making these methods synchronized was not enough to prevent race conditions and causing a concurrentmodificationexception these methods are synchronized on the subject which is the same object between usergroupinformation instances this test tries to cause a cme by exposing the race condition previously this test would fail every time now it does not 
 add a bunch of subnets and ipss to the whitelist check for inclusion in whitelist check for exclusion from whitelist
 save a configuration to a config file in the target dir 
 launch a service with the given list of arguments returns the service launcher from which the created service can be extracted via link servicelauncher getservice this call does not call link launchableservice execute or wait fora simple service to finish it returns the service that has been created initialized and started 
 launch expecting an exception 
 wait for the specified test to return true the test will be performed initially and then every code checkeverymillis until at least code waitformillis time has expired if code check is null or code waitformillis is less than code checkeverymillis this methodwill throw an link illegalargumentexception 
 wait until the method is called 
 after calling proceed this will wait until the call has completed and a result has been returned to the caller 
 periodically check and wait for any threads whose name match the given regular expression 
 get the diff between two files 
 retrieve the test timeout from the system property link property test default timeout falling back to the value in link test default timeout value if theproperty is not defined 
 wait for a condition to be met with a retry policy returning the sleep time before the next attempt is made if at the end of the timeout period the condition is still false or failing with an exception the timeout handler is invoked passing in the timeout and any exception raised in the last invocation the exception returned by this timeout handler is then rethrown example wait s for a condition to be met with a sleep of s between each probe if the operation is failing then after s the timeout handler is called this returns the exception passed in if any or generates a new one await    return     filesystem listfiles new path length   timeout ex  ex   null ex new timeoutexception timeout 
 simplified link await int callable callable timeouthandler operation with a fixed interval and link generatetimeout handler to generate a code timeoutexception example await for probe to succeed await     return     filesystem listfiles new path length 
 repeatedly execute a closure until it returns a value rather than raise an exception exceptions are caught and with one exception trigger a sleep and retry this is similar of scalatest s code eventually timeout closure operation though that lacksthe ability to fail fast if the inner closure has determined that a failure condition is non recoverable example spin until an the number of files in a filesystem is non zero returning the files found the sleep interval backs off by  ms each iteration to a maximum of s filestatus files   eventually    filestatus f   filesystem listfiles new path assertequals  f length return f new proportionalretryinterval   this allows for a fast exit yet reduces probe frequency over time 
 variant of link eventually int callable callable method forvoid lambda expressions 
 simplified link eventually int callable callable methodwith a fixed interval example wait s until an assertion holds sleeping s between each check eventually     assertequals  filesystem listfiles new path length 
 variant of link eventually int int callable method forvoid lambda expressions 
 intercept an exception throw an code assertionerror if one not raised the caught exception is rethrown if it is of the wrong class or does not contain the text defined in code contained example expect deleting a nonexistent file to raise a code filenotfoundexception filenotfoundexception ioe   intercept filenotfoundexception class  filesystem delete new path missing false 
 variant of link intercept class callable to simplify voidinvocations 
 intercept an exception throw an code assertionerror if one not raised the caught exception is rethrown if it is of the wrong class or does not contain the text defined in code contained example expect deleting a nonexistent file to raise a code filenotfoundexception with the code tostring valuecontaining the text code missing filenotfoundexception ioe   intercept filenotfoundexception class missing  filesystem delete new path missing false 
 intercept an exception throw an code assertionerror if one not raised the caught exception is rethrown if it is of the wrong class or does not contain the text defined in code contained example expect deleting a nonexistent file to raise a code filenotfoundexception with the code tostring valuecontaining the text code missing filenotfoundexception ioe   intercept filenotfoundexception class missing path should not be found  filesystem delete new path missing false 
 variant of link intercept class callable to simplify voidinvocations 
 variant of link intercept class callable to simplify voidinvocations 
 invoke a callable wrap all checked exceptions with an assertionerror 
 invoke a callable wrap all checked exceptions with an assertionerror 
 waits for threads to finish or error out 
 returns after stopping all threads by joining them back 
 repeats a given user action until the context is asked to stop or meets an error 
 test tracing the globber this is a regression test for hdfs  
 returns the full path to the jar containing the class it always return a jar 
 test when lock is acquired no other thread can lock it 
 test the correctness under try with resource syntax 
 add a bunch of subnets and ipss to the file setup a low cache refresh test for inclusion check for exclusion add a bunch of subnets and ips wait for cache timeout test for inclusion check for exclusion
 add a bunch of subnets and ipss to the file setup a low cache refresh test for inclusion check for exclusion remove a bunch of subnets and ips wait for cache timeout test for inclusion check for exclusion
 asserts that the specified file is a jar file with a manifest containing a non empty classpath attribute 
 create an empty file with a random name under test directory 
 create an empty directory with a random name under test directory 
 inheritdoc 
 inheritdoc 
 add a bunch of ips to the file check for inclusion check for exclusion
 add a bunch of ips to the file check for inclusion check for exclusion
 add a bunch of subnets and ipss to the file check for inclusion check for exclusion
 specify an existing file but empty test for inclusion should be true as if the feature is turned off
 specify an existing file but ips in wrong format test for inclusion should be true as if the feature is turned off
 add a bunch of subnets and ipss to the file keep one entry wrong the good entries will still be used check for inclusion with good entries check for exclusion
 test d parsing 
 test exclusive access of the lock 
 test the correctness with try with resource syntax 
 tests exclusive access of the write lock 
 tests the read lock 
 test that readrawvarint is compatible with the varints encoded by protobuf s codedoutputstream 
 generate crc  lookup tables 
 construct a jar with two files in it in our test dir 
 test unjarring a big file this checks appending the remainder of the file to the tee output stream in runjar unjarandsave 
 this test takes advantage of the invariant winutils path is valid or access to it will raise an exception holds on linux and without any winutils binary even if hadoop home points to a real hadoop directory the exception reporting can be validated
 assert that an attept to resolve the hadoop home dir failed with an expected text in the exception string value 
 assert that an attept to resolve the code bin winutils exe failed withan expected text in the exception string value 
 test parsing proc stat and proc cpuinfo
 write information to fake proc stat file
 test parsing proc meminfo
 test parsing proc meminfo with inactive file present
 test parsing proc net dev
 test parsing proc diskstats
 validate behavior of chmod commands on directories on windows 
 read in the content from an url connection 
 test the configurable timeout in the kmsclientprovider open up a socket but don t accept connections for it this leads to a timeout when the kms client attempts to connect 
 creates a minikdc 
 starts the minikdc 
 stops the minikdc
 returns list of journalnode addresses from the configuration 
 lookup the http https address of the namenode and replace its hostname with defaulthost when it found out that the address is a wildcard local address 
 create an uri from scheme host and port 
 converts a relative time string into a duration in milliseconds 
 get an rpc proxy for each nn in an ha nameservice used when a given rpc call should be made on every nn in an ha nameservice not just the active 
 creates the namenode proxy with the passed protocol this will handle creation of either ha or non ha enabled proxy objects depending upon if the provided uri is a configured logical uri 
 creates the namenode proxy with the passed protocol this will handle creation of either ha or non ha enabled proxy objects depending upon if the provided uri is a configured logical uri 
 creates an explicitly non ha enabled proxy object most of the time you don t want to use this and should instead use link namenodeproxies createproxy 
 creates an explicitly non ha enabled proxy object most of the time you don t want to use this and should instead use link namenodeproxies createproxy 
 create a non secure tcppeerserver 
 read an op it also checks protocol version 
 process op by the corresponding method 
 receives sasl negotiation from a peer on behalf of a server 
 receives sasl negotiation for specialized encrypted handshake 
 given a secret manager and a username encoded for the encrypted handshake determine the encryption key 
 receives sasl negotiation for general purpose handshake 
 calculates the expected correct password on the server side for the general purpose handshake the password consists of the block access token s password known to the datanode via its secret manager this expects that the client has supplied a user name consisting of its serialized block access token identifier 
 this method actually executes the server side sasl handshake 
 load a layoutflags object from a stream 
 when we ve entered an out of sync state it s still useful to periodically send an empty rpc to the server such that it has the up to date committedtxid this acts as a sanity check during recovery and also allows that node s metrics to be up to date about its lag in the future this method may also be used in order to check that the current node is still the current writer even if no edits are being written 
 construct the http server based on the response the fromurl field in the response specifies the endpoint of the http server however the address might not be accurate since the server can bind to multiple interfaces here the client plugs in the address specified in the configuration and generates the uri 
 wait for the quorum to achieve a certain number of responses note that even after this returns more responses may arrive causing the return value of other methods in this class to change 
 param conf configuration object
 remove any log files and associated paxos files which are older than the given txid 
 purge files in the given directory which match any of the set of patterns the patterns must have a single numeric capture group which determines the associated transaction id of the file only those files for which the transaction id is less than the mintxidtokeep parameter are removed 
 return the last epoch which this node has promised not to acceptany lower epoch or  if no promises have been made 
 try to create a new epoch for this journal 
 write a batch of edits to the journal see qjournalprotocol journal requestinfo long long int byte 
 ensure that the given request is coming from the correct writer and in order 
 start a new segment at the given txid the previous segment must have already been finalized 
 finalize the log segment at the given transaction id 
 see journalmanager purgelogsolderthan long 
 see qjournalprotocol geteditlogmanifest string string long boolean 
 see qjournalprotocol preparerecovery requestinfo long 
 see qjournalprotocol acceptrecovery requestinfo qjournalprotocolprotos segmentstateproto url 
 retrieve the persisted data for recovering the given segment from disk 
 persist data for recovering the given segment from disk 
 return the directory inside our configured storage dir which corresponds to a given journal 
 update block keys if update time  update interval 
 generate an block token for current user 
 check if access should be allowed userid is not checked if null 
 
 
 load secretmanager state from fsimage 
 store the current state of the secretmanager for persistence
 private helper methods to save delegation keys and tokens in fsimage
 private helper methods to load delegation tokens from fsimage
 private helper method to load delegation keys from fsimage 
 balance all namenodes for each iteration for each namenode execute a link balancer to work through all datanodes once 
 parse arguments and then run balancer 
 parse command line arguments 
 dispatch the move to the proxy source wait for the response 
 this method iteratively does the following it first selects a block to move then sends a request to the proxy source to start the block move when the source s block list falls below a threshold it asks the namenode for more blocks it terminates when it has dispatch enough block move tasks or it has received enough blocks from the namenode or the elapsed time of the iteration has exceeded the max time limit 
 dispatch block moves for each source the thread selects blocks to move sends request to proxy source to initiate block move the process is flow controlled block selection is blocked if there are too many un confirmed block moves 
 create link namenodeconnector for the given namenodes 
 the idea for making sure that there is no more than one instance running in an hdfs is to create a file in the hdfs writes the hostname of the machine on which the instance is running to the file but did not close the file until it exits this prevents the second instance from running because it can not creates the file while the first one is running this method checks if there is any running instance if no mark yes note that this is an atomic operation 
 increments logs and then returns the stamp
 commit the last block of the file and mark it as complete if it has meets the minimum redundancy requirement
 convert a specified block of the file to a complete block 
 force the given block in the given file to be marked as complete regardless of whether enough replicas are present this is necessary when tailing edit logs as a standby 
 convert the last block of the file to an under construction block the block is converted only if the file has blocks and the last one is a partial block its size is less than the preferred block size the converted block is returned to the client the client uses the returned block locations to form the data pipeline for this block the methods returns null if there is no partial block at the end the client is supposed to allocate a new block with the next call 
 create a locatedblocks 
 mark the block belonging to datanode as corrupt
 mark a replica of a contiguous block or an internal block of a striped block group as corrupt 
 the given storage is reporting all its blocks update the storage block list and block storage list maps 
 mark block replicas as corrupt except those on the storages in newstorages list 
 processfirstblockreport is intended only for processing initial block reports the first block report received from a dn after it registers it just adds all the valid replicas to the datanode without calculating a toremove list since there won t be any it also silently discards any invalid blocks thereby deferring their processing until the next block report 
 try to process any messages that were previously queued for the given block this is called from fseditlogloader whenever a block s state in the namespace has changed or a new block has been created 
 process any remaining queued datanode messages after entering active state at this point they will not be re queued since we are the definitive master node and thus should be up to date with the namespace information 
 faster version of link addstoredblock intended for use with initial block report at startup if not in startup safe mode will call standard addstoredblock assumes this method is called immediately so there is no need to refresh the storedblock from blocksmap doesn t handle low redundancy extra redundancy or worry about pendingreplications or corruptreplicas because it s in startup safe mode doesn t log every block because there are typically millions of them 
 modify block datanode map remove block from set of needed reconstruction if this takes care of the problem 
 invalidate corrupt replicas this will remove the replicas from the block s location list add them to link invalidateblocks so that they could be furtherdeleted from the respective data nodes and remove the block from corruptreplicasmap this method should be called when the block has sufficient number of live replicas 
 for each block in the name node verify whether it belongs to any file extra or low redundancy place it into the respective queue 
 the given node is reporting that it received a certain block 
 the given node is reporting incremental information about some blocks this includes blocks that are starting to be received completed being received or deleted this method must be called with fsnamesystem lock held 
 choose numofreplicas from all data nodes
 choose localmachine as the target if localmachine is not available choose a node on the same rack
 choose one node from the rack that localmachine is on if no such node is available choose one node from the rack where a second replica is on if still no such node is available choose a random node in the cluster 
 choose numofreplicas nodes from the racks that localmachine is not on if not enough nodes are available choose the remaining ones from the local rack
 randomly choose one target from the given scope 
 choose numofreplicas in order  if total replica expected is less than numofracks in cluster it choose randomly  if total replica expected is bigger than numofracks it choose a fill each rack exactly maxnodesperrack  replicas b for some random racks place one more replica to each one of them until numofreplicas have been chosen  if after step  there are still replicas not placed due to some racks have fewer datanodes than maxnodesperrack the rest of the replicas is placed evenly on the rest of the racks who have datanodes that have not been placed a replica  if after step  there are still replicas not placed a link notenoughreplicasexception is thrown for normal setups step  would suffice so in the end the difference of the numbers of replicas for each two racks is no more than  either way it always prefer local storage 
 choose as evenly as possible from the racks which have available datanodes 
 choose local node of localmachine as the target if localmachine is not available will fallback to nodegroup rack when flag fallbacktonodegroupandlocalrack is set 
 waits for a rescan to complete this doesn t guarantee consistency with pending operations only relative recency since it will not force a new rescan if a rescan is already underway note that this call will release the fsn lock so operations before and after are not atomic 
 used while checking if decommission inprogress datanodes can be marked as decommissioned or entering maintenance datanodes can be marked as in maintenance combines shared logic of prunereliableblocks and handleinsufficientlystored 
 resolve a node s network location if the dns to switch mapping fails then this method guarantees default rack location 
 resolve a node s dependencies in the network if the dns to switch mapping fails then this method returns empty list of dependencies 
 register the given datanode with the namenode nb the given registration is mutated and given back to the datanode 
 rereads conf to get hosts and exclude list file names rereads the files to update the hosts and exclude lists it checks if any of the hosts have changed states 
 parse a datanodeid from a hosts file entry
 read the includes and excludes lists from the named files any previous includes and excludes lists are discarded 
 param dn datanode descriptor
 read from in 
 write to out 
 same as getugi null request conf 
 same as getugi context request conf kerberos ssl true 
 get link usergroupinformation and possibly the delegation token out ofthe request 
 write metrics to the metrics appender when invoked 
 check to see if current directory is empty this method is used before determining to format the directory 
 check consistency of the storage directory 
 check consistency of the storage directory 
 complete or recover storage state from previously failed transition 
 lock storage to provide exclusive access locking is not supported by all file systems e g nfs does not consistently support exclusive locks if locking is supported we guarantee exclusive access to the storage directory otherwise no guarantee is given 
 attempts to acquire an exclusive lock on the storage 
 unlock storage 
 check whether underlying file system supports file locking 
 returns true if the storage directory on the given directory is already loaded 
 check if the given storage directory comes from a version of hadoop prior to when the directory layout changed ie   if this is the case this method throws an ioexception 
 iterate over each of the link formatconfirmable objects potentially checking with the user whether it should be formatted if running in interactive mode will prompt the user for each directory to allow them to format anyway otherwise returns false unless force is specified 
 write properties to the version file in the given storage directory 
 copies a file usually large to a new location using native unbuffered io this method copies the contents of the specified source file to the specified destination file using os specific unbuffered io the goal is to avoid churning the file system buffer cache when copying large files we can t use fileutils copyfile from apache commons io because it is a buffered io based on filechannel transferfrom which uses mmapbytebuffer internally the directory holding the destination file is created if it does not exist if the destination file exists then this method will delete it first note setting preservefiledate to code true tries to preserve the file s last modifieddate times using link file setlastmodified long however it is not guaranteed that the operation will succeed if the modification operation fails no indication is provided 
 recursively delete all the content of the directory first and then the directory itself from the local filesystem 
 write all data storage files 
 unlock all storage directories 
 read properties from the version file in the given storage directory 
 read properties from the the previous version file in the given storage directory 
 get common storage fields should be overloaded if additional fields need to be get 
 validate and set storage type from link properties 
 validate and set ctime from link properties 
 validate and set clusterid from link properties 
 validate and set layout version from link properties 
 validate and set namespaceid version from link properties 
 interprets the passed string as a uri in case of error it assumes the specified string is a file 
 converts the passed file to a uri this method trims the trailing slash if one is appended because the underlying file is in fact a directory that exists 
 converts a collection of strings into a collection of uris 
 downloads the files at the specified url location into destination storage 
 receives file at the url location from the input stream and puts them in the specified destination storage location 
 calculate partial block checksum 
 reconstruct this data block and recalculate checksum 
 load one storage directory recover from previous transitions if required 
 analyze storage directories recover from previous transitions if required the block pool storages are either all analyzed or none of them is loaded therefore a failure on loading any block pool storage results a faulty data volume 
 format a block pool slice storage 
 analyze whether a transition of the bp state is required and perform it if necessary rollback if previouslv   layout version prevctime layout version  this ctime  namenode ctime regular startup if this lv   layout version this ctime   namenode ctime
 upgrade to any release after     included release e g       upgrade procedure is as follows if current previous exists then delete it rename current current to current bpid current previous tmp create new current current directory hard links for block files are created from previous tmp to current save new version file in current directory rename previous tmp to previous 
 restore all files from the trash directory to their corresponding locations under current 
 create a rolling upgrade marker file for each bp storage root if it does not exist already 
 check whether the rolling upgrade marker file exists for each bp storage root if it does exist then the marker file is cleared and more importantly the layout upgrade is finalized 
 close files and release volume reference 
 flush block data and metadata files to disk 
 receives and processes a packet it can contain many chunks returns the number of data bytes that the packet has 
 adjust the file pointer in the local meta file so that the last checksum will be overwritten 
 send an oob response if all acks have been sent already for the block and the responder is about to close the delivery is not guaranteed this is because the other end can close the connection independently an oob coming from downstream will be automatically relayed upstream by the responder this method is used only by originating datanode 
 wait for a packet with given code seqno to be enqueued to ackqueue 
 wait for all pending packets to be acked then shutdown thread 
 thread to process incoming acks 
 finalize the block and close the block file
 the wrapper for the unprotected version this is only called by the responder s run method 
 param ack ack received from downstream
 block synchronization 
 constructor
 wait for rbw replica to reach the length
 sends a packet with up to maxchunks chunks of data 
 sendblock is used to read block and its metadata and stream the data to either a client or to another datanode 
 called by the bpserviceactors when they handshake to a nn if this is the first nn connection this sets the namespace info for this bpofferservice if it s a connection to a new nn it verifies that this namespace matches eg to prevent a misconfiguration where a standbynode from a different cluster is specified 
 after one of the bpserviceactors registers successfully with the nn it calls this function to verify that the nn it connected to is consistent with other nns serving the block pool 
 perform the first part of the handshake with the namenode this calls versionrequest to determine the nn s namespace and version info it automatically retries until the nn responds or the dn is shutting down 
 main loop for each bp thread run until shutdown forever calling remote namenode functions 
 no matter what kind of exception we get keep retrying to offerservice that s the loop that connects to the namenode and provides basic datanode functionality only stop when shouldrun or shouldservicerun is turned off which can happen either at shutdown or due to refreshnamenodes 
 inheritdoc the default link abstractfuture implementation throws code interruptedexception if the current thread is interrupted before orduring the call even if the value is already available 
 inheritdoc the default link abstractfuture implementation throws code interruptedexception if the current thread is interrupted before orduring the call even if the value is already available 
 returns a value suitable for storing in the link value field fromthe given future which is assumed to be done this is approximately the inverse of link getdonevalue object 
 run checks against all volumes of a dataset this check may be performed at service startup and subsequently at regular intervals to detect and handle failed volumes 
 inheritdoc the results of in progress checks are not useful during shutdown so we optimize for faster shutdown by interrupt all actively executing checks 
 create the datanode given a configuration an array of datadirs and a namenode proxy 
 inheritdoc 
 attempts to reload data volumes with new configuration 
 remove volumes from datanode see link removevolumes collection boolean for details 
 report a bad block which is hosted on the local dn 
 report a bad block which is hosted on the local dn 
 report a bad block on another dn eg if we received a corrupt replica from a remote host 
 this method starts the data node with the specified conf if conf s config property simulated property is set then a simulated storage based data node is created 
 check that the registration returned from a namenode is consistent with the information in the storage if the storage is fresh unformatted sets the storage id based on this registration also updates the block pool s state in the secret manager 
 one of the block pools has successfully connected to its nn this initializes the local storage for that block pool checks consistency of the nn s cluster id etc if this is the first block pool to register this also initializes the datanode scoped storage 
 initializes the link data the initialization is done only once when handshake with the the first namenode is completed 
 creates either nio or regular depending on socketwritetimeout 
 do the deed write the bytes
 instantiate a single datanode object this must be run by invoking link datanode rundatanodedaemon subsequently 
 instantiate a single datanode object along with its secure resources this must be run by invoking link datanode rundatanodedaemon subsequently 
 instantiate start a single datanode daemon and wait for it to finish if this thread is specifically interrupted it will stop waiting 
 instantiate start a single datanode daemon and wait for it to finish if this thread is specifically interrupted it will stop waiting 
 make an instance of datanode after ensuring that at least one of the given data directories and their parent directories if necessary can be created 
 allows submission of a disk balancer job 
 cancels a running plan 
 returns the status of current or last executed work plan 
 gets a runtime configuration value from diskbalancer instance for example diskbalancer bandwidth 
 checks whether there are any files anywhere in the directory tree rooted at dir directories don t count as files dir must exist
 prepare a storage directory it creates a builder which can be used to add to the volume if the volume cannot be added it is ok to discard the builder later 
 add a list of volumes to be managed by datastorage if the volume is empty format it otherwise recover it from previous transitions if required 
 analyze storage directories for a specific block pool recover from previous transitions if required perform fs state transition if necessary depending on the namespace info read storage info this method should be synchronized between multiple dn threads only the first dn thread does dn level storage dir recovertransitionread 
 read version file for rollback 
 analize which and whether a transition of the fs state is required and perform it if necessary rollback if the rollback startup option was specified upgrade if this lv  layout version regular startup if this lv   layout version
 upgrade from a pre federation layout move current storage into a backup directory and hardlink all its blocks into the new current directory upgrade from pre   to   or later release e g           if previous exists then delete it rename current to previous tmp create new current current directory hard links for block files are created from previous tmp to current current saves new version file in current current directory rename previous tmp to previous there should be only one namenode in the cluster for first time upgrade to  
 rolling back to a snapshot in previous directory by moving it to current directory rollback procedure if previous directory exists rename current to removed tmp rename previous to current remove removed tmp if previous directory does not exist and the current version supports federation perform a simple rollback of layout version this does not involve saving restoration of actual data 
 hardlink all finalized and rbw blocks in fromdir to todir
 utility function for sending a response 
 wait until the bp is registered upto the configured amount of time throws an exception if times out which should fail the client request 
 main program loop for directoryscanner runs link reconcile and handles any exceptions 
 stops the directory scanner this method will wait for  minute for the main thread to exit and an additional  minute for the report compilation threads to exit if a thread does not exit in that time period it is left running and an error is logged 
 get the lists of blocks on the disks in the dataset sorted by blockid the returned map contains one entry per blockpool keyed by the blockpool id 
 called by the thread before each potential disk scan so that a pause can be optionally inserted to limit the number of scans per second the limit is controlled by link dfsconfigkeys dfs datanode directoryscan throttle limit ms per sec key 
 shutdown the executor 
 takes a client submitted plan and converts into a set of work items that can be executed by the blockmover 
 returns the current work status of a previously submitted plan 
 cancels a running plan 
 returns a volume id to volume base path map 
 returns the current bandwidth 
 verifies that user provided plan is valid 
 convert a node plan to diskbalancerworkitem that datanode can execute 
 copies blocks from a set of volumes 
 initialize output input streams for transferring data to target and send create block request 
 read from minimum source dns required for reconstruction in the iteration first try the success list which we think they are the best dns if source dn is corrupt or slow try to read some other source dn and will update the success list remember the updated success list and return it for following operations and next iteration read 
 see link flushable flush 
 create a file 
 create a fileinputstream using link fileinputstream fileinputstream file wraps the created input stream to intercept read calls before delegating to the wrapped stream 
 create a fileoutputstream using link fileoutputstream fileoutputstream file boolean wraps the created output stream to intercept write calls before delegating to the wrapped stream 
 create a fileoutputstream using link fileoutputstream fileoutputstream file boolean wraps the created output stream to intercept write calls before delegating to the wrapped stream 
 create a fileinputstream using link nativeio getsharedeletefiledescriptor wraps the created input stream to intercept input calls before delegating to the wrapped stream 
 create a fileinputstream using link fileinputstream fileinputstream file and positionit at the given offset wraps the created input stream to intercept read calls before delegating to the wrapped stream 
 create a randomaccessfile using link randomaccessfile randomaccessfile file string wraps the created input stream to intercept io calls before delegating to the wrapped randomaccessfile 
 move the src file to the target using link files move path path copyoption 
 read in the cached du value and return it if it is less than cacheddfsusedchecktime which is set by dfs datanode cached dfsused check interval ms parameter slight imprecision of dfsused is not critical and skipping du can significantly shorten the startup time if the cached value is not available or too old  is returned 
 write the current dfsused to the cache file 
 move replicas in the lazy persist directory to their corresponding locations in the finalized directory 
 add replicas under the given directory to the volume map
 find out the number of bytes in the block that match its crc this algorithm assumes that data corruption caused by unexpected datanode shutdown occurs only in the last crc chunk so it checks only the last chunk 
 an fsdataset has a directory where it loads its data files 
 find the block s on disk length
 get file name for a given block 
 returns handles to the block file and its metadata file
 copy the block and meta files for the given block to the given destination 
 move block files from one storage to another storage 
 moves a block from a given volume to another 
 finalizes newreplica by calling finalizereplica internally 
 moves a given block from one volume to another volume this is used by disk balancer 
 compute and store the checksum for a block file that does not already have its checksum computed 
 sets the offset in the meta file so that the last checksum will be overwritten 
 complete the block write 
 check if a block is valid 
 does the block exist and have the given state 
 we re informed that a block is no longer valid delete it 
 register the fsdataset mbean using the name hadoop service datanode name fsdatasetstate 
 static version of link initreplicarecovery recoveringblock 
 attempt to evict one or more transient block replicas until we have at least bytesneeded bytes free 
 attempt to evict blocks from cache manager to free the requested bytes 
 find the meta file for the specified block file and then return the generation stamp from the name of the meta file 
 find the corresponding meta data file from a given block file 
 compute the checksum for a block file that does not already have its checksum computed and save it to dstmeta file 
 unplanned non dfs usage i e extra usage beyond reserved 
 temporary files they get moved to the finalized block directory when the block is finalized 
 rbw files they get moved to the finalized block directory when the block is finalized 
 param bytesreserved space that was reserved duringblock creation now that the block is being finalized we can free up this space 
 get next volume 
 get next volume 
 wait for the reference of the volume removed from a previous link removevolume fsvolumeimpl call to be released 
 load the block mmap and mlock the block and then verify its checksum 
 verifies the block s checksum this is an io intensive operation 
 reads bytes into a buffer until eof or the buffer s limit is reached
 create an object with a data input stream and a checksum input stream 
 create an object with a data output stream a checksum output stream and a checksum 
 this function breaks hardlinks to the current replica file when doing a datanode upgrade we create a bunch of hardlinks to each block file this cleverly ensures that both the old and the new storage directories can contain the same block file without using additional space for the data however when we want to append to the replica file we need to break the hardlink to ensure that the old snapshot continues to contain the old data length if we failed to do that we could roll back to the previous directory during a downgrade and find that the block contents were longer than they were at the time of upgrade 
 interrupt the writing thread and wait until it dies 
 acquire privileged resources i e the privileged ports for the data node the privileged resources consist of the port of the rpc server and the port of http not https server 
 handle a dfsclient request to create a new memory segment 
 create physical directory for block pools on the data node 
 executes the client calls 
 gets the node set from a file or a string 
 returns a diskbalancer node list from the cluster or null if not found 
 copied from dfsadmin java creates a connection to datanode 
 parse top number of nodes to be processed 
 reads the physical path of the disks we are balancing this is needed to make the disk balancer human friendly and not used in balancing 
 executes the client calls 
 runs the plan command this command can be run with various options like plan node ip plan node hostname plan node datanodeuuid
 constructs an appropriate connector based on the url 
 getnodes function returns a list of diskbalancerdatanodes 
 writes a snapshot of the cluster to the specified directory 
 computes a node plan for the given node 
 computes steps to make a diskbalancervolumeset balanced 
 get the aggregated value for a method for all nameservices 
 get the aggregated value for a method for all nameservices 
 fetches the value for a field name 
 picks the most relevant record registration that matches the query return registrations matching the query in this preference  most recently updated active registration  most recently updated standby registration if showstandby  most recently updated unavailable registration if showunavailable expired registrations are ignored 
 replaces the current in memory cached of the mount table with a new version fetched from the data store 
 get the mount table entry for a path 
 get all the mount records at or beneath a given path 
 create a new proxy wrapper for a client nn connection 
 get a jmx data from a web endpoint 
 create an instance of an interface with a constructor using a context 
 get the status report for the namenode monitored by this heartbeater 
 set quota for the federation path 
 get quota usage for the federation path 
 get valid quota remote locations used in link getquotausage string differentiate the method link getquotaremotelocations string this method will do some additional filtering 
 get the represented java method 
 set the current rpc socket for the router 
 get all the existing mount tables 
 get mount tables which quota was set during this time the quota usage cache will also be updated by quota manager  stale paths entries will be removed  existing entries will be override and updated 
 write out updated mount table entries into state store 
 if we should retry the rpc call 
 invokes a method against the clientprotocol proxy server if a standby exception is generated by the call to the client retries using the alternate server re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes a method on the designated object catches exceptions specific to the invocation re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 get a clean copy of the exception sometimes the exceptions returned by the server contain the full stack trace in the message 
 invokes a clientprotocol method determines the target nameservice via a provided block re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes a clientprotocol method determines the target nameservice using the block pool id re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes a clientprotocol method against the specified namespace re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes a single proxy call for a single location re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes sequential proxy calls to different locations continues to invoke calls until a call returns without throwing a remote exception 
 invokes sequential proxy calls to different locations continues to invoke calls until the success condition is met or until all locations have been attempted the success condition may be specified by an expected result class an expected result value if no expected result class values are specified the success condition is a call that does not throw a remote exception 
 invoke multiple concurrent proxy calls to different clients returns an array of results re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes multiple concurrent proxy calls to different clients returns an array of results re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 invokes multiple concurrent proxy calls to different clients returns an array of results re throws exceptions generated by the remote rpc call as either remoteexception or ioexception 
 get the nameservice identifier for a block pool 
 check if the router is in safe mode we should only see read write and unchecked it includes a default handler when we haven t implemented an operation if not supported it always throws an exception reporting the operation 
 the the delegation token from each name service 
 get the location to create a file it checks if the file already existed in one of the locations 
 excluded and favored nodes are not verified and will be ignored by placement policy if they are not in the same nameservice as the file 
 excluded nodes are not verified and will be ignored by placement if they are not in the same nameservice as the file 
 datanode are not verified to be in the same nameservice as the old block todo this may require validation 
 determines combinations of eligible src dst locations for a rename a rename cannot change the namespace renames are only allowed if there is an eligible dst location in the same namespace as the source 
 get the datanode report with a timeout 
 locate the location with the matching block pool id 
 create a new file status for a mount point 
 updates the state store with any record overrides we detected such as an expired state 
 get all the cached records 
 get all the cached records and the time stamp of the cache 
 read all lines from a file and deserialize into the desired record type 
 overwrite the existing data with a new data set 
 overwrite the existing data with a new data set replaces all records in the data store for this record class if all records in the data store are not successfully committed this function must return false and leave the data store unchanged 
 get the host name of the machine running the driver for debugging 
 create or return the cached protobuf builder for this translator 
 constructor for a mount table entry with a single destinations 
 build a state store api implementation interface 
 initialize the state store and the connection to the backend 
 update the cache for a specific record store 
 return whether the migration requires next round 
 filters discards any existing acl entries that have the same scope type and name of any entry in the acl spec if necessary recalculates the mask entries if necessary default entries may be inferred by copying the permissions of the corresponding access entries it is invalid to request removal of the mask entry from an acl that would otherwise require a mask entry due to existing named entries or an unnamed group entry 
 filters discards any existing default acl entries the new acl retains only the access acl entries 
 merges the entries of the acl spec into the existing acl if necessary recalculates the mask entries if necessary default entries may be inferred by copying the permissions of the corresponding access entries 
 completely replaces the acl with the entries of the acl spec if necessary recalculates the mask entries if necessary default entries are inferred by copying the permissions of the corresponding access entries replacement occurs separately for each of the access acl and the default acl if the acl spec contains only access entries then the existing default entries are retained if the acl spec contains only default entries then the existing access entries are retained if the acl spec contains both access and default entries then both are replaced 
 builds the final list of acl entries to return by trimming sorting and validating the acl entries that have been added 
 creates a validatedaclspec by pre validating and sorting the given acl entries pre validation checks that it does not exceed the maximum entries this check is performed before modifying the acl and it s actually insufficient for enforcing the maximum number of entries transformation logic can create additional entries automatically such as the mask and some of the default entries so we also need additional checks during transformation the up front check is still valuable here so that we don t run a lot of expensive transformation logic while holding the namesystem lock for an attacker who intentionally sent a huge acl spec 
 receive a batch of edits from the namenode depending on bnstate different actions are taken see link backupimage bnstate 
 transition the backupnode from journal only state to in sync state this is done by repeated invocations of tryconvergejournalspool until we are caught up to the latest in progress edits file 
 after link freezenamespaceatnextroll has been called wait untilthe bn receives notification of the next log roll 
 register this backup node with the active name node 
 modifies a directive skipping most error checking this is for careful internal use only modifydirective can be non deterministic since its error checking depends on current system time which poses a problem for edit log replay 
 saves the current state of the cachemanager to the dataoutput used to persist cachemanager state in the fsimage 
 reloads cachemanager state from the passed datainput used during namenode startup to restore cachemanager state from an fsimage 
 save cache pools to fsimage
 load cache pools from fsimage
 load cache directives from the fsimage
 create a new checkpoint
 throws quotaexceededexception if namespace storagespace or storage typespace quota is violated after applying the deltas 
 param file file being scanned and validated 
 read the header of fsedit log
 creates output buffers and file object 
 create empty edits logs file 
 write header information for this editlogfileoutputstream to the provided dataoutputsream 
 flush ready buffer to persistent store currentbuffer is not flushed as it accumulates new log records while readybuffer will be flushed and synced 
 read an operation from the stream
 go through the next operation from the stream storage 
 skip edit log operations up to a given transaction id or until the end of the edit log is reached after this function returns the next call to readop will return either end of file null or a transaction with a txid equal to or higher than the one we asked for 
 flush data to persistent store collect sync metrics 
 throws an exception if the provided path cannot be renamed into the destination because of differing parent encryption zones called while holding the fsdirectory lock 
 cursor based listing of encryption zones called while holding the fsdirectory lock 
 re encrypts the given encryption zone path if the given path is not the root of an encryption zone an exception is thrown 
 cancels the currently running re encryption of the given encryption zone if the given path is not the root of an encryption zone an exception is thrown 
 cursor based listing of zone re encryption status called while holding the fsdirectory lock 
 return whether an inode is an encryption zone root 
 find all editlog segments starting at or above the given txid 
 discard all editlog segments whose first txid is greater than or equal to the given txid by renaming them with suffix trash 
 returns matching edit logs via the log directory simple helper function that lists the files in the logdir and calls matcheditlogs file 
 handle fsck request 
 append to an existing file the method returns the last block of the file if this is a partial block which can still be used for writing more data the client uses the returned block locations to form the data pipeline for this block the link locatedblock will be null if the last block is full the client then allocates a new block with the next call using link org apache hadoop hdfs protocol clientprotocol addblock for description of parameters and exceptions thrown see link org apache hadoop hdfs protocol clientprotocol append 
 convert current node to under construction recreate in memory lease record 
 set the namespace storagespace and typespace quota for a directory note this does not support inodes relative path 
 concat all the blocks from srcs to trg and delete the srcs files
 remove a file directory from the namespace for large directories deletion is incremental the blocks under the directory are collected and deleted a small number at a time holding the link fsnamesystem lock for small directory or file the deletion is done in one shot 
 remove a file directory from the namespace for large directories deletion is incremental the blocks under the directory are collected and deleted a small number at a time holding the link org apache hadoop hdfs server namenode fsnamesystem lock for small directory or file the deletion is done in one shot 
 resolves a given path into an inodesinpath all ancestor inodes that exist are validated as traversable directories symlinks in the ancestry will generate an unresolvedlinkexception the returned iip will be an accessible path that also passed additional sanity checks based on how the path will be used as specified by the dirop read expands reserved paths and performs permission checks during traversal raw paths are only accessible by a superuser write in addition to read checks ensures the path is not a snapshot path create in addition to write checks ensures path does not contain illegal character sequences 
 updates namespace storagespace and typespaces consumed for all directories until the parent directory of file represented by path 
 update usage count without replication factor change
 update usage count with replication factor change due to setreplication
 update count of each inode with quota
 update quota of each inode and check to see if quota is exceeded see link updatecount inodesinpath int quotacounts boolean 
 update the cached quota space for a block that is being completed must only be called once as the block is being completed 
 add the given child to the namespace 
 add a child to the end of the path specified by inodesinpath 
 resolve a reserved path to a non reserved path there are two special hierarchies under reserved reserved inodes performs a path lookup by inodeid reserved raw returns the encrypted raw bytes of a file in an encryption zone for instance if ezone is an encryption zone then ezone a refers to the decrypted file and reserved raw ezone a refers to the encrypted raw bytes of ezone a pathnames in the reserved raw directory that resolve to files not in an encryption zone are equivalent to the corresponding non raw path hence if a b c refers to a file that is not in an encryption zone then reserved raw a b c is equivalent they both refer to the same unencrypted file 
 resolves the given path into inodes reserved paths are not handled and permissions are not verified client supplied paths should be resolved via link resolvepath fspermissionchecker string dirop this method should only be used by internal methods 
 get link inode associated with the file directory see link getinode string dirop 
 get link inode associated with the file directory see link getinode string dirop 
 get link inode associated with the file directory 
 check whether current user have permissions to access the path for more details of the parameters see link fspermissionchecker checkpermission 
 update re encryption progress submitted caller should logsync after calling this outside of the fsn lock the reencryption status is updated during setxattrs 
 update re encryption progress start checkpoint caller should logsync after calling this outside of the fsn lock the reencryption status is updated during setxattrs original reencryption status is passed in to get existing information such as ezkeyversionname and submissiontime 
 log re encrypt complete cancel or  re encrypt to edits caller should logsync after calling this outside of the fsn lock original reencryption status is passed in to get existing information this should include whether it is finished due to cancellation the reencryption status is updated during setxattrs for completion time 
 if the file is in an encryption zone we optimistically create an edek for the file by calling out to the configured keyprovider since this typically involves doing an rpc the fsn lock is yielded since the path can flip flop between being in an encryption zone and not in the meantime the call must re resolve the iip and re check preconditions if this method does not return null 
 get the current key version name for the given ez this will first drain the provider s local cache then generate a new edek the encryption key version of the newly generated edek will be used as the target key version of this re encryption meaning all edeks keyversion are compared with it and only sent to the kms for re encryption when the version is different note keyprovider has a getcurrentkey interface but that is under a different acl hdfs should not try to operate on additional acls but rather use the generate acl it already has 
 set an erasure coding policy on the given path 
 unset erasure coding policy from the given directory 
 get the erasure coding policy information for specified path 
 check if the file or directory has an erasure coding policy 
 get the erasure coding policy including the replication policy this does not do any permission checking 
 for a given absolute path create all ancestors as directories along the path all ancestors inherit their parent s permission plus an implicit uwx permission this is used by create and addsymlink for implicitly creating all directories along the path for example path  foo bar spam foo is an existing directory foo bar is not existing yet the function will create directory bar 
 create all ancestor directories and return the parent inodes 
 note this is to be used by link fseditlogloader only 
 change a path name
 the new rename which has the posix semantic 
 see link unprotectedrenameto fsdirectory string string inodesinpath inodesinpath long blocksmapupdateinfo options rename 
 rename src to dst note this is to be used by link org apache hadoop hdfs server namenode fseditlogloader only 
 rename src to dst see link distributedfilesystem rename path path options rename for details related to rename semantics and exceptions 
 deprecated use link renametoint fsdirectory string string boolean options rename 
 create a snapshot
 check if the given inode or one of its descendants is snapshottable and already has snapshots 
 check if the given path or one of its descendants is snapshottable and already has snapshots 
 get the file info for a specific file 
 get a partial listing of the indicated directory we will stop when any of the following conditions is met  this lslimit files have been added  needlocation is true and enough files have been added such that at least this lslimit block locations are in the response
 get a listing of all the snapshots of a snapshottable directory
 get the file info for a specific file 
 create a hdfs file status from an iip 
 add the given symbolic link to the fs record it in the edits log 
 truncate a file to a given size 
 unprotected truncate implementation unlike link fsdirtruncateop truncate this will not schedule block recovery 
 truncate has the following properties  any block deletions occur now  inode length is truncated now new clients can only read up to the truncated length  inode will be set to uc and lastblock set to under recovery  nn will trigger dn truncation recovery and waits for dns to report  file is considered under recovery until truncation recovery completes  soft and hard lease expiration require truncation recovery to complete 
 part i of getadditionalblock analyze the state of the file under read lock to determine if the client can add a new block detect potential retries lease mismatches and minimal replication of the penultimate block generate target datanode locations for the new block but do not create the new block yet 
 part ii of getadditionalblock should repeat the same analysis of the file state as in part  but under the write lock if the conditions still hold then allocate a new block with the new targets add it to the inode and to the blocksmap 
 create a new file or overwrite an existing file once the file is create the client then allocates a new block with the next call using link clientprotocol addblock for description of parameters and exceptions thrown see link clientprotocol create 
 save allocated block at the given pending filename
 set xattr for a file or directory 
 remove an xattr for a file or directory 
 remove xattrs from the inode and return the removed xattrs 
 initialize the output stream for logging opening the first log segment 
 finalizes the current edit log and opens a new log segment 
 remote namenode just has started a log segment start log segment locally 
 select a list of input streams 
 construct a custom journal manager the class to construct is taken from the configuration 
 load an edit log and apply the changes to the in memory structure this is where we apply edits that we ve been writing to disk all along 
 write an operation to the output stream
 read an operation from the input stream note that the objects returned from this method may be re used by future calls to the same method 
 decode the opcode frame this includes reading the opcode and transaction id and validating the checksum and length it does not include reading the opcode specific fields the input stream will be advanced to the end of the op at the end of this function 
 analyze storage directories recover from previous transitions if required perform fs state transition if necessary depending on the namespace info read storage info 
 load image from a checkpoint directory and save it into the current one 
 toss the current image and namesystem reloading from the specified file 
 choose latest image from one of the directories load it and merge with the edits saving and loading fsimage should never trigger symlink resolution the paths that are persisted do not have intermediate symlinks because intermediate symlinks are resolved at the time files directories and symlinks are created all paths accessed while loading or saving fsimage should therefore only see symlinks as the final path component and the functions called below do not resolve symlinks that are the final path component 
 rollback for rolling upgrade 
 load the specified list of edit files into the image 
 load the image namespace from the given image file verifying it against the md sum stored in its associated md file 
 param timewindow a checkpoint is done if the latest checkpointwas done more than this number of seconds ago 
 save the contents of the fs image to a new image file in each of the current storage directories 
 see savefsimageinalldirs fsnamesystem namenodefile long canceler 
 rename fsimage with the specific txid
 rename all the fsimage files with the specific namenodefile type the associated checksum files will also be renamed 
 start checkpoint if backup storage contains image that is newer than or incompatible with what the active name node has then the backup node should shutdown if the backup image is older than the active one then it should be discarded and downloaded from the active node if the images are the same then the backup image will be used as current 
 this is called by the nn after having downloaded an image and by the nn after having received a new image from the nn it renames the image from fsimage n ckpt to fsimage n and also saves the related md file into place 
 create a compression instance based on the user s configuration in the given configuration object 
 create a compression instance based on a header read from an input stream 
 write out a header to the given stream that indicates the chosen compression codec and return the same stream wrapped with that codec if no codec is specified simply adds buffering to the stream so that the returned stream is always buffered 
 load fsimage files when  only local names are stored and  snapshot is supported 
 load fsimage files assuming only local names are stored used when snapshots are not supported by the layout version 
 load information about root and use the information to update the root directory of namesystem 
 load children nodes for the parent directory 
 load a directory when snapshot is supported 
 load all children of a directory
 load fsimage files assuming full path names are stored
 add the child node to parent and if child is a file update block map this method is only used for image loading so that synchronization modification time update and space count update are not needed 
 load an inode from fsimage except for its name
 load link inodefileattributes 
 when upgrading from an old version the filesystem could contain paths that are now reserved in the new version e g snapshot this renames these new reserved paths to a user specified value to avoid collisions with the reserved name 
 save children inodes 
 save file tree image starting from the given root this is a recursive procedure which first saves all children and snapshot diffs of a current directory and then moves inside the sub directories 
 serializes leases 
 determine the checkpoint time of the specified storagedirectory
 serialize a link inodefile node
 serialize an link inodefileattributes 
 serialize a link inodedirectory 
 serialize a link inodedirectory 
 serialize a link inodesymlink node
 serialize a link inodereference node 
 save one inode s attributes to the image 
 write the byte value 
 write an array of blocks as compactly as possible this uses delta encoding for the generation stamp and size following the principle that genstamp increases relatively slowly and size is equal for all but the last block of a file 
 check the supplied configuration for correctness 
 instantiates an fsnamesystem loaded from the image and edits directories specified in the passed configuration 
 return an ordered list of edits directories to write to the list is ordered such that all shared edits directories are ordered before non shared directories and any duplicates are removed the order they are specified in the configuration is retained 
 lock the checkpoint lock interrupibly 
 return a list of blocks their locations on datanode whose total size is size
 dump all metadata into specified file
 list open files in the system in batches previd is the cursor inode id and the open files returned in a batch will have their inode ids greater than this cursor open files can only be requested by super user and the the list across batches does not represent a consistent view of all open files todo hdfs  to report open files by type 
 set permissions for an existing file 
 set owner for an existing file 
 get block locations within the specified range 
 moves all the blocks from code srcs and appends them to code target to avoid rollbacks we will verify validity of all of the args before we start actual move this does not support inodes relative path
 stores the modification and access time for this inode the access time is precise up to an hour the transaction if needed is written to the edits log but is not flushed 
 truncate file to a lower length truncate cannot be reverted recovered from as it causes data loss truncation at block boundary is atomic otherwise it requires block recovery to truncate the last block of the file 
 create a symbolic link 
 set replication for an existing file the namenode sets new replication and schedules either replication of under replicated data blocks or removal of the excessive block copies if the blocks are over replicated 
 set the storage policy for a file or a directory 
 unset storage policy set for a given file or a directory 
 get the storage policy for a file or a directory 
 return all the existing block storage policies
 create a new file entry in the namespace for description of parameters and exceptions thrown see link clientprotocol create except it returns valid file status upon success
 recover lease immediately revoke the lease of the current lease holder and start lease recovery so that the file can be forced to be closed 
 append to an existing file in the namespace 
 the client would like to obtain an additional block for the indicated filename which is being written to return an array that consists of the block plus a set of machines the first on this list should be where the client writes data subsequent items in the list must be provided in the connection to the first datanode make sure the previous blocks have been reported by datanodes and are replicated will return an empty  elt array if we want the client to try again later 
 see clientprotocol getadditionaldatanode 
 the client would like to let go of the given block
 complete in progress write to the given file 
 create new block with a unique block id and a new generation stamp 
 change the indicated filename 
 remove the indicated file from namespace 
 get the file info for a specific file 
 returns true if the file is closed
 create all the necessary directories
 get the content summary for a specific file dir 
 get the quota usage for a specific file dir 
 set the namespace quota and storage space quota for a directory see link clientprotocol setquota string long long storagetype for thecontract note this does not support inodes relative path 
 persist all metadata about this file 
 move a file that is being written to be immutable 
 param pendingfile open file that needs to be closed
 renew the lease s held by the given client
 get a partial listing of the indicated directory
 save namespace image this will save current namespace into fsimage file and empty edits file requires superuser privilege and safe mode 
 enables disables checks restoring failed storage replicas if the storage becomes available again requires superuser privilege 
 register following mbeans with their respective names fsnamesystemmbean hadoop service namenode name fsnamesystemstate replicatedblocksmbean hadoop service namenode name replicatedblocksstate ecblockgroupsmbean hadoop service namenode name ecblockgroupsstate 
 increments logs and then returns the stamp
 increments logs and then returns the block id
 client is reporting some bad block locations 
 get a new generation stamp together with an access token for a block under construction this method is called for recovering a failed write or setting up a block for appended 
 update a pipeline for a block under construction
 release unregister backup node find and remove the backup stream corresponding to the node 
 param path restrict corrupt files to this portion of namespace 
 convert string cookie to integer 
 param renewer renewer information
 param token token to renew
 param token token to cancel
 return true if delegation token operation is allowed
 returns authentication method used to establish the connection
 log fsck event in the audit log 
 get the list of corrupt blocks and corresponding full file path including snapshots in given snapshottable directories 
 allow snapshot on a directory 
 disallow snapshot on a directory 
 create a snapshot
 rename a snapshot
 get the list of snapshottable directories that are owned by the current user return all the snapshottable directories if the current user is a super user 
 get the difference between two snapshots or between a snapshot and the current status of a snapshottable directory 
 get the difference between two snapshots or between a snapshot and the current status of a snapshottable directory 
 delete a snapshot of a snapshottable directory
 update internal state to indicate that a rolling upgrade is in progress 
 update internal state to indicate that a rolling upgrade is in progress for non ha setup this requires the namesystem is in safemode and after doing a checkpoint for rollback the namesystem will quit the safemode automatically 
 create an encryption zone on directory src using the specified key 
 get the encryption zone for the specified path 
 set an erasure coding policy on the given path 
 add multiple erasure coding policies to the erasurecodingpolicymanager 
 remove an erasure coding policy 
 enable an erasure coding policy 
 disable an erasure coding policy 
 unset an erasure coding policy from the given path 
 get the erasure coding policy information for specified path
 get all erasure coding polices 
 get available erasure coding codecs and corresponding coders 
 guarded by link fsnamesystem readlock 
 guarded by link fsnamesystem readlock 
 guarded by link fsnamesystem readlock 
 verifies that all existing ancestors are directories if a permission checker is provided then the user must have exec access ancestor symlinks will throw an unresolved exception and resolvelink determines if the last inode will throw an unresolved exception this method should always be called after a path is resolved into an iip 
 this is called when using bootstrapstandby for ha upgrade the sbn should also create previous directory so that later when it starts it understands that the cluster is in the upgrade state this function renames the old current directory to previous tmp 
 trigger the active node to roll its logs 
 internal method to move from the existing state to a new state 
 determine the address of the nn we are checkpointing as well as our own http address from the configuration 
 compute link contentsummary blocking call 
 compute link contentsummary 
 check and add namespace storagespace storagetype consumed to itself and the ancestors 
 check and add namespace storagespace storagetype consumed to itself and the ancestors 
 this method is usually called by the undo section of rename before calling this function in the rename operation we replace the original src node of the rename operation with a reference node withname instance in both the children list and a created list delete the reference node from the children list and add it to the corresponding deleted list to undo the above operations we have the following steps in particular  remove the withname node from the deleted list if it exists  replace the withname node in the created list with srcchild  add srcchild back as a child of srcparent note that we already add the node into the created list of a snapshot diff in step  we do not need to add srcchild to the created list of the latest snapshot we do not need to update quota usage because the old child is in the deleted list before 
 undo the rename operation for the dst tree i e if the rename operation with overwrite option removes a file dir from the dst tree add it back and delete possible record in the deleted list 
 the same as valueof inode path false 
 close the journal and stream
 apply the given operation across all of the journal managers disabling any for which the closure throws an ioexception 
 get link inodesinpath for all link inode in the systemwhich has a valid lease 
 check leases periodically 
 display a prompt to the user and get his or her choice 
 format a new filesystem destroys any filesystem that may already exist at this location 
 initialize name node 
 start the services common to active and standby states 
 verify that configured directories exist then interactively confirm that formatting is desired for each existing directory and format them 
 format a new shared edits dir and copy in enough edit log segments so that the standby nn can start up 
 
 check files on dfs starting from the indicated path 
 see dfsutil gethttppolicy org apache hadoop conf configuration for information related to the different configuration options and http policy is decided 
 create a namenoderesourcechecker which will check the edits dirs and any additional dirs to check set in conf 
 add the volume of the passed in directory to the list of volumes to check if required is true and this volume is already present but is marked redundant it will be marked required if the volume is already present but marked required then this method is a no op 
 the client needs to give up on the block 
 the client has detected an error on the specified located blocks and is reporting them to the server for now the namenode will mark the block as corrupt in the future we might check the blocks are actually corrupt 
 tell all datanodes to use a new non persistent bandwidth value for dfs datanode balance bandwidthpersec 
 construct the nnstorage 
 see link nnstorage setstoragedirectories collection collection collection 
 set the storage directories which will be used this should only ever be called from inside nnstorage however it needs to remain package private for testing as storagedirectories need to be reinitialised after using mockito spy on this class as mockito doesn t work well with inner classes such as storagedirectory in this case synchronized due to initialization of storagedirs and removedstoragedirs 
 retrieve current directories of type image 
 retrieve current directories of type edits 
 write a small file in all available storage directories that indicates that the namespace has reached some given transaction id this is used when the image is loaded to avoid accidental rollbacks in the case where an edit log is fully deleted but there is no checkpoint see testnameeditsconfigs testnameeditsconfigsfailure 
 create new dfs name directory caution this destroys all files in this filesystem 
 format all available storage directories 
 processes the startup options for the clusterid and blockpoolid for the upgrade 
 iterate over all of the storage dirs reading their contents to determine their layout versions returns an fsimagestorageinspector which has inspected each directory note this can mutate the storage info fields ctime version etc 
 delete old oiv fsimages since the target dir is not a full blown storage directory we simply list and keep the latest ones for the same reason no storage inspector is used 
 perform any steps that must succeed across all storage dirs journalmanagers involved in an upgrade before proceeding onto the actual upgrade stage if a call to any jm s or local storage dir s dopreupgrade method fails then doupgrade will not be called for any jm the existing current dir is renamed to previous tmp and then a new empty current dir is created 
 main loop it takes at most  zone per scan and executes until the zone is completed see reencryptencryptionzoneint long 
 re encrypts a zone by recursively iterating all paths inside the zone in lexicographic order files are re encrypted and subdirs are processed during iteration 
 restore the re encryption from the progress inside reencryptionstatus this means start from exactly the lastprocessedfile lpf skipping all earlier paths in lexicographic order lexicographically later directories on the lpf parent paths are added to subdirs 
 iterate through all files directly inside parent and recurse down directories the listing is done in batch and can optionally start after a position each batch is then send to the threadpool where kms will be contacted and edek re encrypted link reencryptionupdater handles the tasks completedfrom the threadpool the iteration of the inode tree is done in a depth first fashion but instead of holding all inodedirectory s in memory on the fly only the path components to the current inode is held this is to reduce memory consumption 
 iterates the parent directory and add direct children files to current batch if batch size meets configured threshold a callable is created and sent to the thread pool which will communicate to the kms to get new edeks locks could be released and reacquired when a callable is created 
 throttles the reencryptionhandler in  aspects  prevents generating more callables than the cpu could possibly handle  prevents generating more callables than the reencryptionupdater can handle under its own throttling  prevents contending fsn fsd read locks this is done based on the dfs namenode reencrypt throttle limit ratio key configuration item  and  are to control nn heap usage 
 process an inode for re encryption add to current batch if it s a file no op otherwise 
 iterate tasks for the given zone and update progress accordingly the checkpoint indicates all files before it are done re encryption so it will be updated to the position where all tasks before are completed 
 throttles the reencryptionupdater to prevent from contending fsn fsd write locks this is done by the configuration 
 initialize secondarynamenode 
 shut down this instance of the datanode returns only after shutdown is complete 
 returns the jetty server that the namenode is listening on 
 create a new checkpoint
 param opts the parameters passed to this program 
 main has some simple utility methods 
 compute the difference between two snapshots or a snapshot and the current directory of the directory the diff calculation can be scoped to either the snapshot root or any descendant directory under the snapshot root 
 compute the difference between two snapshots or a snapshot and the current directory of the directory the diff calculation can be scoped to either the snapshot root or any descendant directory under the snapshot root 
 serialize link created 
 serialize link deleted 
 serialize to out 
 the sequence of the ref node in reflist must be strictly the same with the sequence in fsimage
 load the snapshots section from fsimage also add snapshottable feature to snapshottable directories 
 load the snapshot diff section from fsimage 
 load directorydiff list for a directory with snapshot feature 
 this can only be called after serializing both inode dir and snapshotdiff
 save all the snapshot diff to fsimage
 serialize the fields to out 
 save snapshots and snapshot quota for a snapshottable directory 
 save snapshotdiff list for an inodedirectorywithsnapshot 
 load the created list from fsimage 
 load the deleted list from the fsimage 
 load snapshots and snapshotquota for a snapshottable directory 
 load the link snapshotdiff list for the inodedirectorywithsnapshotdirectory 
 load the snapshotinode field of link abstractinodediff 
 load link directorydiff from fsimage 
 set the given directory as a snapshottable directory if the path is already a snapshottable directory update the quota 
 create a snapshot of the given path it is assumed that the caller will perform synchronization 
 delete a snapshot for a snapshottable directory
 rename the given snapshot
 write link snapshotcounter link numsnapshots and all snapshots to the dataoutput 
 read values of link snapshotcounter link numsnapshots and all snapshots from the datainput
 compute the difference between two snapshots of a directory or between a snapshot of the directory and its current tree 
 compute the partial difference between two snapshots of a directory or between a snapshot of the directory and its current tree 
 requests that the namenode download an image from this node 
 requests that the namenode download an image from this node allows for optional external cancelation 
 a server side method to respond to a getfile http request copies the contents of the local file into the output stream 
 client side method to fetch file from a server copies the response from the url to a list of local files 
 handle http put request for the root 
 handle http put request 
 handle http post request for the root 
 handle http post request 
 handle http get request for the root 
 handle http get request 
 handle http delete request for the root 
 handle http delete request 
 command line interface
 gives a report on how the filesystem is doing 
 safe mode maintenance command usage hdfs dfsadmin safemode enter  leave  get  wait  forceexit 
 command to ask the namenode to save the namespace usage hdfs dfsadmin savenamespace
 command to enable disable check restoring of failed storage replicas in the namenode usage hdfs dfsadmin restorefailedstorage truefalsecheck
 command to ask the namenode to reread the hosts and excluded hosts file usage hdfs dfsadmin refreshnodes
 command to list all the open files currently managed by namenode usage hdfs dfsadmin listopenfiles
 command to get balancer bandwidth for the given datanode usage hdfs dfsadmin getbalancerbandwidth literal 
 download the most recent fsimage from the name node and save it to a local file in the given directory 
 command to ask the namenode to finalize previously performed upgrade usage hdfs dfsadmin finalizeupgrade
 dumps dfs data structures into specified file usage hdfs dfsadmin metasave filename
 display each rack and the nodes assigned to that rack as determined by the namenode in a hierarchical manner the nodes and racks are sorted alphabetically 
 refresh the authorization policy on the link namenode 
 refresh the user to groups mappings on the link namenode 
 refreshsuperusergroupsconfiguration link namenode 
 param argv the parameters passed to this program 
 capture local nn s thread dump and write it to zkfc s log 
 execute the command with the given arguments 
 add a mount table entry or update if it exists 
 set quota for a mount table entry 
 clear quota of the mount point 
 manager the safe mode state 
 print all attributes values
 get single value by key
 throws exception initializes mbeanserver
 factory function that creates an editsvisitor object
 loads edits file uses visitor to process all elements
 create a processor that writes to the file named and may or may not also output to the screen as specified 
 create a processor that writes to the file named and may or may not also output to the screen as specified 
 start visitor initialization 
 finish visitor
 load fsimage into the memory 
 return the json formatted filestatus of the specified file 
 return the json formatted list of the files in the specified directory 
 return the json formatted contentsummary of the specified path 
 return the json formatted xattrnames of the specified file 
 return the json formatted xattrs of the specified file 
 return the json formatted acl status of the specified file 
 return the inodeid of the specified path 
 process cachemanager state from the fsimage 
 process the delegation token related section in fsimage 
 process the inodes under construction section of the fsimage 
 process the blocks section of the fsimage 
 extract the inode permissions stored in the fsimage file 
 extract cachepool permissions stored in the fsimage file 
 process the inode records stored in the fsimage 
 process image with full path name
 process image with local path name and snapshot support
 process directories when snapshot is supported 
 process snapshots of a snapshottable directory
 process children under a directory 
 process image with full path name
 process an inode
 read the next tag start or end event 
 load a subtree of the xml into a node structure we will keep consuming xml events until we exit the current subtree if there are any terminators specified we will always leave the terminating end tag event in the stream 
 permission is serialized as a  bit long       in big endian the first and the second parts are the string ids of the user and group name and the last  bits are the permission bits 
 read the version tag which starts the xml file 
 write the string table to the fsimage 
 processes the xml file back into an fsimage 
 run the offlineimagereconstructor 
 process image file 
 check an fsimage datainputstream s version number the datainput stream is returned at the same point as it was passed in this method has no effect on the datainputstream s read pointer 
 entry point to command line driven operation user may specify options and start fsimage viewer from the command line program will process image file and exit cleanly or if an error is encountered inform user and exit 
 entry point to command line driven operation user may specify options and start fsimage viewer from the command line program will process image file and exit cleanly or if an error is encountered inform user and exit 
 construct a pb fsimage writer to generate text file 
 load the directories in the inode section 
 load the filenames of the directories from the inode section 
 scan the inodedirectory section to construct the namespace 
 create a processor that writes to the file named and may or may not also output to the screen as specified 
 close output stream and prevent further writing
 write parameter to output file and possibly screen 
 start webimageviewer and wait until the thread is interrupted 
 given the numofbytes sent received since last time throttle was called make the current thread sleep if io rate is too fast compared to the given bandwidth allows for optional external cancelation 
 verify that the previously saved md for the given file matches expectedmd 
 read the md file stored alongside the given data file and match the md file content 
 read the md checksum stored alongside the given data file 
 read datafile and compute its md checksum 
 save the md file that lists the mdsum of another file 
 atomically write the given value to the given file including fsyncing 
 demangle a string from an xml document see link manglexmlstring string boolean for a description of themangling format 
 add a sax tag with a string inside 
 pull a string entry from a stanza 
 convert a token object to a json string 
 convert a hdfsfilestatus object to a json string 
 convert a locatedblock to a json map 
 convert a locatedblock to a json array 
 convert locatedblocks to a json string 
 test if the structure generator works fine 
 test if the load generator works fine 
 ensure that even if a file is in a directory with the sticky bit on another user can write to that file assuming correct permissions 
 test that one user can t delete another user s file when the sticky bit is set 
 test basic ability to get and set sticky bits on files and directories 
 test that one user can t rename move another user s file when the sticky bit is set 
 ensure that when we set a sticky bit and shut down the file system we get the sticky bit back on re start and that no extra sticky bits appear after re start 
 tests whether binary avro data files are displayed correctly 
 test hdfs fallback reads hdfs streams support the bytebufferreadable interface 
 test the link bytebufferutil fallbackread function directly 
 test fallback reads on a stream which does not support the bytebufferreadable interface 
 test that we can zero copy read cached data even without disabling checksums 
 test case to ensure that globs work on files with special characters tests with a file pair where one has a r at end and other does not 
 run a glob test on filesystem 
 run a glob test on filecontext 
 perform operations such as setting quota deletion of files rename and ensure system can apply edits log during startup 
 perform operations such as setting quota deletion of files rename and ensure system can apply edits log during startup 
 test opening and reading from an inputstream through a hdfs url first generate a file with some content through the filesystem api then try to open and read the file through the url stream api 
 test opening and reading from an inputstream through a file url 
 decommission or perform maintenance for datanodes and wait for them to reach the expected state 
 decommission or perform maintenance for datanodes and wait for them to reach the expected state 
 decommission or perform maintenance for datanodes and wait for them to reach the expected state 
 start a minidfscluster 
 compare the content of a file created from filesystem and path with the specified byte buffer s content
 create a file of the given size filled with random data 
 get the list of blocks for a file 
 get a blockreader for the given block 
 get a blockreader for the given block 
 test the case where we have multiple threads waiting on the shortcircuitcache delivering a certain shortcircuitreplica in this case there should only be one call to createshortcircuitreplicainfo this one replica should be shared by all threads 
 test the case where we have a failure to complete a short circuit read that occurs and then later on we have a success any thread waiting on a cache load should receive the failure if it occurs however the failure result should not be cached we want to be able to retry later and succeed 
 test that by default reads after a failure does not go through scr 
 test the case where if we disable the cache in link org apache hadoop hdfs shortcircuit domainsocketfactory reads after a failure still goes through scr 
 test that in the case of an error the position and limit of a bytebuffer are left unchanged this is not mandated by bytebufferreadable but clients of this class might immediately issue a retry on failure so it s polite 
 create nfiles with random names and directory hierarchies with random but reproducible data in them 
 keep accessing the given file until the namenode reports that the given block in the file contains the given number of corrupt replicas 
 wait for the given file to reach the given replication factor 
 return url content as string utf  encoding assumed 
 return url contents as a byte array
 load file into byte 
 for link testtransferrbw 
 run a set of operations and generate all edit logs
 helper function to create a key in the key provider defaults to the first indexed namenode s key provider 
 change the length of a block at datanode dnindex 
 creates the metadata of a file in striped layout this method only manipulates the namenode state without injecting data to datanode you should disable periodical heartbeat before use this 
 creates the metadata of a file in striped layout this method only manipulates the namenode state without injecting data to datanode you should disable periodical heartbeat before use this 
 close current file system and create a new instance as given link usergroupinformation 
 construct the actual minidfscluster
 used by builder to create and return an instance of minidfscluster
 note if possible the other constructors that don t have namenode port parameter should be used as they will ensure that the servers use free ports modify the config and start up the servers 
 do the rest of the nn configuration for things like shared edits as well as directory formatting etc for a single nameservice
 return uri of the given namenode in minidfscluster
 modify the config and start up additional datanodes the info port for datanodes is guaranteed to use a free port data nodes can run with the name node in the mini cluster or a real name node for example running with a real name node is useful when running simulated data nodes with a real name node if minicluster s name node is null assume that the conf has been set with the right address port of the name node 
 modify the config and start up additional datanodes the info port for datanodes is guaranteed to use a free port data nodes can run with the name node in the mini cluster or a real name node for example running with a real name node is useful when running simulated data nodes with a real name node if minicluster s name node is null assume that the conf has been set with the right address port of the name node 
 modify the config and start up additional datanodes the info port for datanodes is guaranteed to use a free port data nodes can run with the name node in the mini cluster or a real name node for example running with a real name node is useful when running simulated data nodes with a real name node if minicluster s name node is null assume that the conf has been set with the right address port of the name node 
 modify the config and start up the datanodes the info port for datanodes is guaranteed to use a free port 
 modify the config and start up additional datanodes the info port for datanodes is guaranteed to use a free port data nodes can run with the name node in the mini cluster or a real name node for example running with a real name node is useful when running simulated data nodes with a real name node if minicluster s name node is null assume that the conf has been set with the right address port of the name node 
 finalize cluster for the namenode at the given index 
 if the namenode is running attempt to finalize a previous upgrade when this method return the namenode should be finalized but datanodes may not be since that occurs asynchronously 
 restart all namenodes 
 restart the namenode 
 restart the namenode optionally wait for the cluster to become active 
 restart the namenode at a given index 
 restart the namenode at a given index optionally wait for the cluster to become active 
 return the number of corrupted replicas of the given block 
 return the number of corrupted replicas of the given block 
 corrupt a block on a particular datanode 
 corrupt a block on a particular datanode 
 corrupt the metadata of a block on a datanode 
 corrupt the metadata of a block by deleting it 
 corrupt the metadata of a block by truncating it to a new size 
 restart a datanode
 restart a datanode on the same port if requested
 restart a particular datanode 
 get a client handle to the dfs cluster with a single namenode 
 add a namenode to a federated cluster and start it configuration of datanodes in the cluster is refreshed to register with the new namenode 
 in this test all the node types are uniformly distributed i e a node has equal of being disk ram disk ssd and archive this test did two tests first test runs the old chooserandom approach the second test runs the new chooserandom approach 
 there are two storage types disk and archive disk compose the majority and archive only compose   nodes uniformly distributed this test also runs two tests one with old approach and one with the new approach try to search for archive type devices this test show how new approach can outperform the old one 
 there is only one storage type disk in the cluster and tries to select a disk devices every time also one approach will always succeed in one call this test shows how bad the new approach is compared to the optimal situation 
 this is a helper test can be changed to different distribution each run changing the value percentage   x where x is between   to   will result in different outcomes this is to help understanding what is the boundary that makes the new approach better than the old one the lower x is the less likely the old approach will succeed in one call in which case the new approach is more likely to be better 
 similar to the previous test change the percentage value to understand the performance of the mixed approach more specifically this test takes the approach that it uses old approach for the first try only if the old approach failed in the first try it makes another call with the new approach there is no comparison within this test 
 verifies that peerfromsocketandkey honors socket read timeouts 
 verifies that sasldatatransferclient checktrustandsend should not trust a partially trusted channel 
 run through the creation of a log without any faults injected and count how many rpcs are made to each node this sets the bounds for the other test cases so they can exhaustively explore the space of potential failures 
 sets up two of the nodes to each drop a single rpc at all possible combinations of rpcs this may result in the active writer failing to write after this point a new writer should be able to recover and continue writing without data loss 
 expect link unknownhostexception if a hostname can t be resolved 
 test case in which three journalnodes randomly flip flop between up and down states every time they get an rpc the writer keeps track of the latest acked edit and on every recovery operation ensures that it recovers at least to that point or higher since at any given point a majority of jns may be injecting faults any writer operation is allowed to fail so long as the exception message indicates it failed due to injected faults given a random seed the test should be entirely deterministic 
 regression test for hdfs  one of the journal nodes is down during the writing of one segment then comes back up later to take part in a later segment thus its local edits are not a contiguous sequence this should be handled correctly 
 test the case where the nn crashes after starting a new segment on all nodes but before writing the first transaction to it 
 test the case where at the beginning of a segment transactions have been written to one jn but not others 
 test case where a new writer picks up from an old one with no failures and the previous unfinalized segment entirely consistent i e all the journalnodes end at the same transaction id 
 test case where a new writer picks up from an old one which crashed with the three loggers at different txnids
 test the case where one of the loggers misses a finalizelogsegment call and then misses the next startlogsegment call before coming back to life previously this caused it to keep on writing to the old log segment such that one logger had eg edits   while the others had edits   and edits   this caused recovery to fail in certain cases 
 edit lengths    first recovery sees   x picks length  for recoveryendtxid calls acceptrecovery crashes before finalizing second recovery sees x   should pick recovery length  even though it saw a larger txid because a previous recovery accepted it
 set up the following tricky edge case state which is used by multiple tests initial writer writing to  jns jn jn jn a log segment with txnid  through  succeeds the first transaction in the next segment only goes to jn before the writer crashes eg it is partitioned recovery by another writer the new nn starts recovery and talks to all three thus it sees that the newest log segment which needs recovery is  it sends the preparerecovery  call and decides that the recovery length for  is only the  transaction it sends acceptrecovery   to only jn before crashing this yields the following state jn   finalized  inprogress accepted recovery   jn   finalized  inprogress empty jn   finalized  inprogress empty the empty files got moved aside during recovery 
 test an edge case discovered by randomized testing starts with the edge case state set up by link setupedgecaseonejnhassegmentwithacceptedrecovery recovery  new nn starts recovery and only talks to jn and jn jn has crashed since they have no logs open they say they don t need recovery starts writing segment  and writes  transactions before crashing recovery  jn has come back to life new nn starts recovery and talks to all three all three have segments open from txid  so it calls preparerecovery  jn has an already accepted value for segment  so it replies you should recover   former incorrect behavior nn truncates logs to txid  even though it should have recovered through  in this case even though there is an accepted recovery decision the newer log segments should take precedence since they were written in a newer epoch than the recorded decision 
 test another edge case discovered by randomized testing starts with the edge case state set up by link setupedgecaseonejnhassegmentwithacceptedrecovery recovery  new nn starts recovery and only talks to jn and jn jn has crashed since they have no logs open they say they don t need recovery before writing any transactions jn comes back to life and jn crashes starts writing segment  and writes  transactions before crashing recovery  jn has come back to life jn crashes new nn starts recovery and talks to all three all three have segments open from txid  so it calls preparerecovery  jn has an already accepted value for segment  so it replies you should recover   former incorrect behavior nn truncates logs to txid  even though it should have recovered through  in this case even though there is an accepted recovery decision the newer log segments should take precedence since they were written in a newer epoch than the recorded decision 
 set up the given configuration object to point to the set of journalnodes in this cluster 
 test that the journalnode performs correctly as a paxos acceptor process 
 simple test of how fast the code path is to write edits this isn t a true unit test but can be run manually to check performance at the time of development this test ran in sec on an ssd enabled laptop  ms batch 
 test journalnode sync when a jn id down while nn is actively writing logs and comes back up after some time with no edit log queueing queuing disabled during the cluster setup link setupminicluster 
 does an edit and rolls the edit log 
 does specified number of edits and rolls the edit log 
 tests use of qjm with the defined cluster 
 test reading a file with some blocks data blocks or parity blocks or both deleted or corrupted 
 test block key and token handling 
 test that fast repeated invocations of createclientdatanodeprotocolproxy will not end up using up thousands of sockets this is a regression test for hdfs  
 test link blockpooltokensecretmanager 
 this test writes a file and gets the block locations without closing the file and tests the block token in the last block block token is verified by ensuring it is of correct kind 
 wait until balanced each datanode gives utilization within balance allowed variance of average
 make sure that balancer can t move pinned blocks if specified favorednodes when create file blocks will be pinned use sticky bit 
 verify balancer won t violate the default block placement policy 
 verify balancer won t violate upgrade domain block placement policy 
 this test start a cluster with specified number of nodes and fills it to be  full with a single file replicated identically to all datanodes it then adds one new empty node and starts balancing 
 one node cluster test
 two node cluster test 
 test using a user supplied conf 
 test a cluster with even distribution then a new empty node is added to the cluster
 test unevenly distributed cluster 
 verify balancer exits  on success 
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list in a file
 test a cluster with even distribution g then three nodes are added to the cluster runs balancer with two of the nodes in the exclude list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test a cluster with even distribution then three nodes are added to the cluster runs balancer with two of the nodes in the include list
 test special case two replicas belong to same block should not in same node we have  nodes we have a block in dn ssd and dn disk replica in dn ssd should not be moved to dn ssd otherwise dn has  replicas 
 test running many balancer simultaneously case  first balancer is running now running second one should get another balancer is running exiting ioexception and fail immediately case  when running second balancer balancer id file exists but the lease doesn t exists now the second balancer should run successfully 
 balancer should not move blocks with size  minblocksize 
 test balancer runs fine when logging in with a keytab in kerberized env reusing testunknowndatanode here for basic functionality testing 
 test that makes the balancer to disperse rpcs to the namenode in order to avoid nn s rpc queue saturation 
 param args
 test a cluster with even distribution then a new empty node is added to the cluster test start a cluster with specified number of nodes and fills it to be  full with a single file replicated identically to all datanodes it then adds one new empty node and starts balancing 
 first start a cluster and fill the cluster up to a certain size then redistribute blocks according the required distribution finally balance the cluster 
 this test start a cluster fill the datanodes to be  full it then adds an empty node and start balancing 
 test a cluster with even distribution then a new empty node is added to the cluster
 test unevenly distributed cluster 
 create a cluster with even distribution and a new empty node is added to the cluster then test rack locality for balancer policy 
 create a cluster with even distribution and a new empty node is added to the cluster then test node group locality for balancer policy 
 create a  nodes cluster  nodes n n in rack nodegroup  node n in rack nodegroup and  node n in rack nodegroup fill the cluster to  and  replicas so n and n will have replica for all blocks according to replica placement policy with nodegroup as a result n and n will be filled with   x   and no blocks can be migrated from n and n to n or n as balancer policy with node group thus we expect the balancer to end in  iterations without move block process 
 regression test for hdfs  cluster has  racks a and b each with three nodes block initially written on a a b admin decommissions two of these nodes let s say a and a but it doesn t matter re replication should respect rack policy
 test what happens when a block is on three nodes and all three of those nodes are decommissioned it should properly re replicate to three new nodes 
 test what happens when there are two racks and an entire rack is decommissioned since the cluster is multi rack it will consider the block under replicated rather than create a third replica on the same rack adding a new node on a third rack should cause re replication to that node 
 test that the block safe increases up to block threshold once the block threshold is reached the block manger leaves safe mode and increment will be a no op the safe mode status lifecycle off  pending threshold  extension  off
 test the safe mode monitor the monitor will make block manager leave the safe mode after extension period 
 test block manager won t leave safe mode if datanode threshold is not met 
 test get safe mode tip 
 start a  node cluster with only one block report lease when the first datanode gets a lease kill it then wait for the lease to expire and the second datanode to send a full block report 
 testing that append operation can handle token expiration when re establishing pipeline is needed
 testing that write operation can handle token expiration when re establishing pipeline is needed
 this test checks that if a node is re registered with a new software version after the heartbeat expiry interval but before the heartbeatmanager has a chance to detect this and remove it the node s version will still be correctly decremented 
 this test sends a random sequence of node registrations and node removals to the datanodemanager of nodes with different ids and versions and checks that the datanodemanager keeps a correct count of different software versions at all times 
 this test creates a locatedblock with  locations sorts the locations based on the network topology and ensures the locations are still aligned with the storage ids and storage types 
 execute a functional topology script and make sure that helper function works correctly
 run a broken script and verify that helper function is able to ignore the broken script and work correctly
 test with different sorting functions but include datanodes with provided storage
 helper function that tests the datanodemanagers sortedblock function we invoke this function with and without topology scripts
 test whether removing a host from the includes list without adding it to the excludes list will exclude it from data node reports 
 verify the correctness of pending recovery process 
 test that the namenode prunes empty storage volumes that are no longer reported by the datanode 
 verify that the namenode does not prune storages with blocks simply as a result of a heartbeat being sent missing that storage 
 regression test for hdfs  shutting down a datanode removing a storage directory and restarting the datanode should not produce zombie storages 
 test whether we can delay the deletion of unknown blocks in datanode s first several block reports 
 test if datanodeprotocol blockreceivedanddeleted can correctly update the pending reconstruction also make sure the blockreceivedanddeleted call is idempotent to the pending reconstruction 
 test if blockmanager can correctly remove corresponding pending records when a file is deleted
 test when a block s replica is removed from rbw folder in one of the datanode namenode should ask to invalidate that corrupted block and schedule replication for one more replica for that under replicated block 
 regression test for hdfs  a case where upon restart if there were rwr replicas with out of date genstamps the nn could accidentally delete good replicas instead of the bad replicas 
 when there are all the internal blocks available but they are not placed on enough racks namenode should avoid normal decoding reconstruction but copy an internal block to a new rack in this test we first need to create a scenario that a striped block has all the internal blocks but distributed in  racks then we check if the redundancy monitor can correctly schedule the reconstruction work for it 
 in case we have  internal blocks on  racks where  of blocks are live and  decommissioning make sure the reconstruction happens correctly 
 in this testcase it tries to choose more targets than available nodes and check the result with stale node avoidance on the write path enabled 
 test for the high priority blocks are processed before the low priority blocks 
 a basic test to verify that we can write data and read it back again 
 test write a file verifies and closes it then the length of the blocks are messed up and blockreport is forced the modification of blocks length has to be ignored
 test write a file verifies and closes it then a couple of random blocks is removed and blockreport is forced the fsnamesystem is pushed to recalculate required dn s activities such as replications and so on the number of missing and under replicated blocks should be the same in case of a single dn cluster 
 test writes a file and closes it block reported is generated with a bad gs for a single block block report is forced and the check for of corrupted blocks is performed 
 test writes a file and closes it block reported is generated with an extra block block report is forced and the check for of pendingdeletion blocks is performed 
 test creates a file and closes it the second datanode is started in the cluster as soon as the replication process is completed test runs block report and checks that no underreplicated blocks are left
 similar to blockreport  but works with two dns test writes a file and closes it the second datanode is started in the cluster as soon as the replication process is completed test finds a block from the second dn and sets its gs to be  of original one this is the markblockascorrupt case  so we expect one pending deletion block report is forced and the check for of currupted blocks is performed another block is chosen and its length is set to a lesser than original a check for another corrupted block is performed after yet another blockreport
 the test set the configuration parameters for a large block size and restarts initiated single node cluster then it writes a file  block size and closes it the second datanode is started in the cluster as soon as the replication process is started and at least one temporary replica is found test forces blockreport process and checks if the temporary replica isn t reported on it eventually the configuration is being restored into the original state 
 test for the case where one of the dns in the pipeline is in the process of doing a block report exactly when the block is closed in this case the block report becomes delayed until after the block is marked completed on the nn and hence it reports an rbw replica for a complete block such a report should not be marked corrupt this is a regression test for hdfs  
 test link datasetvolumechecker checkvolume propagates thecheck to the delegate checker 
 test link datasetvolumechecker checkallvolumes propagateschecks for all volumes to the delegate checker 
 test timeout in link datasetvolumechecker checkallvolumes 
 test checking a closed volume i e one which cannot be referenced 
 create a mock fsvolumespi whose link fsvolumespi check routinehangs forever 
 verify that all healthy locations are correctly handled and that the check routine is invoked as expected 
 test handling when the number of failed locations is below the max volume failure threshold 
 test handling when the number of volume failures tolerated is the same as the number of volumes 
 test handling all storage locations are failed 
 verify that a link storagelocation check timeout is correctly detectedas a failure this is hard to test without a link thread sleep call 
 return a list of storage locations one per argument whose check method takes at least the specified number of milliseconds to complete 
 test various scheduling combinations to ensure scheduling and throttling behave as expected 
 ensure that the context is passed through to the checkable check method 
 ensure that the exception from a failed check is cached and returned without re running the check when the minimum gap has not elapsed 
 it injects disk failures to data dirs by replacing these data dirs with regular files 
 corrupt a block crc file by truncating it to a newsize 
 corrupt a block crc file by deleting it 
 asserts that the storage lock file in the given directory has been released this method works by trying to acquire the lock file itself if locking fails here then the main code must have failed to release it 
 make sure at least one non transient volume has a saved copy of the replica an infinite loop is used to ensure the async lazy persist tasks are completely done before verification caller of ensurelazypersistblocksaresaved expects either a successful pass or timeout failure 
 if ramdiskstoragelimit is   then ram disk capacity is artificially capped if ramdiskstoragelimit   then it is ignored 
 test block md access via a dn 
 the same as above but use hostnames for dndn communication 
 the following test first creates a file it verifies the block information from a datanode then it updates the block with new information and verifies again 
 test for link fsdatasetimpl updatereplicaunderrecovery extendedblock long long 
 test to verify that interdatanode rpc timesout as expected when the server dn does not respond 
 if one or more replicas of a lazypersist file are lost then the file must be discarded by the nn instead of being kept around as a corrupt file 
 if nn restarted then lazypersist files should not deleted
 concurrent read from the same node and verify the contents 
 concurrent write with eviction ram disk can hold  replicas  threads each write  replicas
 verify that locked ram is released when blocks are evicted from ram disk 
 verify that locked bytes are correctly updated when a block is finalized at less than its max length 
 verify that locked bytes are correctly updated when the client goes away unexpectedly during a write 
 file partially fit in ramdisk after eviction ramdisk can fit  blocks write a file with  blocks expect  blocks are on ramdisk and rest on disk 
 validates lazy persisted blocks are evicted from ram disk based on lru 
 tests if the fileregions provided by the fileregionprovider can belong to the providevolume 
 ensure that reserved space is released when the client goes away unexpectedly the verification is done for each replica in the write pipeline 
 stress test to ensure we are not leaking reserved space 
 this is a test to check the replica map before and after the datanode quick restart less than  minutes 
 test that we can successfully recover a link replicabeingwritten which has inconsistent metadata bytes were written to disk but bytesondisk was not updated but that recovery fails when the block is actually corrupt bytes are not present on disk 
 test that a very slow peer is detected as an outlier 
 get the block map that a given block lives within 
 register the fsdataset mbean using the name hadoop service datanode name fsdatasetstate we use storage id for mbean name since a minicluster within a single java vm may have multiple simulated datanodes 
 blockrecovery   two replicas are in finalized state
 blockrecovery   one replica is finalized and another is rbw 
 blockrecovery   one replica is finalized and another is rwr 
 blockrecovery   two replicas are rbw 
 blockrecovery   one replica is rbw and another is rwr 
 blockrecovery   two replicas are rwr 
 blockrecoveryfi  one dn throws recoveryinprogressexception 
 blockrecoveryfi  all datanodes throws an exception 
 blockrecoveryfi  max replica length from all dns is zero 
 blockrecoveryfi  some all dns failed to update replicas 
 blockrecoveryfi  dn has no replicaunderrecovery 
 blockrecoveryfi  a replica s recovery id does not match new gs 
 test to verify the race between finalizeblock and lease recovery
 attempt to acquire a sempahore within a given timeout this is useful for unit tests where we need to ignore interruptedexception when attempting to take a semaphore but still want to honor the overall test timeout 
 test that an fsdatasetimpl operation does not hold the lock for an unreasonable amount of time if a writer is taking a long time to stop 
 test for block recovery taking longer than the heartbeat interval 
 test for block recovery timeout all recovery attempts will be delayed and the first attempt will be lost to trigger recovery timeout and retry 
 test to verify that the copying of pinned block to a different destination datanode will throw ioexception with error code status error block pinned 
 standby namenode doesn t queue delete block request when the add block request is in the edit log which are yet to be read 
 param args
 test iterating through a bunch of blocks in a volume using a volume iterator we will rewind the iterator when about halfway through the blocks 
 test scanning all blocks set the scan period high enough that we shouldn t rescan any block during this test 
 test scanning all blocks set the scan period high enough that we should rescan all blocks at least twice during this test 
 test that we don t scan too many blocks per second 
 test that we save the scan cursor when shutting down the datanode and restart scanning from there when the datanode is restarted 
 test that we can mark certain blocks as suspect and get them quickly rescanned that way see hdfs  and hdfs  
 test that blocks which are in the wrong location are ignored 
 test concurrent append and scan 
 test that the bpos can register to talk to two different nns sends block reports to both etc 
 test that dna invalidate commands from the standby are ignored 
 ensure that if the two nns configured for a block pool have different block pool ids they will refuse to both register 
 test that the datanode determines the active namenode correctly based on the ha related information in heartbeat responses see hdfs  
 test datanode block pool initialization error handling failure in initializing a block pool should not cause npe 
 create a bpofferservice which registers with and heartbeats with the specified namenode proxy objects 
 this test case test the link bpofferservice reportbadblocks methodsuch that if call to standby namenode times out then that should not affect the active namenode heartbeat processing since this function are in writelock 
 this test case test the link bpofferservice trysenderrorreport methodsuch that if call to standby namenode times out then that should not affect the active namenode heartbeat processing since this function are in writelock 
 this test case tests whether the bpserviceactor processqueuemessages adds back the error report back to the queue when bpserviceactoraction reportto throws an ioexception
 this test case doesn t add the reportbadblock request to link bpserviceactor bpthreadenqueue when the standby namenode throws link standbyexception 
 test the scenario where the datanode defaults to not dropping the cache but our client defaults are set 
 test bpservice thread exit
 wait the block to have the exact number of replicas as expected 
 add volumes to the first datanode 
 test adding one volume on a running minidfscluster with only one namenode 
 test the case that remove a data volume on a particular datanode when the volume is actively being written 
 verify that link datanode checkdiskerror removes all metadata indatanode upon a volume failure thus we can run reconfig on the same configuration to reload the new volume on the same directory as the failed one 
 tests that round trip acks in a datanode write pipeline are correctly measured 
 publish a fake metric under the hadoop domain and ensure it is logged by the metrics logger 
 verify the following scenario  the initial datanodeprotocol registerdatanode succeeds  dn starts heartbeat process  in the first heartbeat nn asks dn to reregister  dn calls datanodeprotocol registerdatanode  datanodeprotocol registerdatanode throws eofexception  dn retries  datanodeprotocol registerdatanode succeeds 
 ensures that the blocks belonging to the deleted file are in trash
 support for layout version change with rolling upgrade was added by hdfs  and hdfs  
 support for layout version change with rolling upgrade was added by hdfs  and hdfs  
 test if there is volume failure the datanode will fail to start we fail a volume by setting the parent directory non writable 
 datanode will start and tolerate one failing disk according to config we fail a volume by setting the parent directory non writable 
 test if data directory is not readable writable datanode won t start 
 datanode will start and tolerate one non writable data directory according to config 
 verifies two things  number of locations of each block in the name node matches number of actual files  block files  pending block equals to total number of blocks that a file has including the replication hdfs file has  blocks repl  total 
 go to each block on the nd datanode until it fails 
 try to access a block on a data node if fails throws exception
 test that individual volume failures do not cause dns to fail that all volumes failed on a single datanode do cause it to fail and that the capacities and liveliness is adjusted correctly in the nn 
 test that the nn re learns of volume failures after restart 
 verify datanode numfailedvolumes and failedstoragelocations after hot swap out of failed volume 
 converts the provided paths to absolute file paths 
 test the dfs datanode failed volumes tolerated key configuration option ie the dn tolerates a failed to use scenario during its start up 
 test for different combination of volume configs and volumes tolerated values 
 tests for a given volumes to be tolerated and volumes failed 
 test that a volume that is considered failed on startup is seen as a failed volume by the nn 
 create a list of storagelocations if asfile sets to true create storagelocation as regular files otherwise create directories for each location 
 this test enforces the behavior that if there is an exception from dotransition during dn starts up the storage directories that have already been processed are still visible i e in datastorage storagedirs 
 ensure that the correct hint is passed to the block receiver when the client is local 
 ensure that hint is always false when the client is remote 
 ensure that the correct hint is passed to the block receiver when the client is remote and dfs datanode allow non local lazy persist is set to true 
 truncate a block file 
 create a block file in a random volume
 create a metafile in a random volume
 create block file and corresponding metafile in a rondom volume 
 test that the timeslice throttle limits the report compiler thread s execution time correctly we test by scanning a large block pool and comparing the time spent waiting to the time spent running the block pool has to be large or the ratio will be off the throttle allows the report compiler thread to finish its current cycle when blocking it so the ratio will always be a little lower than expected the smaller the block pool the further off the ratio will be 
 test the behavior of exception handling during directory scan operation directory scanner shouldn t abort the scan on every directory just because one had an error 
 test that when there is a failure replicating a block the temporary and meta files are cleaned up and subsequent replication succeeds 
 test that if splitthreshold is zero then we always get a separate call per storage 
 tests the behavior when the count of blocks is exactly one less than the threshold 
 tests the behavior when the count of blocks is exactly equal to the threshold 
 run testcacheanduncacheblock with some failures injected into the mlock call this tests the ability of the namenode to resend commands 
 test that when a client has a replica mmapped we will not un mlock that replica for a reasonable amount of time even if an uncache request occurs 
 test that when we have an uncache request and the client refuses to release the replica for a long time we will un mlock it 
 test basic hsync cases 
 ensure that an ibr is generated immediately for a block received by the dn 
 ensure that a delayed ibr is generated for a block deleted on the dn 
 add a received block entry and then replace it ensure that a single ibr is generated and that pending receive request state is cleared this test case verifies the failure in hdfs  
 incremental brs from all storages combined in a single message 
 one incremental br per storage 
 verify that the datanode sends a single incremental block report for all storages 
 checks if code ins matches the provided file from offset code fileoffset for length datalength 
 setup a link minidfscluster create a block with both link state normal and link state read only shared replicas 
 verify that read only shared replicas are not counted towards the overall replication count but are included as replica locations returned to clients for reads 
 verify that the namenode is able to still use read only shared replicas even when the single normal replica is offline and the effective replication count is  
 verify that corrupt read only shared replicas aren t counted towards the corrupt replicas total 
 convenience method to call link assertblocklengthinblockreports simulatedfsdataset simulatedfsdataset with a null second parameter 
 this test doesn t require kdc or other security settings as it expects link java net bindexception testing is done with unprivileged port for code dfs datanode address 
 this test doesn t require kdc or other security settings as it expects link java net bindexception testing is done with unprivileged port for code dfs datanode http address 
 tests if it s allowed to submit and execute plan when datanode is in status other than regular 
 tests running multiple commands under on setup this mainly covers link org apache hadoop hdfs server diskbalancer command command close 
 this test simulates diskbalancercli report command run from a shell with a generic option fs 
 making sure that we can query the node without having done a submit 
 creates a randomdatanode 
 creates a randomcluster 
 this test simulates a real data node working with diskbalancer here is the overview of this test  write a bunch of blocks and move them to one disk to create imbalance  rewrite the capacity of the disks in diskbalancer model so that planner will produce a move plan  execute the move plan and wait unitl the plan is done  verify the source disk has blocks now 
 test disk balancer behavior when one of the disks involved in balancing operation is removed after submitting the plan 
 moves all data to a source disk to create disk imbalance so we can run a planner 
 checks that we return the right error if diskbalancer is not enabled 
 test a second submit plan fails 
 test cancel plan 
 test custom bandwidth 
 copies blocks from a set of volumes 
 gets an existing mount table record in the state store 
 creates test directories via the namenode  target ns testfile  target ns testfile
 wait until the federated cluster is up and ready 
 sets the value of a field on the object 
 fetches the value for a field name 
 create a new state store based on a configuration 
 wait for the state store to initialize its driver 
 delete the default state store 
 clear all the records from the state store 
 clear records from a certain type from the state store 
 test mover runs fine when logging in with a keytab in kerberized env reusing testwithinsamenode here for basic functionality testing 
 test to verify that mover can t move pinned blocks 
 test to verify that mover should work well with pinned blocks as well as failed blocks mover should continue retrying the failed blocks only 
 create files directories and set their storage policies according to the corresponding scheme 
 run the migration tool 
 verify block locations after running the migration tool 
 a normal case for mover move a file into archival storage
 run mover with arguments specifying files and directories
 move an open file into archival storage
 test directories with hot warm and cold polices 
 test disk is running out of spaces 
 test archive is running out of spaces 
 param args arguments
 verify the de duplication of aclfeatures with same entries 
 initializes all filesystem instances used in the tests 
 restarts the cluster without formatting so all data is preserved 
 asserts whether or not the inode for the test path has an aclfeature 
 asserts whether or not the inode for a specific path has an aclfeature 
 asserts the value of the fspermission bits on the inode of the test path 
 calculate the mdsum of an image after zeroing out the transaction id field in the header this is useful for tests that want to verify that two checkpoints have identical namespaces 
 create an aborted in progress log in the given directory containing only a specified number of mkdirs operations 
 param editlog a path of an edit log file
 assert that all of the given directories have the same newest filename for fsimage that they hold the same data 
 given a list of directories assert that any files that are named the same thing have the same contents for example if a file named fsimage  shows up in more than one directory then it must be the same 
 assert that a set of properties files all contain the same data we cannot simply check the mdsums here since properties files contain timestamps thus two properties files from the same savenamespace operation may actually differ in mdsum 
 assert that a set of properties files all contain the same data 
 assert that all of the given paths have the exact same contents 
 assert that the given files are not all the same and in fact that they have expecteduniquehashes unique contents 
 corrupt the given version file by replacing a given key with a new value and re writing the file 
 tests for creating xattr  create an xattr using xattrsetflag create  create an xattr which already exists and expect an exception  create multiple xattrs  restart nn and save checkpoint scenarios 
 tests for replacing xattr  replace an xattr using xattrsetflag replace  replace an xattr which doesn t exist and expect an exception  create multiple xattrs and replace some  restart nn and save checkpoint scenarios 
 tests for removing xattr  remove xattr  restart nn and save checkpoint scenarios 
 steps  set xattrs on a file  remove xattrs from that file  save a checkpoint and restart nn  set xattrs again on the same file  remove xattrs from that file  restart nn without saving a checkpoint  set xattrs again on the same file 
 this tests the unreadable by superuser xattr which denies access to a file for the superuser see hdfs  for details 
 initializes all filesystem instances used in the tests 
 restart the cluster optionally saving a new checkpoint 
 add a thread which periodically triggers deletion reports heartbeats and nn side block work 
 add a thread which periodically triggers failover back and forth between the namenodes 
 trigger an edits log roll on the active and then wait for the standby to catch up to all the edits done by the active this method will check repeatedly for up to nn lag timeout milliseconds and then fail throwing link couldnotcatchupexception 
 gets the filesystem instance by setting the failover configurations 
 gets the filesystem instance by setting the failover configurations
 test for the base success case the primary nn hasn t made any checkpoints and we copy the fsimage  file over and start up 
 test for downloading a checkpoint made at a later checkpoint from the active 
 test for the case where the shared edits dir doesn t have all of the recent edit logs 
 show that bootstrapping will fail on a given namenode if its directories already exist its not run across all the nn because its testing the state local on each node 
 test that even if the other node is not active we are able to bootstrap standby from it 
 test that bootstrapping standby nn is not limited by link dfsconfigkeys dfs image transfer rate key but is limited by link dfsconfigkeys dfs image transfer bootstrap standby rate key created by hdfs  
 bootstrapstandby when the existing nn is standby 
 bootstrapstandby when the existing nn is active 
 test the bootstrapstandby while the other namenode is in upgrade state make sure a previous directory can be created 
 similar with testupgrade but rename nn s current directory to previous tmp before bootstrapstandby and make sure the nn is recovered first then converted into upgrade state 
 similar with testupgrade but rename nn s current directory to a random name so that it s not formatted make sure the nn is formatted and then converted into upgrade state 
 test if correct exception standbyexception or retriableexception can be thrown during the nn failover 
 hdfs  distributedfilesystem getcanonicalservicename throws an exception if the uri is a logical uri this bug fails the combination of ha  mapred  security 
 ensure that an admin cannot finalize an ha upgrade without at least one nn being active 
 make sure that an ha nn can successfully upgrade when configured using journalnodes 
 make sure that even if the nn which initiated the upgrade is in the standby state that we re allowed to finalize 
 test case that reduces replication of a file with a lot of blocks and then fails over right after those blocks enter the dn invalidation queues on the active ensures that fencing is correct and no replicas are lost 
 another regression test for hdfs  this tests the following sequence dn does a block report while file is open this br contains the block in rbw state the block report is delayed in reaching the standby the file is closed the standby processes the op add and op close operations before the rbw block report arrives the standby should not mark the block as corrupt 
 check that no edits files are present in the given storage dirs 
 test that the shared edits dir is automatically added to the list of edits dirs that are marked required 
 multiple shared edits directories is an invalid configuration 
 make sure that the shared edits dirs are listed before non shared dirs when the configuration is parsed this ensures that the shared journals are synced before the local ones 
 test that marking the shared edits dir as being required causes the nn to fail if that dir can t be accessed 
 test that the standby nn won t double replay earlier edits if it encounters a failure to read a later edit 
 test the following case  sbn is reading a finalized edits file when nfs disappears halfway through or some intermittent error happens  sbn performs a checkpoint and uploads it to the nn  nn receives a checkpoint that doesn t correspond to the end of any log segment  both nn and sbn should be able to restart at this point this is a regression test for hdfs  
 ensure that the standby fails to become active if it cannot read all available edits in the shared edits dir when it is transitioning to active state 
 test to verify the processing of pendingdatanodemessagequeue in case of append one block will marked as corrupt if the op add op update blocks comes in one edit log segment and op close edit comes in next log segment which is loaded during failover regression test for hdfs  
 test that fsck still works with ha enabled 
 test that the harfilesystem works with underlying hdfs uris that have no port specified as is often the case with an ha setup 
 create an empty har archive in the filesystem fs at the path p 
 make sure the client retries when the active nn is in safemode
 test case for enter safemode in active namenode when it is already in startup safemode it is a regression test for hdfs  
 test case for enter safemode in standby namenode when it is already in startup safemode it is a regression test for hdfs  
 tests the case where while a standby is down more blocks are added to the namespace but not rolled so when it starts up it receives notification about the new blocks during the safemode extension period 
 similar to link testblocksaddedbeforestandbyrestart except thatthe new blocks are allocated after the sbn has restarted so the blocks were not present in the original block reports at startup but are reported separately by blockreceived calls 
 test for the following case proposed by atm  both nns are up one is active there are  blocks both are out of safemode   block deletions get processed by nn nn enqueues these dn messages until it next reads from a checkpointed edits file  nn gets restarted its queues are lost  nn comes up reads from all the finalized edits files concludes there should still be  blocks  nn receives a block report from all the dns which only accounts for  blocks it doesn t leave safemode  nn dies or is transitioned to standby  nn is transitioned to active it reads all the edits from nn it now knows there should only be  blocks but it s still in safemode  nn doesn t ever recheck whether it should leave safemode this is essentially the inverse of link testblocksaddedbeforestandbyrestart 
 similar to link testblocksremovedbeforestandbyrestart except thatthe blocks are removed after the sbn has restarted so the blocks were present in the original block reports at startup but are deleted separately later by deletion reports 
 tests that the standby node properly tracks the number of total and safe blocks while it is in safe mode since safe mode only counts completed blocks append needs to decrement the total number of blocks and then re increment when the file is closed again 
 regression test for a bug experienced while developing hdfs  the scenario here is image contains some blocks edits log contains at least one block addition followed by deletion of more blocks than were added when node starts up some incorrect accounting of block totals caused an assertion failure 
 set up a namesystem with several edits both deletions and additions and failover to a new nn while that nn is in safemode ensure that it will exit safemode 
 similar to link testblocksremovedwhileinsafemode except thatthe op delete edits arrive at the sbn before the block deletion reports the tracking of safe blocks needs to properly account for the removal of the blocks as well as the safe count this is a regression test for hdfs  
 test that the number of safe blocks is accounted correctly even when blocks move between under construction state and completed state if a finalized report arrives at the sbn before the block is marked complete then when we get the op close we need to count it as safe at that point this is a regression test for hdfs  
 regression test for hdfs  in this bug the following sequence was observed some blocks are written to dns while the sbn was down this causes the blockreceived messages to get queued in the bpserviceactor on the dn when the sbn returns the dn re registers with the sbn and then flushes its blockreceived queue to the sbn before it sends its first block report this caused the first block report to be incorrect ignored the sbn would become stuck in safemode 
 test nn crash and client crash stuck immediately after block allocation 
 tests manual failover back and forth between two namenodes 
 regression test for hdfs  when doing state transitions we need to lock the fsnamesystem so that we don t end up doing any writes while it s in between states this test case starts up several client threads which do mutation operations while flipping a nn back and forth from active to standby 
 test for hdfs  since lease renewals go from the client only to the active nn the sbn will have out of date lease info when it becomes active we need to make sure we don t accidentally mark the leases as expired when the failover proceeds 
 tests manual failover back and forth between two namenodes for federation cluster with two namespaces 
 tests continuing a write pipeline over a failover 
 tests continuing a write pipeline over a failover when a dn fails after the failover ensures that updating the pipeline succeeds even when the pipeline was constructed on a different nn 
 tests lease recovery if a client crashes this approximates the use case of hbase wals being recovered after a nn failover 
 test the scenario where the nn fails over after issuing a block synchronization request but before it is committed the dn running the recovery should then fail to commit the synchronization and a later retry will succeed 
 stress test for pipeline lease recovery starts a number of threads each of which creates a file and has another client break the lease while these threads run failover proceeds back and forth between two namenodes 
 fail over using the given scenario assuming nn is currently active
 do a fail over with the given scenario 
 when nn failover happens if the client did not receive the response and send a retry request to the other nn the same response should be received based on the retry cache 
 add a list of cache pools list cache pools switch active nn and list cache pools again 
 add a list of cache directives list cache directives switch active nn and list cache directives again 
 test for the case when both of the nns in the cluster are in the standby state and thus are both creating checkpoints and uploading them to each other in this circumstance they should receive the error from the other node indicating that the other node already has a checkpoint for the given txid but this should not cause an abort etc 
 test for the case when the sbn is configured to checkpoint based on a time period but no transactions are happening on the active thus it would want to save a second checkpoint at the same txid which is a no op this test makes sure this doesn t cause any problem 
 test cancellation of ongoing checkpoints when failover happens mid checkpoint 
 test cancellation of ongoing checkpoints when failover happens mid checkpoint during image upload from standby to active nn 
 make sure that clients will receive standbyexceptions even when a checkpoint is in progress on the sbn and therefore the standbycheckpointer thread will have fsns lock regression test for hdfs  
 test for the case standby nns can upload fsimage to ann after become non primary standby nn hdfs 
 test that checkpointing is still successful even if an issue was encountered while writing the legacy oiv image 
 check that no edits files are present in the given storage dirs 
 test metrics associated with addition of a file 
 verify low redundancy and corrupt blocks metrics are zero 
 corrupt a block and ensure metrics reflects it 
 create excess blocks by reducing the replication factor for for a file and ensure metrics reflects it
 test to ensure metrics reflects missing blocks 
 wait for the named gauge value from the metrics source to reach the desired value there s an initial delay then a spin cycle of sleep and poll because all the tests use a shared fs instance these tests are not independent that s why the initial sleep is in there 
 test numgetblocklocations metric test initiates and performs file operations create read close open file which results in metrics changes these metrics changes are updated and tested for correctness create file operation does not increment numgetblocklocation one read file operation increments numgetblocklocation by 
 tests that the sync and block report metrics get updated on cluster startup 
 test nn readops count and writeops count
 test metrics indicating the number of active clients and the files under construction
 send a heartbeat to the name node and replicate blocks if requested 
 retry clientprotocol addblock if it throws notreplicatedyetexception because addblock also commits the previous block it fails if enough ibrs are not processed by namenode 
 main method of the benchmark 
 generates edits with all op codes and returns the edits filename
 compare two dumped trees that are stored in two files the following is an example of the dumped tree information of root  the first child of root e g foo  the first child of foo the last child of foo e g foo bar  the first child of foo bar snapshots of foo  snapshot s  snapshot s n  second child of root last child of root the following information is dumped for each inode localname classname hashcode parent permission group user specific information for different types of inode link inodedirectory childrensize link inodefile filesize block list check link blockinfo tostring and link blockunderconstructionfeature tostring for detailed information 
 constructor to build a tree of given code height 
 recursively generate the tree based on the height 
 restart the cluster optionally saving a new checkpoint 
 test snapshot during file appending before the corresponding link fsdataoutputstream instance closes 
 call dfsclient callgetblocklocations for snapshot file make sure only blocks within the size range are returned 
 create a snapshot for test foo and create another snapshot for test foo bar files created before the snapshots should appear in both snapshots and the files created after the snapshots should not appear in any of the snapshots 
 test open files under snapshot directories are getting captured in snapshots as a truly immutable copy verify open files outside of snapshot directory not getting affected level  a level  c  appa log open file not under snap root level  e snapshottable dir level  g  flume log open file under snap root level  b  appb log open file not under snap root level  d snapshottable dir  hbase log open file under snap root 
 test snapshot capturing open files and verify the same across namenode restarts 
 test snapshot capturing open files when an open file with active lease is deleted by the client 
 verify if the namenode can restart properly after an openforwrite file and the only snapshot it was present in were deleted 
 test client writing to open files are not interrupted when snapshots that captured open files get deleted 
 verify snapshots with open files captured are safe even when the current version of the file is truncated and appended later 
 verify open files captured in the snapshots across config disable and enable 
 test renaming a dir and then delete snapshots 
 test renaming a file and then delete snapshots 
 test rename a dir and a file multiple times across snapshottable directories dir foo  dir foo  dir foo  dir foo  dir foo only create snapshots in the beginning before the rename 
 test rename a dir multiple times across snapshottable directories dir foo  dir foo  dir foo  dir foo  dir foo create snapshots after each rename 
 after rename delete the snapshot in src
 rename a file and then append the same file 
 test rename to an invalid name xxx snapshot 
 move a directory to its prior descendant
 move a directory to its prior descedant
 after the following operations rename a dir  create a snapshot s on dst tree  delete the renamed dir  delete snapshot s on dst tree make sure we destroy everything created after the rename under the renamed dir 
 after the following operations rename a dir  create a snapshot s on dst tree  rename the renamed dir again  delete snapshot s on dst tree make sure we only delete the snapshot s under the renamed dir 
 rename and deletion snapshot under the same the snapshottable directory 
 unit test for hdfs  
 make sure we clean the whole subtree under a dstreference node after deleting a snapshot see hdfs  
 rename of the underconstruction file in snapshot should not fail nn restart after checkpoint unit test for hdfs  
 similar with testrenameucfileinsnapshot but do renaming first and then append file without closing it unit test for hdfs  
 make changes modification deletion creation to the current files dir then check if the previous snapshots are still correct 
 test if the offlineimageviewerpb can correctly parse a fsimage containing snapshots
 prepare a list of modifications a modification may be a file creation file deletion or a modification operation such as appending to an existing file 
 test deleting a file with snapshots need to check the blocksmap to make sure the corresponding record is updated correctly 
 test deleting a directory which is a descendant of a snapshottable directory in the test we need to cover the following cases  delete current inodefile inodedirectory without taking any snapshot  delete current inodefile inodedirectory while snapshots have been taken on ancestor s  delete current inodefilewithsnapshot  delete current inodedirectorywithsnapshot 
 test deleting the earliest first snapshot in this simplest scenario the snapshots are taken on the same directory and we do not need to combine snapshot diffs 
 test deleting the earliest first snapshot in this more complicated scenario the snapshots are taken across directories the test covers the following scenarios  delete the first diff in the diff list of a directory  delete the first diff in the diff list of a file also the recursive cleantree process should cover both inodefile and inodedirectory 
 test deleting snapshots in a more complicated scenario need to combine snapshot diffs but no need to handle diffs distributed in a dir tree
 test deleting snapshots in more complicated scenarios snapshot diffs are distributed in the directory sub tree 
 when combine two snapshots make sure files directories created after the prior snapshot get destroyed 
 test snapshot deletion
 test deleting snapshots with modification on the metadata of directory 
 a test covering the case where the snapshot diff to be deleted is renamed to its previous snapshot 
 test the computation and representation of diff between snapshots 
 make changes under a sub directory then delete the sub directory make sure the diff report computation correctly retrieve the diff from the deleted sub directory 
 rename a directory to its prior descendant and verify the diff report 
 rename a file dir outside of the snapshottable dir should be reported as deleted rename a file dir from outside should be reported as created 
 renaming a file dir then delete the ancestor dir of the rename target should be reported as deleted 
 rename a file and then append some data to it
 nested renamed dir file and the withnamelist in the withcount node of the parental directory is empty due to snapshot deletion see hdfs  for details 
 test snapshot diff report for snapshots with open files captures in them also verify if the diff report remains the same across namenode restarts 
 check to see access time is not captured in snapshot when applicable when dfs namenode snapshot skip capture accesstime only change is set to true and if a file s access time changed between two snapshots but has no other modification then the access time is not captured in snapshot 
 tests to verfy the diff report with maximum snapsdiffreportentries limit over an rpc being set to  
 test replication number calculation for a normal file without snapshots 
 check the replication for both the current file and all its prior snapshots
 test replication number calculation for a file with snapshots 
 test replication for a file with snapshots also including the scenario where the original file is deleted
 test getting snapshotstatsmxbean information
  save xattrs then create snapshot assert that inode of original and snapshot have same xattrs  change the original xattrs assert snapshot still has old xattrs 
  save xattrs then create snapshot assert that inode of original and snapshot have same xattrs  remove some original xattrs assert snapshot still has old xattrs 
 restart the cluster optionally saving a new checkpoint 
 restart the cluster optionally saving a new checkpoint 
 retry addblock while another thread is in choosetarget see hdfs  
 check if the scheduled block size on each dn storage is correctly updated
 verify that the audit logger is aware of the call context
 test that allowed operation puts proper entry in audit log 
 test that allowed stat puts proper entry in audit log 
 test that denied operation puts proper entry in audit log 
 test that access via webhdfs puts proper entry in audit log 
 test that stat via webhdfs puts proper entry in audit log 
 test that denied access via webhdfs puts proper entry in audit log 
 test that open via webhdfs puts proper entry in audit log 
 make sure that r n isn t made into a newline in audit log 
 ensure that the backupnode will tail edits from the nn and keep in sync even while the nn rolls checkpoints occur etc 
 verify that a file can be read both from namenode and backupnode 
 test namenode getblocklocations on reading un closed files 
 a storage id can be invalid if the storage failed or the node reregisters when the node heart beats the storage report in it causes storage volumes to be added back an invalid storage id should not cause an npe 
 tests stepping the cache replication factor up and down checking the number of cached replicas and blocks as well as the advertised locations 
 check that the namenode is not attempting to cache anything 
 simulate a secondary node failure to transfer image uses an unchecked error and fail transfer before even setting the length header this used to cause image truncation regression test for hdfs  
 simulate nn failing to send the whole file error type  the length header in the http transfer should prevent this from corrupting the nn 
 simulate nn sending a corrupt image error type  the digest header in the http transfer should prevent this from corrupting the nn 
 run a test where the nn runs into some kind of error when sending the checkpoint back to the nn 
 test that the nn locks its storage and edits directories and won t start up if the directories are already locked
 test that if the edits dir is separate from the name dir it is properly locked 
 test that the secondarynamenode properly locks its storage directories 
 test the importcheckpoint startup option verifies  if the nn already contains an image it will not be allowed to import a checkpoint  if the nn does not contain an image importing a checkpoint succeeds and re saves the image
 tests checkpoint in hdfs 
 tests save namespace 
 tests the following sequence of events secondary successfully makes a checkpoint it then fails while trying to upload it it then fails again for the same reason it then tries to checkpoint a third time
 starts two namenodes and two secondary namenodes verifies that secondary namenodes are configured correctly to talk to their respective namenodes and can do the checkpoint 
 test that the secondary doesn t have to re download image if it hasn t changed 
 test nn restart if a failure happens in between creating the fsimage md file and renaming the fsimage 
 test that a fault while downloading edits does not prevent future checkpointing
 test that a fault while downloading edits the first time after the nn starts up does not prevent future checkpointing 
 test that the secondary namenode correctly deletes temporary edits on startup 
 test case where two secondary namenodes are checkpointing the same namenode this differs from link testmultiplesecondarynamenodes since that test runs against two distinct nns this case tests the following interleaving nn a downloads image up to txid  nn a about to save its own checkpoint nn b downloads image up to txid  nn b uploads checkpoint txid  nn a uploads checkpoint txid  it verifies that this works even though the earlier txid checkpoint gets uploaded after the later txid checkpoint 
 test case where two secondary namenodes are checkpointing the same namenode this differs from link testmultiplesecondarynamenodes since that test runs against two distinct nns this case tests the following interleaving nn a calls rolledits nn b calls rolledits nn a paused at getremoteeditlogmanifest nn b calls getremoteeditlogmanifest returns up to txid  nn b uploads checkpoint fsimage  nn a allowed to proceed also returns up to txid  nn a uploads checkpoint fsimage  as well should fail gracefully it verifies that one of the two gets an error that it s uploading a duplicate checkpoint and the other one succeeds 
 test case where the name node is reformatted while the secondary namenode is running the secondary should shut itself down if if talks to a nn with the wrong namespace 
 test that the primary nn will not serve any files to a nn who doesn t share its namespace id and also will not accept any files from one 
 test that if a storage directory is failed when a checkpoint occurs the non failed storage directory receives the checkpoint 
 test case where the nn is configured with a name only and an edits only dir with storage restore turned on in this case if the name only dir disappears and comes back a new checkpoint after it has been restored should function correctly 
 test that the nn triggers a checkpoint after the configurable interval
 test case where the secondary does a checkpoint then stops for a while in the meantime the nn saves its image several times so that the logs that connect the nn s old checkpoint to the current txid get archived then the nn tries to checkpoint again 
 regression test for hdfs  edit log files are never being purged from nn 
 regression test for hdfs  long lived nn cannot perform a checkpoint if security is enabled and the nn restarts without outstanding delegation tokens 
 regression test for hdfs  this makes sure that when we re load the fsimage in the nn we clear the existing leases 
 tests save namespace 
 test namenode format with format option format should succeed 
 test namenode format with format option when an empty name directory exists format should succeed 
 test namenode format with format force options when name directory exists format should succeed 
 test namenode format with format force clusterid option when name directory exists format should succeed 
 test namenode format with clusterid force option format command should fail as no cluster id was provided 
 test namenode format with format clusterid options format should fail was no clusterid was sent 
 test namenode format with format clusterid and empty clusterid format should fail as no valid if was provided 
 test namenode format with format noninteractive options when name directory does not exist format should succeed 
 test namenode format with force noninteractive force option format should succeed 
 test namenode format with format option when a non empty name directory exists enter y when prompted and the format should succeed 
 tests decommissioning status in dfs 
 verify a dn remains in decommission inprogress state if it is marked as dead before decommission has completed that will allow dn to resume the replication process after it rejoins the cluster 
 verify the support for decommissioning a datanode that is already dead under this scenario the datanode should immediately be marked as decommissioned
 verify rack local node selection for the rack local client in case of no local node
 verify local node selection
 verify decommissioned nodes should not be selected 
 verify random rack node selection for remote client
 test the sequence of deleting a file that has snapshot and lease manager s hard limit recovery 
 test if the quota can be correctly updated for create file
 test if the quota can be correctly updated for append
 test if the quota can be correctly updated when file length is updated through fsync
 test append over storage quota does not mark file as uc or create lease
 test append over a specific type of storage quota does not mark file as uc or create a lease
 test truncate over quota does not mark file as uc or create a lease
 check whether the quota is initialized correctly 
 test that the cached quota stays correct between the commit and complete block steps even if the replication factor is changed during this time 
 test that the cached quota remains correct when the block has been written to but not yet committed even if the replication factor is updated during this time 
 test case for an empty edit log from a prior version of hadoop 
 test case for loading a very simple edit log from a format prior to the inclusion of edit transaction ids in the log 
 tests transaction logging in dfs 
 test edit log with different initial buffer size
 test what happens with the following sequence thread a writes edit thread b calls logsyncall calls close on stream thread a calls logsync this sequence is legal and can occur if entersafemode is closely followed by savenamespace 
 test what happens if the nn crashes when it has has started but had no transactions written 
 test what happens if the nn crashes when it has has started and had a few transactions written
 tests the geteditlogmanifest function using mock storage for a number of different situations 
 create a mock nnstorage object with several directories each directory holding edit logs according to a specification each directory is specified by a pipe separated string for example      specifies a directory which includes two finalized segments one from   and one from   the syntax  specifies an in progress log starting at txid  
 set up directories for tests each rolled file is  txns long a failed file is  txns long 
 set up directories for tests each rolled file is  txns long a failed file is  txns long 
 test loading an editlog which has had both its storage fail on alternating rolls two edit log directories are created the first one fails on odd rolls the second on even test that we are able to load the entire editlog regardless 
 test loading an editlog with gaps a single editlog directory is set up on of the edit log files is deleted this should fail when selecting the input streams as it will not be able to select enough streams to load up to  txns per roll there should be  txns per roll transactions as we rolled  times 
 fuzz test for the edit log this tests that we can read random garbage from the edit log without crashing the jvm or throwing an unchecked exception 
 test edit log failover if a single edit log is missing other edits logs should be used instead 
 test edit log failover from a corrupt edit log
 throws exception
 regression test for hdfs  which verifies that fseditlogfileinputstream scanop verifies op checksums 
 tests writing to the editlogfileoutputstream due to preallocation the length of the edit log will usually be longer than its valid contents 
 create the mini cluster for testing and sub in a custom runtime so that edit log journal failures don t actually cause the jvm to exit 
 tests rolling edit logs while transactions are ongoing 
 tests saving fs image while transactions are ongoing 
 the logsync method in fseditlog is unsynchronized whiel syncing so that other threads can concurrently enqueue edits while the prior sync is ongoing this test checks that the log is saved correctly if the saveimage occurs while the syncing thread is in the unsynchronized middle section this replicates the following manual test proposed by konstantin i start the name node in debugger i do mkdir and stop the debugger in logsync just before it does flush then i enter safe mode with another client i start savenamepsace and stop the debugger in fsimage savefsimage  fseditlog createeditlogfile  editlogfileoutputstream create  after truncating the file but before writing layout version into it then i let logsync run then i terminate the name node after that the name node wont start since the edits file is broken 
 most of the fsnamesystem methods have a synchronized section where they update the name system itself and write to the edit log and then unsynchronized they call logsync this test verifies that if an operation has written to the edit log but not yet synced it we wait for that sync before entering safe mode 
 test the normal operation of loading transactions from file journal manager  edits directories are setup without any failures test that we read in the expected number of transactions 
 test that inprogress files are handled correct set up a single edits directory fail on after the last roll then verify that the logs have the expected number of transactions 
 test a mixture of inprogress files and finalised set up  edits directories and fail the second on the last roll verify that reading the transactions reads from the finalised directories 
 test that filejournalmanager behaves correctly despite inprogress files in all its edit log directories set up  directories and fail all on the last roll verify that the correct number of transaction are then loaded 
 corrupt an edit log file after the start segment transaction
 test that we can read from a stream created by filejournalmanager create a single edits directory failing it on the final roll then try loading from the point of the rd roll verify that we read the correct number of transactions from this point 
 make requests with starting transaction ids which don t match the beginning txid of some log segments this should succeed 
 test that we receive the correct number of transactions when we count the number of transactions around gaps set up a single edits directory with no failures delete the th logfile test that getnumberoftransactions returns the correct number of transactions before this gap and after this gap also verify that if you try to count on the gap that an exception is thrown 
 test that we can load an edits directory with a corrupt inprogress file the corrupt inprogress file should be moved to the side 
 make sure that we starting reading the correct op when we request a stream with a txid in the middle of an edit log file 
 make sure that in progress streams aren t counted if we don t ask for them 
 truncate files of different sizes byte by byte 
 truncate the same file multiple times until its size is zero 
 truncate the same file multiple times until its size is zero 
 truncate files and then run other operations such as rename set replication set permission etc 
 create three snapshots with appended and truncated file delete snapshots in the specified order and verify that remaining snapshots are still readable 
 create three snapshots with file truncated  times delete snapshots in the specified order and verify that remaining snapshots are still readable 
 failure recovery test for truncate in this failure the dns fail to recover the blocks and the nn triggers lease recovery file stays in recoveryinprogress until datanodes report recovery 
 the last block is truncated at mid non copy on truncate dn is shutdown before truncate and restart after truncate successful 
 the last block is truncated at mid copy on truncate dn is shutdown before truncate and restart after truncate successful 
 the last block is truncated at mid non copy on truncate dn dn are restarted immediately after truncate 
 the last block is truncated at mid non copy on truncate shutdown the datanodes immediately after truncate 
 editlogop load test for truncate 
 upgrade rollback and restart test for truncate 
 check truncate recovery 
 while rolling upgrade is in progress the test truncates a file such that copy on truncate is triggered then deletes the file and makes sure that no blocks involved in truncate are hanging around 
 do fsck 
 test fsck with permission set on inodes 
 test if fsck can return  in case of failure 
 check if option list corruptfiles of fsck command works properly 
 test for checking fsck command on illegal arguments should print the proper usage 
 tests that the of missing block replicas and expected replicas is correct 
 tests that the of misreplaced replicas is correct 
 test fsck with filenotfound 
 test fsck with symlinks in the filesystem 
 test for including the snapshot files in fsck report 
 test for blockidck 
 test for blockidck with datanode decommission 
 test for blockidck with datanode maintenance 
 test for blockidck with block corruption 
 test storage policy display 
 test for blocks on decommissioning hosts are not shown as missing 
 test for blocks on maintenance hosts are not shown as missing 
 test that corrupted snapshot files are listed with full dir 
 test a cluster with even distribution then a new empty node is added to the cluster
 dump the tree make some changes and then dump the tree again 
 corrupt the byte at the given offset in the given file by subtracting  from it 
 truncate the given file to the given length
 return the length of bytes in the given file after subtracting the trailer of  op invalid s this seeks to the end of the file and reads chunks backwards until it finds a non  byte 
 create an unfinalized edit log for testing purposes
 test if a inodefile with blockinfostriped can be saved by fsimageserialization and loaded by fsimageformat loader 
 test if a inodefileunderconstruction with blockinfostriped can be saved and loaded by fsimageserialization
 on checkpointing stale fsimage checkpoint file should be deleted 
 test persist and load erasure coding policies 
 testing steps  creating modifying directories files while snapshots are being taken  dump the fsdirectory tree of the namesystem  save the namesystem to a temp file fsimage saving  restart the cluster and format the namesystem  load the namesystem from the temp file fsimage loading  dump the fsdirectory again and compare the two dumped string 
 test the fsimage saving loading while file appending 
 test the fsimage loading while there is file under construction 
 verifies that parent directory is correct after a failed call to mkdir
 verifies that parent directory is correct after a failed call to mkdir
 test when fsnamesystem read lock is held for a long time logger will report it 
 test that a dummy implementation of journalmanager can be initialized on startup
 with the custom provider configured verify file status attributes a superuser can bypass permission check while resolving paths so verify file status for both superuser and non superuser 
 fsdirectory unprotectedsetquota creates a new inodedirectorywithquota to replace the original inodedirectory before hdfs  the parent field of all the children inodes of the target inodedirectory is not changed to point to the new inodedirectorywithquota this testcase tests this scenario 
 tests for addressing files using reserved inodes in file system operations 
 check reserved path is reserved and cannot be created 
 test for link fsdirectory getpathcomponents inode 
 tests for link fsdirectory resolvepath string fsdirectory 
 test whether the inode in inodemap has been replaced after regular inode replacement
 create a large number of directories and files 
 run multiple threads doing simultaneous operations on the namenode while a large directory is being deleted 
 end the thread by setting the live p
 check that leasemanager checklease release some leases
 test whether the internal lease holder name is updated properly 
 test leased files counts from link leasemanager getinodewithleases link leasemanager getinodeidwithleases and link leasemanager getinodewithleases inodedirectory 
 test leased files counts at various scale from link leasemanager getinodewithleases link leasemanager getinodeidwithleases and link leasemanager getinodewithleases inodedirectory 
 verify leased inode details across lease get and release from link leasemanager getinodeidwithleases and link leasemanager getinodewithleases inodedirectory 
 check if nn getcorruptfiles returns a file that has corrupted blocks 
 check that listcorruptfileblocks works while the namenode is still in safemode 
 test listcorruptfileblocks in distributedfilesystem
 test if nn listcorruptfiles returns the right number of results the corrupt blocks are detected by the blockpoolslicescanner also test that dfs listcorruptfileblocks can make multiple successive calls 
 verify dfsadmin listopenfiles command in ha mode 
 tests metasave
 tests metasave after delete to make sure there are no orphaned blocks
 tests that metasave overwrites the output file not append 
 test various configuration options of dfs namenode name dir and dfs namenode edits dir the test creates files and restarts cluster with different configs  starts cluster with shared name and edits dirs  restarts cluster by adding additional different name and edits dirs  restarts cluster by removing shared name and edits dirs by allowing to start using separate name and edits dirs  restart cluster by adding shared directory again but make sure we do not read any stale image or edits all along the test we create and delete files at reach restart to make sure we are reading proper edits and image 
 test various configuration options of dfs namenode name dir and dfs namenode edits dir this test tries to simulate failure scenarios  start cluster with shared name and edits dir  restart cluster by adding separate name and edits dirs  restart cluster by removing shared name and edits dir  restart cluster with old shared name and edits dir but only latest name dir this should fail since we don t have latest edits dir  restart cluster with old shared name and edits dir but only latest edits dir this should succeed since the latest edits will have segments leading all the way from the image in name and edits 
 test dfs namenode checkpoint dir and dfs namenode checkpoint edits dir should tolerate white space between values 
 the following test first creates a file it verifies the block information from a datanode then it updates the block with new information and verifies again 
 this test creates a file and modifies the block generation stamp to number that name node has not seen yet it then asserts that name node moves into safe mode while it is in startup mode 
 pretty much the same tests as above but does not setup safemode    true hence we should not have positive count of blocks in future 
 publish a fake metric under the hadoop domain and ensure it is logged by the metrics logger 
 create a namenode object that listens on a randomly chosen port number 
 test an empty edit log 
 test an empty edit log with padding 
 test an empty edit log with extra long padding 
 test an empty edit log with extra long padding 
 test that we can successfully recover from a situation where there is garbage in the middle of the edit log file output stream 
 create a test configuration that will exercise the initializegenerickeys code path this is a regression test for hdfs  
 test that we can successfully recover from a situation where the last entry in the edit log has been truncated 
 test that we can successfully recover from a situation where the last entry in the edit log has been padded with garbage 
 test that don t need to recover from a situation where the last entry in the edit log has been padded with bytes 
 test that don t need to recover from a situation where the last entry in the edit log has been padded with  bytes 
 tests that namenode resource monitor causes the nn to enter safe mode when resources are low 
 https test is different since we need to setup ssl configuration nn also binds the wildcard address for https port by default so we must pick a different host port combination 
 tests for concat call
 verify that writing to ssd and disk will write to the correct storage types 
 test case where two directories are configured as name and edits and one of them fails to save storage since the edits and image failure states are decoupled the failure of image saving should not prevent the purging of logs from that dir 
 test the easy case where we have more images in the directory than we need to keep should purge the old ones 
 same as above but across multiple directories
 test that if we have fewer fsimages than the configured retention we don t purge any of them
 check for edge case with no logs present at all 
 check for edge case with no logs or images present at all 
 test that old in progress logs are properly purged
 check if dfs remains in proper condition after a restart 
 the corrupt block has to be removed when the number of valid replicas matches replication factor for the file in this the above condition is tested by reducing the replication factor the test strategy bring up cluster with  datanodes create a file of replication factor  corrupt one replica of a block of the file verify that there are still  good replicas and  corrupt replica corrupt replica should not be removed since number of good replicas  is less than replication factor  set the replication factor to  verify that the corrupt replica is removed corrupt replica should not be removed since number of good replicas  is equal to replication factor  
 the corrupt block has to be removed when the number of valid replicas matches replication factor for the file in this test the above condition is achieved by increasing the number of good replicas by replicating on a new datanode the test strategy bring up cluster with  datanodes create a file of replication factor  corrupt one replica of a block of the file verify that there are still  good replicas and  corrupt replica corrupt replica should not be removed since number of good replicas  is less than replication factor  start a new data node verify that the a new replica is created and corrupt replica is removed 
 the corrupt block has to be removed when the number of valid replicas matches replication factor for the file the above condition should hold true as long as there is one good replica this test verifies that the test strategy bring up cluster with  datanodes create a file of replication factor  corrupt one replica of a block of the file verify that there is one good replicas and  corrupt replica corrupt replica should not be removed since number of good replicas  is less than replication factor  set the replication factor to  verify that the corrupt replica is removed corrupt replica should be removed since number of good replicas  is equal to replication factor  
 none of the blocks can be removed if all blocks are corrupt the test strategy bring up cluster with  datanodes create a file of replication factor  corrupt all three replicas verify that all replicas are corrupt and  replicas are present set the replication factor to  verify that all replicas are corrupt and  replicas are present 
 both traditional space quota and the storage type quota for ssd are set and exceeded expect dsquotaexceededexception is thrown as we check traditional space quota first and then storage type quota 
 both traditional space quota and the storage type quota for ssd are set and ssd quota is exceeded but traditional space quota is not exceeded 
 both traditional space quota and the storage type quota for ssd are set and traditional space quota is exceeded but ssd quota is not exceeded 
 make sure the nn can detect the scenario where there are enough number of internal blocks   by default but there is still missing data parity block 
 verify that a savenamespace command brings faulty directories in fs name dir and fs edit dir back online 
 test case where savenamespace fails in all directories and then the nn shuts down here we should recover from the failed checkpoint since it only affected ckpt files not valid image files
 test case where savenamespace fails in all directories but then the operator restores the directories and calls it again this should leave the nn in a clean state for next start 
 injects a failure on all storage directories while saving namespace 
 tests transaction logging in dfs 
 secnn  checkpoint for edits and image is the same directory
 seccn  checkpoint for edits and image are different directories 
 this test tests hosts include list contains host names after namenode restarts the still alive datanodes should not have any trouble in getting registrant again 
 verify the following scenario  nn restarts  heartbeat rpc will retry and succeed nn asks dn to reregister  after reregistration completes dn will send heartbeat followed by blockreport  nn will mark datanodestorageinfo blockcontentsstale to false 
 calls doget on the servlet captures the response body as a string and returns it to the caller 
 test  create dfs cluster with  storage directories  edits image  edits  create a cluster and write a file  corrupt disable one storage or two by removing  run docheckpoint it will fail on removed dirs which will invalidate the storages  write another file  check that edits and fsimage differ  run docheckpoint  verify that all the image and edits files are the same 
 test to simulate interleaved checkpointing by  nns after a storage directory has been taken offline the first will cause the directory to come back online but it won t have any valid contents the second nn will then try to perform a checkpoint the nn should not serve up the image or edits from the restored empty dir 
 test to verify the timeout of image upload
 restart the cluster optionally saving a new checkpoint 
 check file content reading as user code readinguser 
 check the file content reading as user code readinguser 
 test that file data can be read by reading the block file directly from the local store 
 try a short circuit from a reader that is not allowed to to use short circuit the test ensures reader falls back to non shortcircuit reads when shortcircuit is disallowed 
 test to run benchmarks between short circuit read vs regular read with specified number of threads simultaneously reading run this using the following command bin hadoop config confdir org apache hadoop hdfs testshortcircuitlocalread 
 test that file data can be read by reading the block through blockreaderremote
 wait for all the internalblocks of the blockgroups of the given file to be reported 
 wait for the reconstruction to be finished when the file has corrupted blocks 
 wait for the reconstruction to be finished when the file has corrupted blocks the function can take care file with any length 
 write out the config files needed by the minikms the minikms doesn t provide a way to set the configs directly so the only way to pass config parameters is to write them out into config files 
 setup a fresh minikms and minidfs 
 setup a fresh minidfs and a minikms the resetkms parameter controls whether the minikms will start fresh or reuse the existing data 
 setup a minidfs and minikms the resetkms and resetdfs parameters control whether the services will start fresh or reuse the existing data 
 test the full life cycle of a key using a config with whitelist key acls the configuration used is the correct configuration to pass the full acl test in link dofullacltest 
 test the full life cycle of a key using a config with key acls the configuration used is the correct configuration to pass the full acl test in link dofullacltest 
 test the full life cycle of a key using a config with whitelist key acls and without blacklist acls the configuration used is the correct configuration to pass the full acl test in link dofullacltest 
 test the full life cycle of a key using a config with whitelist key acls and without blacklist acls the configuration used is the correct configuration to pass the full acl test in link dofullacltest 
 run a full key life cycle test using the provided configuration and users 
 test that key creation is correctly governed by acls 
 test that zone creation is correctly governed by acls 
 test that in zone file creation is correctly governed by acls 
 test that in zone file read is correctly governed by acls 
 test that key deletion is correctly governed by acls 
 read a file as the specified user and compare the contents to expectations 
 this test does not run since switching chunksize with append is not implemented please see hdfs  for a discussion of the difficulties in doing so 
 simple unit test which writes some data with one algorithm then appends with another 
 test which randomly alternates between appending with crc and with crcc crossing several block boundaries then checks that all of the data can be read back correct 
 test randomly mixing append snapshot and truncate operations 
 test dfs raid
 consider a file with hot storage policy increase replication factor of that file from  to  make sure all replications are created in disks 
 consider a file with warm temperature increase replication factor of that file from  to  make sure all replicas are created in disks and archive 
 consider a file with cold temperature increase replication factor of that file from  to  make sure all replicas are created in archive 
 test to make sure the checksum is set correctly after pipeline recovery transfers  byte partial block if fails the test case will say java io ioexception failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try this indicates there was a real failure after the staged failure 
 this test creates a file with three block replicas corrupt all of the replicas make dfs client read the file no block corruption should be reported 
 this test creates a file with three block replicas corrupt two of the replicas make dfs client read the file the corrupted blocks with their owner data nodes should be reported to the name node 
 read a file served entirely from one dn seek around and read from different offsets and verify that they all use the same socket 
 make a single dn cluster corrupt a block and make sure there s no infinite loop but rather it eventually reports the exception to the client 
 same thing with multiple datanodes in history this has behaved differently than the above this test usually completes in around  seconds if it times out this suggests that the client is retrying indefinitely 
 test that a data node does not start if configuration specifies incorrect uri scheme in data directory test that a data node starts if data directory is specified as uri   file path or as a non uri path 
 test that writing to files is good even when datanodes in the pipeline dies 
 write to one file then kill one datanode in the pipeline and then close the file 
 this test attempts to different types of datanode report 
 perform a upgrade using the test image corresponding to testcasename 
 upgrade from   no storage ids per volume correctly generates guid based storage ids test case for hdfs  
 startup from a   layout that has legacy storage ids correctly generates new storage ids test case for hdfs  
 startup from a   layout that already has unique storage ids does not regenerate the storage ids test case for hdfs  
 regression test for hdfs  check that the datanode is respecting its configured keepalive timeout 
 test that the client respects its keepalive timeout 
 tests decommission for non federated cluster
 tests decommission with replicas on the target datanode cannot be migrated to other datanodes and satisfy the replication factor make sure the datanode won t get stuck in decommissioning state 
 test decommission for federeated cluster
 test decommission process on standby nn verify admins can run dfsadmin refreshnodes on sbn and decomm process can finish as long as admins run dfsadmin refreshnodes on active nn sbn used to mark excess replica upon recommission the sbn s pick for excess replica could be different from the one picked by ann that creates inconsistent state and prevent sbn from finishing decommission 
 tests cluster storage statistics during decommissioning for non federated cluster
 tests cluster storage statistics during decommissioning for federated cluster
 test host include file functionality only datanodes in the include file are allowed to connect to the namenode in a non federated cluster 
 test host include file functionality only datanodes in the include file are allowed to connect to the namenode in a federated cluster 
 tests restart of namenode while datanode hosts are added to exclude file
 decommissioned node should not be considered while calculating node usage
 decommission inprogress node should not be considered while calculating node usage
 verify if multiple datanodes can be decommission at the same time 
 tests to verify that the file checksum should be able to compute after the decommission operation below is the block indices list after the decommission represents decommissioned node index           here this list contains duplicated blocks and does not maintaining any order 
 make sure that a helpful error message is shown if a proxy provider is configured for a given uri but no actual addresses are configured for that uri 
 spy on the java dns infrastructure this likely only works on sun derived jdks but uses junit s assume functionality so that any tests using it are skipped on incompatible jdks 
 this makes sure that when dn closes clients socket after client had successfully connected earlier the data can still be fetched 
 verify that client will correctly give up after the specified number of times trying to add a block
 test dfsclient can continue to function after renewlease rpc receives sockettimeoutexception 
 test that getadditionalblock and close are idempotent this allows a client to safely retry a call and still produce a correct file see hdfs  
 test that a dfsclient waits for random time before retry on busy blocks 
 test that timeout occurs when dn does not respond to rpc start up a server and ask it to sleep for n seconds make an rpc to the server and set rpctimeout to less than n and ensure that sockettimeoutexception is obtained
 test client retry with namenode restarting 
 test that the send buffer size default value is  in which case the socket will use a tcp auto tuned value 
 note that link java net socket setsendbuffersize int is only a hint if this test is flaky it should be ignored see hadoop  
 test that if the send buffer size is  the socket will use a tcp auto tuned value 
 this test attempts to finalize the namenode and datanode 
 tests all fseditlogops that are converted to inotify events 
 the computepacketchunksize method of dfsoutputstream should set the actual packet size  kb see hdfs  for details 
 this tests preventing overflows of package size and bodysize see also https issues apache org jira browse hdfs  
 configuredwritepacketsize the configured writepacketsize 
 this tests if permission setting in create mkdir and setpermission works correctly
 this test attempts to rollback the namenode and datanode under a number of valid and invalid conditions 
 create a local file whose content contains its full path 
 check command error outputs and exit statuses 
 test that head displays first kilobyte of the file to stdout 
 test that tail displays last kilobyte of the file to stdout 
 test that tail f outputs appended data as the file grows 
 test chmod 
 tests various options of dfsshell 
 test stat format prints statistics about the file directory at in the specified format 
 delete a file optionally configuring trash on the server and client 
 test that the server trash configuration is respected when the client configuration is not set 
 test that server trash configuration is respected even when the client configuration is set 
 test that the client trash configuration is respected when the server configuration is not set 
 test that trash is disabled by default 
 this test ensures the appropriate response successful or failure from a datanode when the system is started with differing version combinations for each  tuple in the cross product oldlayoutversion currentlayoutversion futurelayoutversion currentnamespaceid incorrectnamespaceid pastfssctime currentfssctime futurefssctime  startup namenode with version file containing currentlayoutversion currentnamespaceid currentfssctime  attempt to startup datanode with version file containing this iterations version  tuple 
 this test iterates over the testcases table and attempts to startup the namenode normally 
 this test iterates over the testcases table for datanode storage and attempts to startup the datanode normally 
 this test iterates over the testcases table for block pool storage and attempts to startup the datanode normally 
 randomly pick a length and run tests with multiple data failures todo enable this later
 when the two datanodes with partial data blocks fail 
 test writing very short ec files with many failures 
 runtest implementation 
 this test attempts to upgrade the namenode and datanode under a number of valid and invalid conditions 
 test that sets up a fake image from hadoop    and tries to start a nn verifying that the correct error message is thrown 
 test upgrade from   image
 test upgrade from   image with corrupt md make sure it fails to upgrade
 test upgrade from a branch   image with reserved paths
 test upgrade from a    image with reserved paths
 test upgrade from   image with a variety of snapshot and reserved paths to test renaming on upgrade
 test upgrade from a  x image with some blocksbeingwritten
 test how name service uris are handled with a variety of configuration settings
 tests the retry logic in startfile we release the lock while generating an edek so tricky things can happen in the intervening time 
 test running the offlineimageviewer on a system with encryption zones 
 test listing encryption zones after zones had been deleted but still exist under snapshots this test first moves ezs to trash folder so that an inodereference is created for the ez then it removes the ez from trash folder to emulate condition where the ez inode will not be complete 
 creates a file with stable link distributedfilesystem tests the following  scenarios  the decrypted data using link webhdfsfilesystem should be same asinput data  gets the underlying raw encrypted stream and verifies that the encrypted data is different than input data 
 filesystem setxattr getxattr getxattrs removexattr listxattrs api call should succeed without failure 
 filesystem allowsnapshot createsnapshot deletesnapshot renamesnapshot getsnapshotdiffreport disallowsnapshot api call should succeed without failure 
 filesystem createsymlink getfilelinkstatus api call should succeed without failure 
 filesystem create open append concat getfilechecksum rename delete api call should succeed without failure 
 filesystem setstoragepolicy unsetstoragepolicy api call should succeed without failure 
 append is not supported in ec 
 truncate is not supported in ec 
 set default acl to a directory and make sure existing sub dirs files does not have default acl 
 create a parent dir and set default acl to allow foo read write access create a sub dir and set default acl to allow bar group read write access parent dir file can not be viewed appended by bar group parent dir child dir file can be viewed appended by bar group 
 verify that sub directory can restrict acl with acl inherited from parent create a parent dir and set default to allow foo and bar full access create a sub dir and set default to restrict bar to empty access parent dir file can be viewed by foo parent dir child dir file can be viewed by foo parent dir child dir file can not be viewed by bar
 download a few fsimages using `hdfs dfsadmin fetchimage and verify the results 
 test a simple flush on a simple hdfs file 
 test that file data can be flushed 
 tests appending after soft limit expires 
 tests appending after soft limit expires 
 old replica of the block should not be accepted as valid for append read
 old replica of the block should not be accepted as valid for append read
 test that appends to files at random offsets 
 test case that stops a writer after finalizing a block but before calling completefile and then tries to recover the lease from another thread 
 test case that stops a writer after finalizing a block but before calling completefile recovers a file from another writer starts writing from that writer and then has the old lease holder call completefile
 regression test for hdfs  creates and appends to files where blocks start end on block boundaries 
 test to verify that the checksum can be computed for a small file less than bytespercrc size 
 test to verify that the checksum can be computed for a small file less than bytespercrc size 
 test to verify that the checksum can be computed by giving bytespercrc length of file range for checksum calculation  is the value of bytespercrc 
 test to verify that the checksum can be computed by giving cellsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving less than cellsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving greater than cellsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving two times cellsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving stripsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving less than stripsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving greater than stripsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving less than blockgroupsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving greaterthan blockgroupsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving greater than blockgroupsize length of file range for checksum calculation 
 test to verify that the checksum can be computed by giving lessthan filesize length of file range for checksum calculation 
 test to verify that the checksum can be computed for a length greater than file size 
 test to verify that the checksum can be computed for a small file less than bytespercrc size 
 test to verify that the checksum can be computed for a small file less than bytespercrc size 
 test to verify that the checksum can be computed for a small file less than bytespercrc size 
 test to verify that the checksum can be computed with greater than file length 
 test to verify that the checksum can be computed for small file with less than file length 
 test that that writes to an incomplete block are available to a reader
 test case if the blocksender decides there is only one packet to send the previous computation of the pktsize based on transfertoallowed would result in too small a buffer to do the buffer copy needed for partial chunks 
 check if local fs can handle corrupted blocks properly 
 test that server default values are cached on the client size and are stale after namenode update 
 test that server defaults are updated on the client after cache expiration 
 same test but the client should use dn hostnames 
 same test but the client should bind to a local interface 
 test if file creation and disk space consumption works right
 test deleteonexit
 test that file data does not become corrupted even in the face of errors 
 test that the filesystem removes the last block from a file if its lease expires 
 test addblock when replicationmin and excludenodes  null 
 test that file leases are persisted across namenode restarts 
 test that all open files are closed when client dies abnormally 
 test file creation using createnonrecursive 
 test that file data becomes available before file is closed 
 create a file write something hflush but not close then change lease period and wait for lease recovery finally read the block directly from each datanode and verify the content 
 regression test for hdfs  creates a file using a non canonical path i e with extra slashes between components and makes sure that the nn can properly restart this test rpcs directly to the nn to ensure that even an old client which passes an invalid path won t cause corrupt edits 
 another regression test for hdfs  this one creates files using a path instantiated from a string object 
 another regression test for hdfs  this one creates files using a path instantiated from a uri object 
 test complete verifies that the fileid in the request matches that of the inode this test checks that filenotfoundexception exception is thrown in case the fileid does not match 
  check the blocks of old file are cleaned after creating with overwrite  restart nn check the file  save new checkpoint and restart nn check the file
 test lease recovery triggered by dfsclient 
 tests the filelength when we sync the file and restart the cluster and datanodes not report to namenode yet 
 test the filestatus obtained calling getfilestatus on a file 
 test filestatus objects obtained from a directory 
 tests read seek getpos skipped opeation for input stream 
 test write operation for output stream in dfs 
 test getblocks 
 make sure that a non hdfs uri throws a helpful error 
 check whether the backupnode can be started 
 verify namenode port usage 
 verify datanode port usage 
 verify secondary namenode port usage 
 verify backupnode port usage 
 the test uses link dothejob configuration string long short boolean enumset to write a file with a standard block size
 the test uses link dothejob configuration string long short boolean enumset to write a file with a custom block size so the writes will be happening across block boundaries
 the test uses link dothejob configuration string long short boolean enumset to write a file with a custom block size so the writes will be happening across block s and checksum boundaries
 the test calls link dothejob configuration string long short boolean enumset while requiring the semantic of link syncflag update length 
 the test calls link dothejob configuration string long short boolean enumset while requiring the semantic of link syncflag end block 
 the test calls link dothejob configuration string long short boolean enumset while requiring the semantic of link syncflag end block and link syncflag update length 
 the test calls link dothejob configuration string long short boolean enumset while requiring the semantic of link syncflag update length similar with link hflush  it writes a file with a custom blocksize so the writes will be happening across block boundaries
 the test calls link dothejob configuration string long short boolean enumset while requiring the semantic of link syncflag update length similar with link hflush  it writes a file with a custom blocksize so the writes will be happening across block s and checksum boundaries 
 this creates a slow writer and check to see if pipeline heartbeats work fine
 test for block size of gb  b this test can take a rather long time to complete on windows reading the file back can be slow so we use a larger timeout here 
 test that we can write to and read from large blocks
 the following test first creates a file with a few blocks it randomly truncates the replica of the last block stored in each datanode finally it triggers block synchronization to synchronize all stored block 
 block recovery when the meta file not having crcs for all chunks in block file
 test the namenode s revoke lease on current lease holder function 
 this test makes the client does not renew its lease and also set the hard lease expiration period to be short s thus triggering lease expiration to happen while the client is still alive the test makes sure that the lease recovery completes and the client fails if it continues to write to the file 
 this test makes it so the client does not renew its lease and also set the hard lease expiration period to be short thus triggering lease expiration to happen while the client is still alive the test also causes the nn to restart after lease recovery has begun but before the dns have completed the blocks this test verifies that when the nn comes back up the client no longer holds the lease the test makes sure that the lease recovery completes and the client fails if it continues to write to the file even after nn restart 
 write a file with blocks of different lengths this method depends on completing before the dfs socket timeout otherwise the client will mark timed out streamers as failed and the write will fail if there are too many failed streamers 
 test when input path is a file 
 test when input path is a directory 
 test when input patch has a symbolic links as its children 
 tests get set working directory in dfs 
 when a node is put to maintenance it first transitions to adminstates entering maintenance it makes sure all blocks have minimal replication before it can be transitioned to adminstates in maintenance if node becomes dead when it is in adminstates entering maintenance it should stay in adminstates entering maintenance state 
 when a node is put to maintenance it first transitions to adminstates entering maintenance it makes sure all blocks have been properly replicated before it can be transitioned to adminstates in maintenance the expected replication count takes dfs namenode maintenance replication min key and its file s replication factor into account 
 test file block replication lesser than maintenance minimum 
 transition from in maintenance to decommissioned 
 first put a node in maintenance then put a different node in decommission make sure decommission process take maintenance replica into account 
 verify if multiple datanodes can transition to maintenance state at the same time 
 after the change of replication factor of live replicas   the new replication factor 
 verify the following scenario a put a live node to maintenance    maintenance  live b the maintenance node becomes dead   block map still has  maintenance  live c take the node out of maintenance   nn should schedule the replication and end up with  live 
 verify the following scenario a put a live node to maintenance    maintenance  live b the maintenance node becomes dead   block map still has  maintenance  live c restart nn   block map only has  live   restore the  live d restart the maintenance dn    maintenance  live e take the node out of maintenance   over replication    live 
 tests modification time in dfs 
 regression test for hdfs  nn does not update internal file mtime for op close when reading from the edit log 
 test case where a bunch of threads are both appending and flushing they all finish before the file is closed 
 test case where a bunch of threads are continuously calling hflush while another thread appends some data and then closes the file the hflushing threads should eventually catch an ioexception stating that the stream was closed and not an npe or anything like that 
 seek to somewhere random and read 
 positional read 
 runs a standard workload using a helper class which provides the read implementation to use 
 do parallel read several times with different number of files and threads note that while this is the only test in a junit sense we re actually dispatching a lot more failures in the other methods and other threads need to be manually collected which is inconvenient 
 check if dfs remains in proper condition after a restart 
 check if dfs remains in proper condition after a restart 
 check if dfs remains in proper condition after a restart
 tests positional read in dfs 
 tests positional read in dfs with hedged reads enabled 
 tests positional read in localfs 
 scenario  write a file with rf  dn and dn  open the stream consider locations are dn dn in locatedblock  move block from dn to dn  let block gets replicated to another dn  stop dn also  current valid block locations in namenode dn dn  consider next calls to getblocklocations always returns dn as last location 
 scenario  write a file with rf  dn and dn  open the stream consider locations are dn dn in locatedblock  move block from dn to dn  let block gets replicated to another dn  stop dn also  current valid block locations in namenode dn dn  consider next calls to getblocklocations always returns dn as last location 
 test quota related commands setquota clrquota setspacequota clrspacequota and count 
 test commands that change the size of the name space mkdirs rename and delete 
 test hdfs operations that change disk space consumed by a directory tree namely create rename delete append and setreplication this is based on testnamespacecommands above 
 test limit cases for setting space quotas 
 violate a space quota using files of size   block test that block allocation conservatively assumes that for quota checking the entire space of the block is used 
 like the previous test but create many files this covers bugs where the quota adjustment is incorrect but it takes many files to accrue a big enough accounting error to violate the quota 
 file count on root should return total value of files in filesystem when one folder contains files more than dfs content summary limit 
 test to set and clear space quote regular usage 
 test to all the commands by passing the fully qualified path 
 test to set and clear space quote by storage type 
 test to set and clear space quote when directory doesn t exist 
 test to set and clear space quote when path is a file 
 test to set and clear space quote when user has no access right 
 after reading a corrupted block make sure the client can correctly report the corruption to the namenode 
 shutdown tolerable number of datanode before reading verify the decoding works correctly 
 test reading while writing 
 try openning a file for append 
 write something to a file 
 test the file blocks reconstruction  check the replica is reconstructed in the target datanode and verify the block replica length generationstamp and content  read the file and verify content 
 open user dir file user dir file mkdir user dir move user dir user dir
 open user dir file user dir file move user dir user dir
 open user dir file mkdir user dir move user dir file user dir 
 open user dir file move user dir file user dir 
 test fail last datanode in the pipeline 
 test fail first datanode in the pipeline 
 test fail all the datanodes except first in the pipeline 
 test fail all the datanodes except lastnode in the pipeline 
 test when number of live nodes are less than the dfs client block write replace datanode on failure min replication 
 test replace datanode on failure 
 tests replication in dfs 
 test if replication can detect mismatched length on disk blocks
 this test makes sure that if a file is under construction blocks in the middle of that file are properly re replicated if they become corrupt 
 tests that getfilestatus on raw and non raw resolve to the same file 
 check if dfs remains in proper condition after a restart 
 check if dfs remains in proper condition after a restart this rerun is with  ports enabled for rpc in the namenode
 test dfsadmin upgrade command 
 verify that the namenode at the given index has an fsimage with a txid up to txid 
 test that the nn initializes its under replicated blocks queue before it is ready to exit safemode hdfs  
 assert that the given function fails to run due to a safe mode exception 
 test if the seek bug exists in fsdatainputstream in dfs 
 tests if the seek bug exists in fsdatainputstream in localfs 
 tests mod access time in dfs 
 tests mod time change at close in dfs 
 tests small block size in in dfs 
 junit test reading while writing 
 junit test position read while writing 
 junit test position read of the current block being written 
 open the file to read from begin to end then close the file return number of bytes read support both sequential read and position read 
 common routine to do position read while open the file for write after each iteration of write do a read of the file from begin to end return  on success else number of failure 
 entry point of the test when using a real cluster usage loop ntimes chunksize nn f filename useseqread  useposread append  truncate verbose  noverbose loop iterate ntimes each iteration consists of a write then a read chunksize number of byte for each write f filename filename to write and read useseqread  useposread use position read or default sequential read append  truncate if file already exist truncate or default append verbose  noverbose additional debugging messages if verbose is on default loop    chunksize    f filename   tmp filex use sequential read append mode verbose on 
 test writing a file with shutting down some dns data dns or parity dns or both 
 test the offlineeditsviewer
 checks that the edits file has all opcodes
 tests that the reversexml processor doesn t accept xml files with the wrong layoutversion 
 test reconfiguration and check the status outputs 
 test that if automatic ha is enabled none of the mutative operations will succeed unless the forcemanual flag is specified 
 test that even if automatic ha is enabled the monitoring operations still function correctly 
 test that the fencing configuration can be overridden per nameservice or per namenode
 test failover with various options
 test case to check whether both the name node is active or not
 test that thread dump is captured after nn state changes 
 test that automatic failover is triggered by shutting the active nn down 
 get address list for a given type of address command expected to fail if code success is false 
 using link getconf methods get the list of given code type ofaddresses
 test empty configuration
 test invalid argument to the tool
 tests to make sure the returned addresses are correct in case of default configuration with no federation
 tests to make sure the returned addresses are correct in case of federation of setup 
 tests for journal node addresses 
 tests commands other than link command namenode link command backup link command secondary and link command nnrpcaddresses 
 initialize the data structures used by this class important note this method must be called once before calling any other public method on this class creates a singleton master populated storage directory for a namenode contains edits fsimage version and time files and a datanode contains version and block files this can be a lengthy operation 
 compute the checksum of all the files in the specified directory this method provides an easy way to ensure equality between the contents of two directories 
 simulate the link dfsconfigkeys dfs namenode name dir key of a populated dfs filesystem this method populates for each parent directory parent dirname with the content of namenode storage directory that comes from a singleton namenode master that contains edits fsimage version and time files if the destination directory does not exist it will be created if the directory already exists it will first be deleted 
 simulate the link dfsconfigkeys dfs datanode data dir key of a populated dfs filesystem this method populates for each parent directory parent dirname with the content of datanode storage directory that comes from a singleton datanode master that contains version and block files if the destination directory does not exist it will be created if the directory already exists it will first be deleted 
 simulate the link dfsconfigkeys dfs datanode data dir key of a populated dfs filesystem this method populates for each parent directory parent dirname with the content of block pool storage directory that comes from a singleton datanode master that contains version and block files if the destination directory does not exist it will be created if the directory already exists it will first be deleted 
 create a version file for datanode inside the specified parent directory if such a file already exists it will be overwritten the given version string will be written to the file as the layout version none of the parameters may be null 
 create a version file for datanode inside the specified parent directory if such a file already exists it will be overwritten the given version string will be written to the file as the layout version none of the parameters may be null 
 create empty block pool directories
 test case where there is no existing file
 test case where there is no existing file
 test case where the flush fails at close time make sure that we clean up after ourselves and don t touch any existing file at the destination
 test when md file exists but has a bad format
 test read and write large files 
 test fsserver defaults response from link distributedfilesystem and link webhdfsfilesystem are the same 
 test get with length parameter greater than actual file length 
 test get with offset and length parameters that combine to request a length greater than actual file length 
 make sure the webhdfsfilesystem will retry based on retriableexception when rpcserver is null in namenodewebhdfsmethods while namenode starts up 
 expect connect timeout because the connection backlog is consumed 
 expect connect timeout on a url that requires auth because the connection backlog is consumed 
 on the second step of two step write expect read timeout accessing the redirect location because the bogus server never sends a reply 
 starts a background thread that accepts one and only one client connection on the server socket sends an http  temporary redirect response and then exits this is useful for testing timeouts on the second step of methods that issue  http requests request  redirect request  for handling the first request this method sets socket timeout to use the initial values defined in urlutils afterwards it guarantees that the second request will use a very short timeout optionally the thread may consume the connection backlog immediately after receiving its one and only client connection this is useful for forcing a connection timeout on the second request on teardown open client connections are closed and the thread is joined 
 consumes the test server s connection backlog by spamming non blocking socketchannel client connections we never do anything with these sockets beyond just initiaing the connections the method saves a reference to each new socketchannel so that it can be closed during teardown we define a very small connection backlog but the os may silently enforce a larger minimum backlog than requested to work around this we create far more client connections than our defined backlog 
 this test specifically checks whether the flusher thread is automatically flushing the files it unfortunately can only test with the alternative flushing schedule because of test timing but it s better than nothing 
 setup the mini dfs cluster 
 main entry point 
 starts dfs as specified in member variable options also writes out configuration and details if requested 
 extracts an integer argument with specified default value 
 starts a minidfsclustermanager with parameters drawn from the command line 
 test jmx connection to namenode 
 test jmx connection to datanode 
 the blocklocation of returned locatedfilestatus will have different formats for replicated and erasure coded file please refer to link filecontext getfileblocklocations path long long formore details 
 get the next item in the list
 create an encryption zone rooted at an empty existing directory using the specified encryption key an encryption zone has an associated encryption key used when reading and writing files within the zone additional options such as provisioning the trash directory can be specified using link createencryptionzoneflag flags 
 provision a trash directory for a given encryption zone 
 sync buffered data to datanodes flush to disk devices 
 build a blockreader with the given options this function will do the best it can to create a block reader that meets all of our requirements we prefer short circuit block readers blockreaderlocal and blockreaderlocallegacy over remote ones since the former avoid the overhead of socket communication if short circuit is unavailable our next fallback is data transfer over unix domain sockets if dfs client domain socket data traffic has been enabled if that doesn t work we will try to create a remote block reader that operates over tcp sockets there are a few caches that are important here the shortcircuitcache stores file descriptor objects which have been passed from the datanode the domainsocketfactory stores information about unix domain socket paths that we not been able to use in the past so that we don t waste time retrying them over and over like all the caches it does have a timeout though the peercache stores peers that we have used in the past if we can reuse one of these peers we avoid the overhead of re opening a socket however if the socket has been timed out on the remote end our attempt to reuse the socket may end with an ioexception for that reason we limit our attempts at socket reuse to dfs client cached conn retry times after that we create new sockets this avoids the problem where a thread tries to talk to a peer that it hasn t talked to in a while and has to clean out every entry in a socket cache full of stale entries 
 fetch a pair of short circuit block descriptors from a local datanode 
 request file descriptors from a domainpeer 
 read from the block file into a buffer this function overwrites checksumbuf it will increment datapos 
 fill the data buffer if necessary validate the data against the checksums we always want the offsets of the data contained in databuf to be aligned to the chunk boundary if we are validating checksums we accomplish this by seeking backwards in the file until we re on a chunk boundary this is necessary because we can t checksum a partial chunk if we are not validating checksums we simply only fill the latter part of databuf 
 read using the bounce buffer a direct read actually has three phases the first drains any remaining bytes from the slow read buffer after this the read is guaranteed to be on a checksum chunk boundary if there are still bytes to read the fast direct path is used for as many remaining bytes as possible up to a multiple of the checksum chunk size finally any odd bytes remaining at the end of the read cause another slow read to be issued which involves an extra copy every slow read tries to fill the slow read buffer in one go for efficiency s sake as described above all non checksum chunk aligned reads will be served from the slower read path 
 the only way this object can be instantiated 
 reads bytes into a buffer until eof or the buffer s limit is reached
 tries to read as many bytes as possible into supplied buffer checksumming each chunk if needed preconditions if checksumming is enabled buf remaining must be a multiple of bytesperchecksum note that this is not a requirement for clients of read bytebuffer in the case of non checksum sized read requests read bytebuffer will substitute a suitably sized buffer to pass to this method postconditions buf limit and buf mark are unchanged buf position   min offsetfromchunkboundary totalbytesread so the requested bytes can be read straight from the buffer 
 ensures that up to len bytes are available and checksummed in the slow read buffer the number of bytes available to read is returned if the buffer is not already empty the number of remaining bytes is returned and no actual read happens 
 when the reader reaches end of the read it sends a status response e g checksum ok to the dn failure to do so could lead to the dn closing our connection which we will re open but won t affect data correctness 
 serialize the actual read result on the wire 
 create a new blockreader specifically to satisfy a read this method also sends the op read block request 
 periodically check in with the namenode and renew all the leases when the lease period is half over 
 create a socket for a write pipeline
 wait for the ack of seqno
 wait for space of dataqueue and queue the packet
 examine whether it is worth waiting for a node to restart 
 if this stream has encountered any errors shutdown threads and mark the stream as closed 
 open a datastreamer to a datanode pipeline so that it can be written to this happens when a file is appended or data streaming fails it keeps on trying until a pipeline is setup
 sleep if a node is restarting this process is repeated until the deadline or the node starts back up 
 open a datastreamer to a datanode so that it can be written to this happens when a file is created and each time a new block is allocated must get block id and the ids of the destinations from the namenode returns the list of target datanodes 
 this function sleeps for a certain amount of time when the writing pipeline is congested the function calculates the time based on a decorrelated filter 
 create a new dfsclient connected to the given namenodeuri or rpcnamenode if ha is enabled and a positive value is set for link hdfsclientconfigkeys dfs client test drop namenode response num key in the configuration the dfsclient will use link lossyretryinvocationhandler as its retryinvocationhandler otherwise one of namenodeuri or rpcnamenode must be null 
 see clientprotocol getpreferredblocksize string 
 get server default values for a number of configuration params 
 report corrupt blocks that were discovered by the client 
 recover a file s lease
 get block location info about file getblocklocations returns a list of hostnames that store data for a specific file region it returns a set of hostnames for every block within the indicated region this function is very useful when writing code that considers data placement when performing operations for example the mapreduce system tries to schedule tasks on the same machines as the data block the task processes please refer to link filesystem getfileblocklocations filestatus long long for more details 
 wraps the stream in a cryptoinputstream if the underlying file is encrypted 
 wraps the stream in a cryptooutputstream if the underlying file is encrypted 
 wraps the stream in a cryptooutputstream if the underlying file is encrypted 
 create an input stream that obtains a nodelist from the namenode and then reads from all the right places creates inner subclass of inputstream that does the right out of band work 
 create an input stream that obtains a nodelist from the namenode and then reads from all the right places creates inner subclass of inputstream that does the right out of band work 
 create an input stream from the link hdfspathhandle if theconstraints encoded from link distributedfilesystem createpathhandle filestatus options handleopt are satisfied note that hdfs does not ensure that these constraints remain invariant for the life of the stream it only checks that they still held when the stream was opened 
 call link create string boolean short long progressable withdefault replication and blocksize and null progress 
 call link create string boolean short long progressable withdefault replication and blocksize 
 call link create string boolean short long progressable withnull progress 
 call link create string boolean short long progressable int with default buffersize 
 call link create string fspermission enumset short long progressable int checksumopt with default permission link fspermission getfiledefault 
 call link create string fspermission enumset boolean short long progressable int checksumopt with createparentset to true 
 create a new dfs file with the specified block replication with write progress reporting and return an output stream for writing into the file 
 same as link create string fspermission enumset boolean short long progressable int checksumopt with the addition of favorednodes that isa hint to where the namenode should place the file blocks the favored nodes hint is not persisted in hdfs hence it may be honored at the creation time only hdfs could move the blocks during balancing or replication to move the blocks from favored nodes a value of null means no favored nodes for this create
 same as link create string fspermission enumset boolean short long progressable int checksumopt inetsocketaddress with the addition ofecpolicyname that is used to specify a specific erasure coding policy instead of inheriting any policy from this new file s parent directory this policy will be persisted in hdfs a value of null means inheriting parent groups whatever policy 
 append to an existing file if link createflag append is present
 same as link create string fspermission enumset short long progressable int checksumopt except that the permissionis absolute ie has already been masked with umask 
 creates a symbolic link 
 resolve the first symlink if any in the path 
 invoke namenode append rpc it retries in case of some link retriableexception 
 method to get stream returned by append call 
 append to an existing hdfs file 
 append to an existing hdfs file 
 set replication for an existing file 
 set storage policy for an existing file directory
 unset storage policy set for a given file directory 
 param path file directory name
 return all the existing storage policies
 rename file or directory 
 move blocks from src to trg and delete src see link clientprotocol concat 
 rename file or directory 
 truncate a file to an indicated size see link clientprotocol truncate 
 delete file or directory see link clientprotocol delete string boolean 
 delete file or directory delete contents of the directory if non empty and recursive set to true
 implemented using getfileinfo src 
 get a partial listing of the indicated directory no block locations need to be fetched
 get a partial listing of the indicated directory recommend to use hdfsfilestatus empty name as startafter if the application wants to fetch a listing starting from the first entry in the directory
 get the file info for a specific file or directory 
 get the file info for a specific file or directory 
 close status of a file
 get the file info for a specific file or directory if src refers to a symlink then the filestatus of the link is returned 
 return true if data sent between this client and dns should be encrypted false otherwise 
 get the checksum of the whole file or a range of the file note that the range always starts from the beginning of the file the file can be in replicated form or striped mode it can be used to checksum and compare two replicated files or two striped files but not applicable for two files of different block layout forms 
 infer the checksum type for a replica by sending an op read block for the first byte of that replica this is used for compatibility with older hdfs versions which did not include the checksum type in opblockchecksumresponseproto 
 set permissions to a file or directory 
 set file or directory owner 
 see clientprotocol getstats 
 returns count of blocks with no good replicas left normally should be zero 
 returns count of blocks with replication factor  and have lost the only replica 
 returns count of blocks pending on deletion 
 returns aggregated count of blocks with less redundancy 
 returns count of blocks with at least one replica marked corrupt 
 returns number of bytes that reside in blocks with future generation stamps 
 return a list in which each entry describes a corrupt file block
 enter leave or get safe mode 
 create one snapshot 
 delete a snapshot of a snapshottable directory 
 rename a snapshot 
 get all the current snapshottable directories 
 allow snapshot on a directory 
 disallow snapshot on a directory 
 get the difference between two snapshots or between a snapshot and the current tree of a directory 
 save namespace image 
 rolls the edit log on the active namenode 
 enable disable restore failed storage 
 refresh the hosts and exclude files rereads them see link clientprotocol refreshnodes for more details 
 dumps dfs data structures into specified file 
 requests the namenode to tell all datanodes to use a new non persistent bandwidth value for dfs datanode balance bandwidthpersec see link clientprotocol setbalancerbandwidth long for more details 
 see clientprotocol finalizeupgrade 
 
 create a directory or hierarchy of directories with the given name and permission 
 same link mkdirs string fspermission boolean exceptthat the permissions has already been masked against umask 
 same link mkdirs string fspermission boolean exceptthat the permissions has already been masked against umask 
 get link contentsummary rooted at the specified directory 
 get link quotausage rooted at the specified directory 
 sets or resets quotas for a directory 
 sets or resets quotas by storage type for a directory 
 set the modification and access time of a file
 probe for encryption enabled on this filesystem 
 get the erasure coding policy information for the specified path
 get a remote iterator to the open files list managed by namenode 
 get a remote iterator to the open files list by path managed by namenode 
 get a remote iterator to the open files list by type managed by namenode 
 get a remote iterator to the open files list by type and path managed by namenode 
 returns the next event batch in the stream waiting up to the specified amount of time for a new batch returns null if one is not available at the end of the specified amount of time the time before the method returns may exceed the specified amount of time by up to the time required for an rpc to the namenode 
 returns the next batch of events in the stream waiting indefinitely if a new batch is not immediately available 
 grab the open file info from namenode
 read the block length from one of the datanodes 
 return collection of blocks that has already been located 
 get block at the specified position fetch it from the namenode if not cached 
 fetch a block from namenode and cache it 
 get blocks in the specified range fetch them from the namenode if not cached this function will not get a read request beyond the eof 
 get blocks in the specified range includes only the complete blocks fetch them from the namenode if not cached 
 open a datainputstream to a datanode so that it can be read from we get block id and the ids of the destinations at startup from the namenode 
 read the entire buffer 
 choose datanode to read from 
 read data from one datanode 
 refresh cached block locations 
 like link fetchblockbyterange except we start up a second parallel hedged read if the first read is taking longer than configured amount of time we then wait on which ever read returns first 
 read bytes starting from the specified position 
 seek to a new arbitrary location
 same as link seektonewsource long except that it does not excludethe current datanode and might connect to the same node 
 seek to given position on a node other than the current node if a node other than the current node is found then returns true if another node could not be found then returns false 
 construct a new output stream for append 
 create an empty packet to mark the end of the block 
 if encountering a block boundary send an empty packet to indicate the end of block and reset bytescurblock 
 flushes out to all replicas of the block the data is in the buffers of the dns but not necessarily in the dn s os buffers it is a synchronous operation when it returns it guarantees that flushed data become visible to new readers it is not guaranteed that data has been flushed to persistent store on the datanode block allocations are persisted on namenode 
 the expected semantics is all data have flushed out to all replicas and all replicas have done posix fsync equivalent ie the os has flushed it to the disk device but the disk may have it in its cache note that only the current block is flushed to the disk device to guarantee durable sync across block boundaries the stream should be created with link createflag sync block 
 flush sync buffered data to datanodes 
 deprecated use link hdfsdataoutputstream getcurrentblockreplication 
 note that this is not a public api use link hdfsdataoutputstream getcurrentblockreplication instead 
 waits till all existing data is flushed and confirmations received from datanodes 
 aborts this output stream and releases any system resources associated with this stream 
 closes this output stream and releases any system resources associated with this stream 
 write data to this packet 
 write checksums to this packet
 write the full packet including the header to the given output stream 
 when seeking into a new block group create blockreader for each internal block in the group 
 the super method link dfsinputstream refreshlocatedblock refreshescached locatedblock by executing link dfsinputstream getblockat again this method extends the logic by first remembering the index of the internal block and re parsing the refreshed block group with the same index 
 real implementation of pread 
 check and handle data streamer failures this is called only when we have written a full stripe i e enqueue all packets for a full stripe or when we re closing the outputstream 
 waits for streamers to be created 
 converts a string to a byte array using utf encoding 
 decode a specific range of bytes of the given byte array to a string using utf 
 connect to the given datanode s datantrasfer port and return the resulting iostreampair this includes encryption wrapping etc 
 start the lease recovery of a file
 same as link create path fspermission boolean int short long progressable with the addition of favorednodes that is a hint towhere the namenode should place the file blocks the favored nodes hint is not persisted in hdfs hence it may be honored at the creation time only and with favored nodes blocks will be pinned on the datanodes to prevent balancing move the block hdfs could move the blocks during replication to move the blocks from favored nodes a value of null means no favored nodes for this create
 same as link create path fspermission enumset int short long progressable checksumopt with a few additions first addition offavorednodes that is a hint to where the namenode should place the file blocks the favored nodes hint is not persisted in hdfs hence it may be honored at the creation time only and with favored nodes blocks will be pinned on the datanodes to prevent balancing move the block hdfs could move the blocks during replication to move the blocks from favored nodes a value of null means no favored nodes for this create the second addition is ecpolicyname a non null ecpolicyname specifies an explicit erasure coding policy for this file overriding the inherited policy a null ecpolicyname means the file will inherit its ec policy or replication policy from its ancestor the default ecpolicyname and should replicate createflag are mutually exclusive it s invalid to set both should replicate and a non null ecpolicyname 
 similar to link create path fspermission enumset int short long progressable checksumopt inetsocketaddress string it provides a hdfs specific version of link createnonrecursive path fspermission enumset int short long progressable with a few additions 
 move blocks from srcs to trg and delete srcs afterwards the file block sizes must be the same 
 this rename operation is guaranteed to be atomic 
 set a directory s quotas
 set the per type storage quota of a directory 
 list all the entries of a directory note that this operation is not atomic for a large directory the entries of a directory may be fetched from namenode multiple times it only guarantees that each name occurs once if a directory undergoes changes between the calls 
 the blocklocation of returned locatedfilestatus will have different formats for replicated and erasure coded file please refer to link filesystem getfileblocklocations filestatus long long formore details 
 returns a remote iterator so that followup calls are made on demand while consuming the entries this reduces memory consumption during listing of a large directory 
 create a directory only when the parent directories exist see link fspermission applyumask fspermission for details of howthe permission is applied 
 create a directory and its parent directories see link fspermission applyumask fspermission for details of howthe permission is applied 
 return datanode statistics 
 enter leave or get safe mode 
 save namespace image namenode always does the checkpoint 
 utility function that returns if the namenode is in safemode or not in ha mode this api will return only activenn s safemode status 
 see hdfsadmin allowsnapshot path 
 see hdfsadmin disallowsnapshot path 
 returns a remote iterator so that followup calls are made on demand while consuming the snapshotdiffreportlisting entries this reduces memory consumption overhead in case the snapshotdiffreport is huge 
 get the difference between two snapshots or between a snapshot and the current tree of a directory 
 get the close status of a file
 see link addcachedirective cachedirectiveinfo enumset 
 see link modifycachedirective cachedirectiveinfo enumset 
 set the source path to the specified erasure coding policy 
 get erasure coding policy information for the specified path
 unset the erasure coding policy from the source path 
 get all the trash roots of hdfs for current user or for all the users  file deleted from non encryption zone user username trash  file deleted from encryption zones e g ez rooted at ez has its trash root at ez trash user
 build hdfsdataoutputstream to write 
 perform the file checksum computing the intermediate results are stored in the object and will be used later 
 return true when sounds good to continue or retry false when severe condition or totally failed 
 creates the namenode proxy with the clientprotocol this will handle creation of either ha or non ha enabled proxy objects depending upon if the provided uri is a configured logical uri 
 generate a dummy namenode proxy instance that utilizes our hacked link lossyretryinvocationhandler proxy instance generated using this method will proactively drop rpc responses currently this method only support ha setup null will be returned if the given configuration is not for ha 
 creates the failover proxy provider instance
 periodically check in the cache and expire the entries older than expiryperiod minutes 
 read from in 
 reads all of the data for the next packet into the appropriate buffers the data slice and checksum slice members will be set to point to the user data and corresponding checksums the header will be parsed and set 
 see receivenextpacket readablebytechannel 
 rewrite the last read packet on the wire to the given output stream 
 send sasl message and negotiated cipher option to client 
 sends a sasl negotiation message indicating an error 
 sends a sasl negotiation message 
 send a sasl negotiation message and negotiation cipher options to server 
 sends a sasl negotiation message 
 sends client sasl negotiation for a newly allocated socket if required 
 sends client sasl negotiation for a peer if required 
 sends client sasl negotiation for a socket if required 
 checks if an address is already trusted and then sends client sasl negotiation if required 
 sends client sasl negotiation if required determines the correct type of sasl handshake based on configuration 
 sends client sasl negotiation for specialized encrypted handshake 
 sends client sasl negotiation for general purpose handshake 
 this method actually executes the client side sasl handshake 
 creates a saslparticipant wrapping a saslserver 
 creates a saslparticipant wrapping a saslclient 
 see link saslserver evaluateresponse 
 wraps a byte array 
 unwraps a byte array 
 initialize a operation 
 read from in 
 write to out 
 constructor 
 readfields peeks at the first byte of the datainput and determines if it was written using writableutils legacy or protobuf we can do this because we know the first field is the expiry date in the case of the legacy buffer the expiry date is a vint so the size which should always be  is encoded in the first byte which is always negative due to this encoding however there are sometimes null blocktokenidentifier written so we also need to handle the case there the first byte is also  in the case of protobuf the first byte is a type tag for the expiry date which is written as field number so as long as the field number is less than  but also positive then we know we have a protobuf 
 read the checksum header from the meta file inputstream must be closed by the caller 
 read the checksum header from the meta input stream 
 read the header without changing the position of the filechannel this is used by the client for short circuit reads 
 this reads all the fields till the beginning of checksum 
 reads header at the top of metadata file and returns the header closes the input stream after reading the header 
 read the header at the beginning of the given block meta file the current file position will be altered by this method if an error occurs the file is not closed 
 this writes all the fields till the beginning of checksum 
 writes all the fields till the beginning of checksum 
 close all the proxy objects which have been opened over the lifetime of this proxy provider 
 close the proxy 
 creates a executor and invokes all proxies concurrently this implementation assumes that clients have configured proper socket timeouts else the call can block forever 
 ask the datanode for a new shared memory segment this function must be called with the manager lock held we will release the lock while communicating with the datanode 
 allocate a new shared memory slot connected to this datanode must be called with the endpointshmmanager lock held 
 fetch or create a replica you must hold the cache lock while calling this function 
 close the cache and free all associated resources 
 calculate the usable size of a shared memory segment we round down to a multiple of the slot size and do some validation 
 create the shortcircuitshm 
 we need decoding thus go through all the data chunks and make sure we submit read requests for all of them 
 read the whole stripe do decoding if necessary
 allocate a byte array if the number of allocated arrays   maximum the current thread is blocked until the number of allocated arrays drops to below the maximum the byte array allocated by this method must be returned for recycling via the link fixedlengthmanager recycle byte method 
 allocate a byte array where the length of the allocated array is the least power of two of the given length unless the given length is less than link min array length in such case the returned array length is equal to link min array length if the number of allocated arrays exceeds the capacity the current thread is blocked until the number of allocated arrays drops to below the capacity the byte array allocated by this method must be returned for recycling via the link impl release byte method 
 deserialize a set of datanodeadminproperties from a json file 
 serialize a set of datanodeadminproperties to a json file 
 load user defined ec policies from a xml configuration file 
 load ec policies from a xml configuration file 
 load layoutversion from root element in the xml configuration file 
 path to the xml file containing user defined ec policies if the path is relative it is searched for in the classpath 
 load a schema from a schema element in the xml configuration file 
 load a ec policy from a policy element in the xml configuration file 
 close the closeable objects and ignore any link ioexception ornull pointers must only be used for cleanup in exception handlers 
 get the next completed striped read task 
 create with the specified urlopeners original url is used to open the stream for the first time resolved url is used in subsequent requests 
 convert a json map to a token of blocktokenidentifier 
 convert a json map to an datanodeinfo object 
 convert an object to a datanodeinfo 
 convert a json map to locatedblock 
 convert an list of object to a list of locatedblock 
 expecting only single xattr in the map return its value 
 convert a json map to a token of delegationtokenidentifier 
 convert a json map to locatedblock 
 convert the parameters to a sorted string 
 construct a new urlconnectionfactory that supports oaut based connections it will also try to load the ssl configuration when they are specified 
 opens a url with read and connect timeouts
 opens a url with read and connect timeouts
 return a url pointing to given path on the namenode 
 two step requests redirected to a dn create append step  submit a http request with neither auto redirect nor data step  submit another http request with the url from the location header with data the reason of having two step create append is for preventing clients to send out the data before the redirect this issue is addressed by the expect  continue header in http   see rfc  section    unfortunately there are software library bugs e g jetty  http server and java  http client which do not correctly implement expect  continue the two step create append is a temporary workaround for the software library bugs open checksum also implements two step connects for other operations redirected to a dn such as open and checksum
 create a symlink pointing to the destination path 
 setup offset url and connect 
 remove offset parameter if there is any from the url 
 remove offset parameter before returning the resolved url 
 get link filestatus of files directories in the given path if pathcorresponds to a file then link filestatus of that file is returned else if path represents a directory then link filestatus of allfiles directories inside given path is returned 
 get directoryentries of the given path directoryentries contains an array of link filestatus as well as iteration information 
 test load ec policy 
 test load null ec schema option 
 test load repetitive ec schema 
 test load bad ec policy layoutversion 
 test load bad ec policy cellsize 
 test load bad ec policy 
 convenience method that creates a httpurlconnection for the httpfsserver file system operations this methods performs and injects any needed authentication credentials via the link getconnection url string method
 convenience method that creates a httpurlconnection for the httpfsserver file system operations this methods performs and injects any needed authentication credentials via the link getconnection url string method
 convenience method that creates a httpurlconnection for the specified url this methods performs and injects any needed authentication credentials 
 called after a new filesystem instance is constructed 
 opens an fsdatainputstream at the indicated path important the returned fsdatainputstream does not support the positionreadable and seekable methods 
 opens an fsdataoutputstream at the indicated path with write progress reporting important the progressable parameter is not used 
 append to an existing file optional operation important the progressable parameter is not used 
 truncate a file 
 concat existing files together 
 renames path src to path dst can take place on local fs or remote dfs 
 delete a file 
 delete a file 
 get link filestatus of files directories in the given path if pathcorresponds to a file then link filestatus of that file is returned else if path represents a directory then link filestatus of allfiles directories inside given path is returned 
 get link directoryentries of the given path link directoryentries contains an array of link filestatus as well as iteration information 
 make the given file and all non existent parents into directories has the semantics of unix mkdir p existence of the directory hierarchy is not an error 
 return a file status object that represents the path 
 return the current user s home directory in this filesystem the default implementation returns user user 
 get the root directory of trash for a path in hdfs  file in encryption zone returns ez trash username  file not in encryption zone or encountered exception when checking the encryption zone of the path returns users username trash caller appends either current or checkpoint timestamp for trash destination the default implementation returns user username trash 
 set owner of a path i e a file or a directory the parameters username and groupname cannot both be null 
 set permission of a path 
 set access time of a file
 set replication for an existing file 
 modify the acl entries for a file 
 remove the specified acl entries from a file
 removes the default acl for the given file
 remove all acls from a file
 set the acls for the given file
 get the acl information for a given file
 convenience method that creates an http url for the httpfsserver file system operations 
 convenience method that creates an http url for the httpfsserver file system operations 
 convenience method that json parses the inputstream of a httpurlconnection 
 executes the filesystem operation 
 executes the filesystem operation 
 executes the filesystem operation 
 executes the filesystem operation 
 returns the hadoop auth configuration from httpfsserver s configuration it returns all httpfsserver s configuration properties prefixed with httpfs authentication the httpfs authentication prefix is removed from the returned property names 
 special binding for as it is not handled by the wildcard binding 
 binding to handle get requests supported operations are
 invokes the wrapped callable runnable as a callable 
 invokes the wrapped callable runnable as a runnable 
 initializes the service it collects all service properties properties having the server service prefix the property names are then trimmed from the server service prefix after collecting the service properties it delegates to the link init method 
 initializes the server the initialization steps are it verifies the service home and temp directories exist loads the server server default xml configuration file from the classpath initializes logj logging if the server logj properties file does not exist in the config directory it load default logj properties from the classpath loads the server site xml file from the server config directory and merges it with the default configuration loads the services initializes the services post initializes the services sets the server startup status 
 initializes logj logging 
 loads and inializes the server configuration 
 loads the specified services 
 loads services defined in services and services ext and de dups them 
 initializes the list of services 
 adds a service programmatically if a service with the same interface exists it will be destroyed and removed before the given one is initialized and added if an exception is thrown the server is destroyed 
 resolves the requester hostname and delegates the request to the chain the requester hostname is available via the link get method 
 resolves the host and port inetsocketaddress the web server is listening to this implementation looks for the following  properties server name http hostname server name http port 
 returns the hostname port inetsocketaddress the webserver is listening to 
 create a configuration from an inputstream error canibalized from configuration loadresource 
 set xattr 
 get xattrs 
 remove xattr 
 list xattrs 
 simple acl tests on a file set an acl add an acl remove one acl and remove all acls 
 simple acl tests on a directory set a default acl remove default acls 
 talks to the http interface to create a file 
 talks to the http interface to get the json output of a status command on the given file 
 general purpose http put command to the httpfs server 
 general purpose http put command to the httpfs server which returns relted httpurlconnection instance 
 parse xattrs from json result of getxattrs call return xattrs map 
 validate that files are created with  permissions when no permissions attribute is specified and when permissions is specified that value is honored 
 validate xattr get set remove calls 
 validate the various acl set modify remove calls general strategy is to verify each of the following steps with getfilestatus liststatus and getaclstatus create a file with no acls add a user  group acl add another user acl remove the first user acl remove all acls 
 test acl operations on a directory including default acls general strategy is to use getfilestatus and getaclstatus to verify initial status with no acls the addition of a default acl the removal of default acls 
 create an httpfs server to talk to the minidfscluster we created 
 talks to the http interface to get the json output of a status command on the given file 
 general purpose http put command to the httpfs server 
 test without acls ensure that getfilestatus and liststatus work happily aclstatus throws an exception the acl set remove etc calls all fail 
 create an httpfs server to talk to the minidfscluster we created 
 talks to the http interface to get the json output of a status command on the given file 
 general purpose http put command to the httpfs server 
 ensure that getxattrs setxattr removexattr fail 
 makes the current thread sleep for the specified number of milliseconds the sleep time is multiplied by the link getwaitforratio 
 waits up to the specified timeout for the given link predicate tobecome true the timeout time is multiplied by the link getwaitforratio 
 returns the authority hostname port used by the jettyserver 
 returns the base url schema host port of the test jetty server see link getjettyserver once started 
 dump the given intput stream to stderr 
 wait for the given process to return and check that it exited as required log if the process failed 
 exec the given command and assert it executed successfully 
 exec the given command and assert it executed successfully 
 exec the given command and assert it executed successfully 
 exec the given command assert it returned an error code 
 create and write the given file 
 check that the given file exists with the given contents 
 run a fuse dfs process to mount the given dfs 
 tear down the fuse dfs process and mount 
 test basic directory creation access removal 
 test basic file creation and writing 
 test creating a file via touch 
 test random access to a file 
 test copying a set of files from the mount to itself 
 test concurrent creation and access of the mount 
 dump data into a file 
 writing the data into a local file after the writing if link datastate is still allow dump set link data to null and set link datastate to dumped 
 note that this transition method is called directly and synchronously by mrappmaster s init method i e no rpc no thread switching just plain sequential call within am context so we can trigger modifications in am state from here at least if am is written that way mr version is 
 create a link localresource record with all the given parameters the nm that hosts am container will upload resources to shared cache thus there is no need to ask task container s nm to upload the resources to shared cache set the shared cache upload policy to false 
 create the common link containerlaunchcontext for all attempts 
 parse the url that needs to be notified of the end of the job along with the number of retries in case of failure the amount of time to wait between retries and proxy settings
 notify the url just once use best effort 
 notify a server of the completion of a submitted job the user must have configured mrjobconfig mr job end notification url
 clean up staging directories for the job 
 obtain the tokens needed by the job and put them in the ugi
 check if the current job attempt should try to recover from previous job attempts if any 
 executes the given action that can throw a checked exception with the job classloader set as the configuration classloader as well as the thread context class loader if the job classloader is enabled after the call the original classloader is restored if the job classloader is enabled and the code needs to load user supplied classes via configuration or thread context classloader this method should be used in order to load them 
 render the singlejobcounter page
 render the singletaskcounter page
 runs memory and time benchmark with mock mrapp 
 tests whether scheduled reducers are excluded from headroom while calculating headroom 
 the test verifies that the am re runs maps that have run on bad nodes it also verifies that the am records all success killed events so that reduces are notified about map output status changes it also verifies that the re run information is preserved across am restart
 am with  maps and  reduce for st map one attempt fails one attempt completely disappears because of failed launch one attempt gets killed and one attempt succeeds am crashes after the first tasks finishes and recovers completely and succeeds in the second generation 
 am with  maps and  reduce for st map one attempt fails one attempt completely disappears because of failed launch one attempt gets killed and one attempt succeeds am crashes after the first tasks finishes and recovers completely and succeeds in the second generation 
 set up the distributed cache by localizing the resources and updating the configuration with references to the localized resources 
 creates a class loader that includes the designated files and archives 
 run a set of tasks and waits for them to complete 
 task is reporting that it is in commit pending and it is waiting for the commit response
 constructs the job history file name from the jobindexinfo 
 parses the provided job history file name to construct a jobindexinfo object which is returned 
 helper function to encode the url of the filename of the job history log file 
 helper function to decode the url of the filename of the job history log file 
 looks for the dirs to clean the folder structure is yyyy mm dd serial so we can use that to more efficiently find the directories to clean by comparing the cutoff timestamp with the timestamp from the folder structure 
 creates and sets a link applicationclassloader on the givenconfiguration and as the thread context classloader if link mrjobconfig mapreduce job classloader is set to true andthe app classpath environment variable is set 
 creates a link applicationclassloader if link mrjobconfig mapreduce job classloader is set to true andthe app classpath environment variable is set 
 add the jvm system properties necessary to configure link containerlogappender or link containerrollinglogappender 
 tests using the local job runner 
 verify the name of jobhistory file is not greater than  bytes even if there are some multibyte characters in the job name 
 returns mtime of a given cache file on hdfs internal to mapreduce 
 write the given k v to the cache write to memcache if space is available else write to the filecache
 this function is called the valuesiterator when a mark is called outside of a reset zone 
 for writing the first key and value bytes directly from the value iterators pass the current underlying output stream
 this method determines if there is enough space left in the memory cache to write to the requested length  space for subsequent eof makers 
 add files in the input path recursively into the results 
 list input directories subclasses may override to e g select only files matching a regular expression 
 splits files returned by link liststatus jobconf whenthey re too big 
 this function identifies and returns the hosts that contribute most for a given split for calculating the contribution rack locality is treated on par with host locality so hosts from racks that contribute the most are preferred over hosts on racks that contribute less
 this function identifies and returns the hosts that contribute most for a given split for calculating the contribution rack locality is treated on par with host locality so hosts from racks that contribute the most are preferred over hosts on racks that contribute less
 close the input stream note that we need to read to the end of the stream to validate the checksum 
 read bytes from the stream at eof checksum is validated but the checksum bytes are not passed back in the buffer 
 read bytes from the stream at eof checksum is validated and sent back as the last four bytes of the buffer the caller should handle these bytes appropriately
 finishes writing data to the output stream by writing the checksum bytes to the end the underlying stream is not closed 
 write bytes to the stream 
 this method gets the index information for the given mapid and reduce it reads the index file into cache if it is not already present 
 deprecated applications should rather use link killtask taskattemptid boolean 
 build a job client with the given link jobconf and connect to the default cluster
 build a job client with the given link configuration and connect to the default cluster
 submit a job to the mr system this returns a handle to the link runningjob which can be used to trackthe running job 
 submit a job to the mr system this returns a handle to the link runningjob which can be used to trackthe running job 
 get an link runningjob object to track an ongoing job returnsnull if the id does not correspond to any known job 
 deprecated applications should rather use link getjob jobid 
 get the information of the current state of the map tasks of a job 
 deprecated applications should rather use link getmaptaskreports jobid 
 get the information of the current state of the reduce tasks of a job 
 get the information of the current state of the cleanup tasks of a job 
 get the information of the current state of the setup tasks of a job 
 deprecated applications should rather use link getreducetaskreports jobid 
 display the information about a job s tasks of a particular type and in a particular state
 get the jobs that are not completed and not failed 
 utility that submits a job then polls for progress until the job is complete 
 monitor a job and print status in real time as progress is made and tasks fail 
 returns an array of queue information objects about root level queues configured
 returns an array of queue information objects about immediate children of queue queuename 
 return an array of queue information objects about all the job queues configured 
 use mrasyncdiskservice moveanddeleteallvolumes instead 
 constructs a local file name files are distributed among configured local directories 
 get the link jobpriority for this job 
 get the priority for this job 
 method used to display information pertaining to a single jobqueue registered with the link queuemanager display of the jobs is determine by the boolean
 interpret a given string as a composite expression code func   func   tbl class   see java lang class forname java lang string path   see org apache hadoop fs path path java lang string reads expression from the mapred join expr property and user supplied join types from mapred join define lt ident gt types paths supplied to tbl are given as input paths to the inputformat class listed 
 build a compositeinputsplit from the child inputformats by assigning the ith split from each child to the ith composite split 
 construct a compositerecordreader for the children of this inputformat as defined in the init expression the outermost join need only be composable not necessarily a composite mandating tuplewritable isn t strictly correct 
 inheritdoc 
 write the next value into key value as accepted by the operation associated with this set of recordreaders 
 if key provided matches that of this composite give joincollector iterator over values it may emit 
 close all child rrs 
 inheritdoc 
 return the node type registered for the particular identifier by default this is a cnode for any composite node and a wnode for quot wrapped quot nodes user nodes will likely be composite nodes 
 for a given identifier add a mapping to the nodetype for the parse tree and to the composablerecordreader to be created including the formals required to invoke the constructor the nodetype and constructor signature should be filled in from the child node 
 let the first actual define the inputformat and the second define the mapred input dir property 
 parse a list of comma separated nodes 
 given an expression and an optional comparator build a tree of inputformats using the comparator to sort keys 
 skip key value pairs with keys less than or equal to the key provided 
 add an iterator to the collector at the position occupied by this recordreader over the values in this stream paired with the key provided ie register a stream of values from this source matching k with a collector 
 write key value pair at the head of this stream to the objects provided get next key value pair from proxied rr 
 construct a jvmid object from given string 
 create an aggregate based map reduce job 
 create an aggregate based map reduce job 
 create and run an aggregate based map reduce job 
 a generic recordreader that can hand out different recordreaders for each chunk in the combinefilesplit 
 inheritdoc 
 create a composite record writer that can write key value data to different output files
 gets the output collector for a named output 
 gets the output collector for a multi named output 
 read a line 
 get the progress within the split
 start executing and return filestatuses based on the parameters specified
 serialize the key value to intermediate storage when this method returns kvindex must refer to sufficient unused storage to store one metadata 
 set position from last mark to end of writable buffer then rewrite the data between last mark and kvindex this handles a special case where the key wraps around the buffer if the key is to be passed to a rawcomparator then it must be contiguous in the buffer this recopies the data in the buffer back into itself but starting at the beginning of the buffer note that this method should only be called immediately after detecting this condition to call it at any other time is undefined and would likely result in data loss or corruption 
 attempt to write a sequence of bytes to the collection buffer this method will block if the spill thread is running and it cannot write 
 for committing job s output after successful job completion note that this is invoked for jobs with final runstate as successful this is called from the application master process for the entire job this is guaranteed to only be called once if it throws an exception the entire job will fail 
 for aborting an unsuccessful job s output note that this is invoked for jobs with final runstate as link jobstatus failed or link jobstatus killed this is called from the application master process for the entire job this may be called multiple times 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 this method implements the new interface by calling the old method note that the input types are different between the new and old apis and this is a bridge between the two 
 start the child process to handle the task for us 
 clean up the child procress and socket 
 run a given command in a subprocess including threads to copy its stdout and stderr to our stdout and stderr 
 create a proxy object that will speak the binary protocol on a socket upward messages are passed on the specified handler and downward downward messages are public methods on this object 
 close the connection and shutdown the handler thread 
 write the given object to the stream if it is a text or byteswritable write it directly otherwise write it to a buffer and then write the length and data to the stream 
 this is called by application and blocks the thread until authentication response is received 
 process all of the keys and values start up the application if we haven t started it yet 
 handle the end of the input by closing down the application 
 submit a job to the map reduce cluster all of the necessary modifications to the job to run under pipes are made to the configuration 
 submit a job to the map reduce cluster all of the necessary modifications to the job to run under pipes are made to the configuration 
 submit a job to the map reduce framework this returns a handle to the link runningjob which can be used to trackthe running job 
 submit a pipes job based on the command line arguments 
 method to load the resource file generates the root 
 param parent name of the parent queue
 factory method to create an appropriate instance of a queue configuration parser returns a parser that can parse either the deprecated property style queue configuration in mapred site xml or one that can parse hierarchical queues in mapred queues xml first preference is given to configuration in mapred site xml if no queue configuration is found there then a parser that can parse configuration in mapred queues xml is created 
 dumps the configuration of hierarchy of queues
 dumps the configuration of hierarchy of queues with the xml file path given it is to be used directly only for testing 
 method to perform depth first search and write the parameters of every queue in json format 
 create a record reader for the given split
 write this spill record to the location provided 
 the communication thread handles communication with the parent task tracker it sends progress updates if progress has been made or if the task needs to let the parent know that it s alive it also pings the parent to see if it s alive 
 sends last status update before sending umbilical done 
 start processing next unique key 
 read a log file from start to end positions the offsets may be negative in which case they are relative to the end of the file for example reader taskid kind   is the entire file and reader taskid kind   is the last  bytes 
 wrap a command in a shell to capture stdout and stderr to files setup commands such as setting memory limit can be passed which will be executed before exec if the taillength is  the entire output will be saved 
 construct the command line for running the task jvm
 write the object to the byte stream handling text as a special case 
 get all the jobs in cluster 
 clone a link jobcontext or link taskattemptcontext with a new configuration 
 genericcounter   keyname isdistinctdisplayname displayname value
 wraps a given fsdataoutputstream with a cryptooutputstream the size of the data buffer required for the stream is specified by the mapreduce job encrypted intermediate data buffer kb job configuration variable 
 wraps a given fsdataoutputstream with a cryptooutputstream the size of the data buffer required for the stream is specified by the mapreduce job encrypted intermediate data buffer kb job configuration variable 
 wraps a given inputstream with a cryptoinputstream the size of the data buffer required for the stream is specified by the mapreduce job encrypted intermediate data buffer kb job configuration variable if the value of length is gt  the inputstream is additionally wrapped in a limitinputstream cryptostreams are late buffering in nature this means they will always try to read ahead if they can the limitinputstream will ensure that the cryptostream does not read past the provided length from the given input stream 
 wraps a given fsdatainputstream with a cryptoinputstream the size of the data buffer required for the stream is specified by the mapreduce job encrypted intermediate data buffer kb job configuration variable 
 determines timestamps of files to be cached and stores those in the configuration determines the visibilities of the distributed cache files and archives the visibility of a cache path is public if the leaf component has read permissions for others and the parent subdirs have execute permissions for others this is an internal method 
 see clientdistributedcachemanager determinetimestampsandcachevisibilities configuration 
 determines timestamps of files to be cached and stores those in the configuration this is intended to be used internally by jobclient after all cache files have been added this is an internal method 
 determines the visibilities of the distributed cache files and archives the visibility of a cache path is public if the leaf component has read permissions for others and the parent subdirs have execute permissions for others
 gets the file status for the given uri if the uri is in the cache returns it otherwise fetches it and adds it to the cache 
 returns a boolean to denote whether a cache file is visible to all public or not
 returns true if all ancestors of the specified path have the execute permission set for all users i e that other users can traverse the directory heirarchy to the given path 
 checks for a given path whether the other permissions on it imply the permission in the passed fsaction
 add an archive path to the current set of classpath entries it adds the archive to cache as well intended to be used by user code 
 deprecated use link getinstance configuration string 
 creates a new link job with no particular link cluster a cluster will be created with a generic link configuration 
 creates a new link job with no particular link cluster and a given link configuration the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter a cluster will be created from the conf parameter only when it s needed 
 creates a new link job with no particular link cluster and a given jobname a cluster will be created from the conf parameter only when it s needed the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter 
 creates a new link job with no particular link cluster and given link configuration and link jobstatus a cluster will be created from the conf parameter only when it s needed the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter 
 creates a new link job with no particular link cluster a cluster will be created from the conf parameter only when it s needed the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter 
 creates a new link job with no particular link cluster and given link configuration a cluster will be created from the conf parameter only when it s needed the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter 
 creates a new link job with no particular link cluster and given link configuration and link jobstatus a cluster will be created from the conf parameter only when it s needed the job makes a copy of the configuration so that any necessary internal modifications do not reflect on the incoming parameter 
 some methods rely on having a recent job status object refresh it if necessary
 returns the current state of the job 
 get finish time of the job 
 get scheduling info of the job 
 dump stats to screen 
 get the information of the current state of the tasks of a job 
 get the progress of the job s map tasks as a float between   and   when all map tasks have completed the function returns   
 get the progress of the job s reduce tasks as a float between   and   when all reduce tasks have completed the function returns   
 get the progress of the job s cleanup tasks as a float between   and   when all cleanup tasks have completed the function returns   
 get the progress of the job s setup tasks as a float between   and   when all setup tasks have completed the function returns   
 check if the job is finished or not this is a non blocking call 
 check if the job completed successfully 
 kill the running job blocks until all job tasks have been killed as well if the job is no longer running it simply returns 
 set the priority of a running job 
 set the priority of a running job 
 get events indicating completion success failure of component tasks 
 get events indicating completion success failure of component tasks 
 kill indicated task attempt 
 kill indicated task attempt 
 fail indicated task attempt 
 gets the counters for this job may return null if the job has been retired and the job is no longer in the completed job store 
 gets the diagnostic messages for a given task attempt 
 set the number of reduce tasks for the job 
 set the current working directory for the default file system 
 set the link inputformat for the job 
 set the link outputformat for the job 
 set the link mapper for the job 
 set the combiner class for the job 
 set the link reducer for the job 
 set the link partitioner for the job 
 set the key class for the map output data this allows the user to specify the map output key class to be different than the final output value class 
 set the value class for the map output data this allows the user to specify the map output value class to be different than the final output value class 
 set the key class for the job output data 
 set the value class for job outputs 
 define the comparator that controls which keys are grouped together for a single call to combiner link reducer reduce object iterable org apache hadoop mapreduce reducer context 
 define the comparator that controls how the keys are sorted before they are passed to the link reducer 
 define the comparator that controls which keys are grouped together for a single call to link reducer reduce object iterable org apache hadoop mapreduce reducer context 
 set the user specified job name 
 add an file path to the current set of classpath entries it adds the file to cache as well files added with this method will not be unpacked while being added to the classpath to add archives to classpath use the link addarchivetoclasspath path method instead 
 add an archive path to the current set of classpath entries it adds the archive to cache as well archive files will be unpacked and added to the classpath when being distributed 
 default to the new apis unless they are explicitly set or the old mapper or reduce attributes are used 
 submit the job to the cluster and return immediately 
 submit the job to the cluster and wait for it to finish 
 monitor a job and print status in real time as progress is made and tasks fail 
 create a new event reader
 close the event reader
 print the job task attempt summary information to stdout 
 parse the entire history file and populate the jobinfo object the first invocation will populate the object subsequent calls will return the already parsed object the input stream is closed on return this api ignores partial records and stops parsing on encountering one link getparseexception can be used to fetch the exception if any 
 only used for unit tests 
 print out the job history to the given link printstream in amachine readable json format 
 construct a jobid object from given string 
 upload and configure files libjars jobjars and archives pertaining to the passed job this client will use the shared cache for libjars files archives and jobjars if it is enabled when shared cache is enabled it will try to use the shared cache and fall back to the default behavior when the scm isn t available  for the resources that have been successfully shared we will continue to use them in a shared fashion  for the resources that weren t in the cache and need to be uploaded by nm we won t ask nm to upload them 
 verify that the resources this job is going to localize are within the localization limits we count all resources towards these limits regardless of where they are coming from i e local distributed cache or shared cache 
 convert a string to a path and gracefully remove fragments queries if they exist in the string 
 recursively explore the given path and enforce the limits for resource localization this method assumes that there are no symlinks in the directory structure 
 checksum a local resource file and call use for that resource with the scm 
 takes input as a path string for file and verifies if it exist it defaults for file if the files specified do not have a scheme it returns the paths uri converted defaulting to file so an input of home user file would return file home user file
 initializes the staging directory and returns the path it also keeps track of all necessary ownership and permissions
 internal method for submitting jobs to the system the job submission process involves checking the input and output specifications of the job computing the link inputsplit s for the job setup the requisite accounting information for the link distributedcache of the job if necessary copying the job s jar and configuration to the map reduce system directory on the distributed file system submitting the job to the jobtracker and optionally monitoring it s status 
 create an instance of the given class
 create an aggregate based map reduce job 
 create and run an aggregate based map reduce job 
 advance to the next key value pair returning null if at end 
 writes a key value pair 
 close this recordwriter to future operations 
 inheritdoc 
 retrieve the value from the column in a type appropriate manner and return its timestamp since the epoch if the column is null then return long min value this will cause a special split to be generated for the null case but may also cause poorly balanced splits if most of the actual dates are positive time since the epoch etc 
 returns a connection object o the db 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 initializes the reduce part of the job with the appropriate output settings
 initializes the reduce part of the job with the appropriate output settings
 inheritdoc 
 deprecated use link nextkeyvalue 
 inheritdoc 
 set session time zone
 this method needs to determine the splits between two user provided strings in the case where the user s strings are a and z this is not hard we could create two splits from a m and m z  splits for strings beginning with each letter etc if a user has provided us with the strings ham and haze however we need to create splits that differ in the third letter the algorithm used is as follows since there are   unicode characters we interpret characters as digits in base  given a string s containing characters s  s  s n we interpret the string as the number  s  s  s  s n in base  having mapped the low and high strings into floating point values we then use the bigdecimalsplitter to establish the even split points then map the resulting floating point values back into strings 
 return all the splits in the specified set of paths
 a generic recordreader that can hand out different recordreaders for each chunk in the combinefilesplit 
 get the record reader for the next chunk in this combinefilesplit 
 copy constructor
 list input directories subclasses may override to e g select only files matching a regular expression 
 add files in the input path recursively into the results 
 generate the list of files and make them into filesplits 
 sets the given comma separated paths as the list of inputs for the map reduce job 
 add the given comma separated paths to the list of inputs for the map reduce job 
 get the progress within the split
 logically splits the set of input files for the job splits n lines of the input as one split 
 create a record reader for the given split
 filtering method if md key frequency   return true otherwise return false
 check and update the state of this job the state changes depending on its current state and the states of the depending jobs 
 the main loop for the thread the loop does the following check the states of the running jobs update the states of waiting jobs submit the jobs in ready state
 interpret a given string as a composite expression code func   func   tbl class   see java lang class forname java lang string path   see org apache hadoop fs path path java lang string reads expression from the mapreduce join expr property and user supplied join types from mapreduce join define lt ident gt types paths supplied to tbl are given as input paths to the inputformat class listed 
 build a compositeinputsplit from the child inputformats by assigning the ith split from each child to the ith composite split 
 construct a compositerecordreader for the children of this inputformat as defined in the init expression the outermost join need only be composable not necessarily a composite mandating tuplewritable isn t strictly correct 
 inheritdoc 
 write the next value into key value as accepted by the operation associated with this set of recordreaders 
 if key provided matches that of this composite give joincollector iterator over values it may emit 
 close all child rrs 
 inheritdoc 
 return the node type registered for the particular identifier by default this is a cnode for any composite node and a wnode for quot wrapped quot nodes user nodes will likely be composite nodes 
 let the first actual define the inputformat and the second define the mapred input dir property 
 parse a list of comma separated nodes 
 given an expression and an optional comparator build a tree of inputformats using the comparator to sort keys 
 writes each writable to out tuplewritable format code 
 inheritdoc 
 writes the bit set to the stream the first  bit positions of the bit set are written as a vlong for backwards compatibility with older versions of tuplewritable all bit positions    are encoded as a byte for every  bit positions 
 reads a bitset from the stream that has been written with link writebitset dataoutput int bitset 
 for a given recordreader rr occupy position id in collector 
 skip key value pairs with keys less than or equal to the key provided 
 add an iterator to the collector at the position occupied by this recordreader over the values in this stream paired with the key provided ie register a stream of values from this source matching k with a collector 
 read the next k v pair into the head of this object return true iff the rr and this are exhausted 
 run the application s maps using a thread pool 
 the job has completed so do works in commitjobinternal could retry on failure if using algorithm  
 the job has completed so do following commit job include move all committed tasks to the final output dir algorithm  only delete the temporary directory including all of the work directories create a success file to make it as successful 
 merge two paths together anything in from will be moved into to if there are any name conflicts while merging the files or directories in from win 
 delete the temporary directory including all of the work directories 
 move the files from the work directory to the job output directory
 delete the work directory
 did this task write any files in the work directory 
 helper function to generate a link path for a file that is unique forthe task within the job output directory the path can be used to create custom files from within the map and reduce tasks the path name will be unique for each task the path parent will be the job output directory ls this method uses the link getuniquefile method to make the file nameunique for the task 
 get the default path and filename for the output format 
 write key and value to the namedoutput output path is a unique file generated for the namedoutput for example namedoutput mr part number 
 write key and value to baseoutputpath using the namedoutput 
 write key value to an output file name gets the record writer from job s output format job s output format should be a fileoutputformat 
 create an output committer for a task attempt 
 write a partition file for the given job using the sampler provided queries the sampler for a sample keyset sorts by the output key comparator selects the keys for each rank and writes to the destination returned from link totalorderpartitioner getpartitionfile 
 driver for inputsampler from the command line configures a jobconf instance and calls link writepartitionfile 
 read in the partition file and build indexing data structures if the keytype is link org apache hadoop io binarycomparable andtotal order partitioner natural order is not false a trie of the first total order partitioner max trie depth    bytes will be built otherwise keys will be located using a binary search of the partition keyset using the link org apache hadoop io rawcomparator defined for this job the input file must be sorted with the same comparator and contain link job getnumreducetasks  keys 
 read the cut points from the given ifile 
 expert users can override this method for more complete control over the execution of the mapper 
 for committing job s output after successful job completion note that this is invoked for jobs with final runstate as successful this is called from the application master process for the entire job this is guaranteed to only be called once if it throws an exception the entire job will fail 	
 for aborting an unsuccessful job s output note that this is invoked for jobs with final runstate as link jobstatus state failed or link jobstatus state killed this is called from the application master process for the entire job this may be called multiple times 
 advanced application writers can use the link run org apache hadoop mapreduce reducer context method tocontrol how the reduce task works 
 aux util to calculate hash of a string
 verify that basehash is same as hmachash msg 
 convenience method to obtain delegation tokens from namenodes corresponding to the paths passed 
 load job token from a file
 the crux of the matter 
 create the map output url this will contain all the map ids separated by commas
 the connection establishment is attempted multiple times and is given up only on the last failure instead of connecting with a timeout of x we try connecting with a timeout of x  x but multiple times 
 the crux of the matter 
 wait until the shuffle finishes or until the timeout 
 start processing next unique key 
 construct a taskattemptid object from given string 
 construct a taskid object from given string 
 dump a list of currently running jobs
 dump a list of all jobs submitted 
 parse a pre   counters string into a counter object 
 kills the process or process group by sending the signal sigkill in the current thread
 test dbinputformat class class should split result for chunks
 writes the input test file
 test queuemanager configuration from file
 write cofiguration
 test taskattemptid
 this is to test the limit on bytes written the test is limited in that the check is done only once at the first loop of taskreport run 
 set the specific key to a string which is not a factory class expect a failure 
 inheritdoc 
 returns true if the history dirs were created false if they could not be created because the filesystem is not reachable or in safe mode and throws and exception otherwise 
 populates index data structures should only be called at initialization times 
 scans the specified path and populates the intermediate cache 
 scans old directories known by the idtodatestring map for the specified jobid if the number of directories is higher than the supported size of the idtodatestring cache the jobid will not be found 
 clean up older history files 
  versioning scheme major minor for e g           etc  any incompatible change of state store is a major upgrade and any compatible change of state store is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade state or remove incompatible old state 
 initialize the state storage
 start the state storage for use
 shutdown the state storage 
 test clean old history files files should be deleted after  week by default 
 trivial test case that verifies basic functionality of link jobidhistoryfileinfomap 
 this create a test case in which two threads are trying to load two different jobs of the same user under the intermediate directory one thread should not be blocked by the other thread that is loading a huge job files this is simulated by hanging up parsing the job files forever the test will fail by triggering the timeout if one thread is blocked by the other while the other thread is holding the lock on its associated job files and hanging up parsing the files 
 create a dummy jhist file under the intermediate directory for given job 
 gets the uri to access the given application on mapreduce history server
 constructs all the necessary information to start the mr am 
 run the test with default parameters 
 run the test with the specified parameters 
 run distributed checkup for the entire files system 
 run distributed checkup for the specified directory 
 map file name and offset into statistical data the map task is to get the key which contains the file name and the value which is the offset within the file the parameters are passed to the abstract method link doio reporter string long which performs the io operation usually read or write data and then link collectstats outputcollector string long object is called to prepare stat data for a subsequent reducer 
 update with non null fields of the task attempt log record 
 collect information about one job 
 run job history log analyser 
 result is combined from all reduce output files and is written to result file in the format column  
 main function called by tool runner it first initializes data by parsing the command line arguments it then calls the loadgenerator
 parse the command line arguments and initialize the data only parse the first arg mr must be first three args the rest are parsed by the parent loadgenerator
 main program
 merges the given command line parsed output with the given configuration object and returns the new configuration object with the correct options overwritten
 handles the specific task of merging operations from the command line or extractor object into the base configuration provided
 handles merging all options and verifying from the given command line output and the given base configuration and returns the merged configuration
 verifies a given number of bytes from a file less number of bytes may be read if a header can not be read in due to the byte limit
 verifies a given number of bytes from a file less number of bytes may be read if a header can not be read in due to the byte limit
 reads a header from the given input stream
 writes a set of bytes to the output stream for full segments it will write out the complete segment but for partial segments ie when the last position does not fill up a full long then a partial set will be written out containing the needed bytes from the expected full segment
 writes to a stream the given number of bytes specified
 writes a header to the given output stream
 merges according to the documented rules for merging only will merge if measurement type and operation type is the same 
 sleep for a random amount of time between a given positive range
 sleeps the current thread for x milliseconds
 runs the job given the provided config
 attempts to write the report to the given output using the specified config it will open up the expected reducer output file and read in its contents and then split up by operation output and sort by operation type and then for each operation type it will generate a report to the specified result file and the console 
 cleans up a file or directory recursively if need be 
 deletes a dir and its contents 
 parse input arguments
 analyze the results
 returns when the current number of seconds from the epoch equals the command line argument given by starttime this allows multiple instances of this program running on clock synchronized nodes to start at roughly the same time 
 reduce method
 this launches a given namenode operation operation starting at a given time starttime the files used by the openread rename and delete operations are the same files created by the createwrite operation typically the program would be run four times once for each operation in this order createwrite openread rename delete usage nnbench operation basedir starttime numfiles blocksperfile bytesperblock bytesperchecksum 
 process params from command line and run set of benchmarks specified 
 check that we can reach a namenode or resource manager using a specific socket factory
 creates hadoop cluster and dfs before a test case is run 
 starts the cluster within a testcase note that the cluster is already started when the testcase method is invoked this method is useful if as part of the testcase the cluster has to be shutdown and restarted again if the cluster is already running this method does nothing 
 destroys hadoop cluster and dfs after a test case is run 
 configure a job given argv 
 generates data that can be used for job control tests 
 this is a main function for testing jobcontrol class it first cleans all the dirs it will use then it generates some random text data in testjobcontroldata indir then it creates  jobs job  copy data from indir to outdir  job  copy data from indir to outdir  job  copy data from outdir  and outdir  to outdir  job  copy data from outdir to outdir  the jobs  and  have no dependency the job  depends on jobs  and  the job  depends on job  then it creates a jobcontrol object and add the  jobs to the jobcontrol object finally it creates a thread to run the jobcontrol object and monitors reports the job states 
 this is a main function for testing jobcontrol class it first cleans all the dirs it will use then it generates some random text data in testjobcontroldata indir then it creates  jobs job  copy data from indir to outdir  job  copy data from indir to outdir  job  copy data from outdir  and outdir  to outdir  job  copy data from outdir to outdir  the jobs  and  have no dependency the job  depends on jobs  and  the job  depends on job  then it creates a jobcontrol object and add the  jobs to the jobcontrol object finally it creates a thread to run the jobcontrol object and monitors reports the job states 
 tests compatibility with pre   versions of tuplewritable
 launches all the tasks in order 
 test pipesmaprunner test the transfer data from reader
 test org apache hadoop mapred pipes application test a internal functions messagetype register counter increment counter status progress 
 test org apache hadoop mapred pipes submitter
 test org apache hadoop mapred pipes pipesreducer test the transfer of data key and value
 run a map reduce word count that does all of the map input and reduce output directly rather than sending it back up to java 
 test pipesnonjavainputformat
 the main driver for sort validator program invoke this method to submit the map reduce job 
 test getsplits
 test using the gzip codec for reading
 test the default comparator for map reduce use the identity mapper and see if the keys are sorted at the end
 test user defined comparator for map reduce we provide our own comparator that is the reverse of the default int comparator keys should be sorted in reverse order in the reducer 
 test user defined grouping comparator for grouping values in reduce we generate composite keys that contain a random number which acts as a timestamp associated with the record in our reduce function values for a key should be sorted by the timestamp 
 test all user comparators super test of all tests here we generate composite keys that contain a random number which acts as a timestamp associated with the record in our reduce function values for a key should be sorted by the timestamp we also provide our own comparators that reverse the default sorting order this lets us make sure that the right comparators are used 
 test using hadoop s original native zlib gzip codec for reading 
 test using the raw inflater codec for reading gzip files 
 test using the new builtingzipdecompressor codec for reading gzip files 
 test using the bzip codec for reading
 extended bzip test similar to builtingzipdecompressor test above 
 parse the command line arguments into lines and display the result 
 launches all the tasks in order 
  random tests of various record file and split sizes all tests have uncompressed file as input 
  random tests of various record file and split sizes all tests have compressed file as input 
 test with no record length set 
 test with record length set to 
 test with record length set to a negative value
 test with partial record at the end of a compressed input file 
 test with partial record at the end of an uncompressed input file 
 test using the gzip codec with two input files 
 hadoop  this test verifies the javserialization impl can write to sequencefiles by virtue other sequencefileoutputformat is not coupled to writable types if so the job will fail 
 test default cleanup abort behavior
 test if a failed job with custom committer runs the abort code 
 test if a failed job with custom committer runs the deprecated link fileoutputcommitter cleanupjob jobcontext code for apicompatibility testing 
 tests link taskcounter s link taskcounter committed heap bytes the test consists of running a low memory job which consumes less heap memory and then running a high memory job which consumes more heap memory and then ensuring that committed heap bytes of low memory job is smaller than that of the high memory job 
 test using the gzip codec for reading
 writes the input test file
 test the case when a custom record delimiter is specified using the textinputformat record delimiter configuration property
 test the default behavior when the textinputformat record delimiter configuration property is not specified
 test the local job submission options of jt local libjars 
 test the local job submission options of jt local libjars 
 test local job submission with a file option 
 test local job submission with an archive option 
 validates map phase progress after each record is processed by map task using custom task reporter 
 public testmapred int range int counts configuration conf throws ioexception this range   range this counts   counts this conf   conf 
 
 test to test if the user set env variables reflect in the child processes mainly x y x can be a already existing env variable or a new variable 
 test to test if the user set old env variables reflect in the child processes mainly x y x can be a already existing env variable or a new variable 
 regression test for mapreduce  verifies that even if a map task makes lots of spills more than fit in the spill index cache that it will succeed 
 test that findcontainingjar works correctly even if the path has a  sign or spaces in it
 given a path with a jar make a classloader with that jar on the classpath and check that findcontainingjar can correctly identify the path of the jar 
 test will run with  maps all opportunistic 
 test will run with  maps  opportunistic and  guaranteed 
 verifies entity by reading the entity file written via fs impl 
 test method getlocations
 launches a mr job and tests the job counters against the expected values 
 test jobconf 
 test run from command line jobqueueclient
 test xml generation 
 verify that at least one segment does not hit disk 
 test readline for various kinds of line termination sequneces varies buffer size to stress test also check that returned value matches the string length 
 test readline for correct interpretation of maxlinelength returned string should be clipped at maxlinelength and the remaining bytes on the same line should be thrown out also check that returned value matches the string length varies buffer size to stress test 
 test using the gzip codec for reading
 test using the gzip codec and an empty input file
 parse the command line arguments into lines and display the result 
 test compressed file
 this is the main routine for launching the benchmark it generates random input data the input is non splittable sort is used for benchmarking this benchmark reports the effect of having multiple sort and spill cycles over a single sort and spill 
 the waiting function the map exits once it gets a signal here the signal is the file existence 
 the waiting function the reduce exits once it gets a signal here the signal is the file existence 
 signal the maps reduces to start 
 signal the maps reduces to start 
 this creates a file in the dfs
 the main driver for word count map reduce program invoke this method to submit the map reduce job 
 tests link distributedcache checkuris uri uri 
 configure a job given argv 
 the test suppresses unchecked warnings in link org mockito mockito reset although calling the method is a bad manner we call the method instead of splitting the test i e restarting minidfscluster to save time 
 the test suppresses unchecked warnings in link org mockito mockito reset although calling the method is a bad manner we call the method instead of splitting the test i e restarting minidfscluster to save time 
 test that cfif can handle missing blocks 
 test using the gzip codec for reading
  random tests of various record file and split sizes all tests have uncompressed file as input 
  random tests of various record file and split sizes all tests have compressed file as input 
 test with no record length set 
 test with record length set to 
 test with record length set to a negative value
 test with partial record at the end of a compressed input file 
 test with partial record at the end of an uncompressed input file 
 test using the gzip codec with two input files 
 test the case when a custom record delimiter is specified using the textinputformat record delimiter configuration property
 test the default behavior when the textinputformat record delimiter configuration property is not specified
 test using the gzip codec for reading
 test default cleanup abort behavior
 test if a failed job with custom committer runs the abort code 
 test if a failed job with custom committer runs the deprecated link fileoutputcommitter cleanupjob jobcontext code for api compatibility testing 
 generates random data consisting of  lines 
 reads tasklog and returns it as string after trimming it 
 main entry point 
 starts dfs and mr clusters as specified in member variable options also writes out configuration and details if requested 
 starts a minihadoopcluster 
 this is the main routine for launching a distributed random write job it runs  maps node and each node writes  gig of data to a dfs file the reduce doesn t do anything 
 attempts to access tokencache as from client
 map method with different behavior based on the thread id 
 create a single input file in the input directory 
 create the inputs for the multimaps test 
 verify that we got the correct amount of output 
 test that the gc counter actually increments when we know that we ve spent some time in the gc during the mapper 
 run a test with several mappers in parallel operating at different speeds verify that the correct amount of output is created 
 run a test with a misconfigured number of mappers expect failure 
 write out an input file containing an integer 
 tally up the values and ensure that we got as much data out as we put in each mapper generated number file val values  number file val  verify that across all our reducers we got exactly this much data back 
 run a test which creates a sequencemapper identityreducer job over a set of generated number files 
 launches all the tasks in order 
 main test method
 test fail task
 test a kill task
 test a kill job
 test submit task from file
 black list 
 print attemptids list 
 print tracker list
 print job history from file 
 download job config
 print job events list 
 print job status 
 print counters 
 print a job list 
 test list option displays job name the name is capped to  characters for display 
 configure a job given argv 
 move the path name on each volume to a temporary location and then delete them this functions returns when the moves are done but not necessarily all deletions are done this is usually good enough because applications won t see the path name under the old name anyway after the move 
 move all files directories inside volume into tobedeleted and then delete them the tobedeleted directory itself is ignored 
 move the path name to a temporary location and then delete it note that if there is no volume that contains this path the path will stay as it is and the function will return false this functions returns when the moves are done but not necessarily all deletions are done this is usually good enough because applications won t see the path name under the old name anyway after the move 
 this test creates some directories and then removes them through mrasyncdiskservice 
 used on windows to determine if the specified file is a symlink that targets a directory on most platforms these checks can be done using commons io on windows the commons io implementation is unreliable and always returns false instead this method checks the output of the dir command after migrating to java  this method can be removed in favor of the new method java nio file files issymboliclink which is expected to work cross platform 
 this is the main routine for launching a distributed random write job it runs  maps node and each node writes  gig of data to a dfs file the reduce doesn t do anything 
 test that unicode characters outside the basic multilingual plane such as this cat face are properly encoded 
 this method transfers data using local buffer it transfers data from a disk to a local buffer in memory and then it transfers data from the buffer to the target this is used only if transferto is disallowed in the configuration file super transferto does not perform well on windows due to a small io request generated customshuffletransfer can control the size of the io requests by changing the size of the intermediate buffer 
  versioning scheme major minor for e g           etc  any incompatible change of db schema is a major upgrade and any compatible change of db schema is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade shuffle info or remove incompatible old state 
 calls sendmapoutput for the mapid pointed by reducecontext mapstosend and increments it this method is first called by messagereceived maxsessionopenfiles times and then on the completion of every sendmapoutput operation this limits the number of open files on a node which can get really large exhausting file descriptors on the nm if all sendmapoutputs are called in one go as was done previous to this change 
 verify client prematurely closing a connection 
 simulate a reducer that sends an invalid shuffle header sometimes a wrong header name and sometimes a wrong version
 validate the limit on number of shuffle connections 
 validate the ownership of the map output files being pulled in the local file system owner of the file should match the user component in the
 check if the file is a symlink to the same directory 
 tool entry point 
 test whether we can filter a class path properly 
 test building a tarball from source jars 
 test upload to hdfs 
 prepare a mock directory tree to compress and upload 
 test native io 
 write output to files 
 inheritdoc 
 inheritdoc 
 run a map reduce job to compute pi 
 parse arguments and then runs a map reduce job 
 break the prefix string into moves a sequence of integer row ids that will be selected for each column in order find all solutions with that prefix 
 create the input file with all of the possible combinations of the given depth 
 set up a puzzle board to the given size boards may be asymmetric but the squares will always be divided to be more cells wide than they are tall for example a x puzzle will make sub squares that are x  cells wide  cells tall clearly that means the board is made up of x sub squares 
 solves a set of sudoku puzzles 
 populates the access table with generated records 
 verifies the results are correct 
 the main driver for sort program invoke this method to submit the map reduce job 
 inheritdoc 
 parse parameters 
 inheritdoc 
 choose a machine in runtime according to the cluster status 
 start a job to compute sigma 
 start the computation 
 covert a string to a summation 
 parse a file or a directory tree 
 parse a path 
 parse input and re write results 
 main 
 read summation from datainput 
 inheritdoc 
 write sigma to dataoutput 
 inheritdoc 
 read arithmeticprogression from datainput 
 write arithmeticprogression to dataoutput 
 inheritdoc 
 inheritdoc 
 execute the callables by a number of threads 
 create a writer of a local file 
 run a job 
 read job outputs 
 write results 
 parse arguments and then runs a map reduce job print output in standard out 
 read the two integers encoded as min value gt   gt min value max value gt 
 the main driver for sort program invoke this method to submit the map reduce job 
 param args the cli arguments
 use the input splits to take samples of the input and generate sample keys by default reads   keys from  locations in the input sorts them and picks n  keys to generate n equally sized partitions 
 solve the schedule and modify the filesplit array to reflect the new schedule it will move placed splits to front and unplacable splits to the end 
 read the cut points from the given sequence file 
 parse a hex string
 set the number from a hex string
 reads the output file and parses the summation of lengths and the word count to perform a quick calculation of the mean 
 this is a standard program to read and find a median value based on a file of word counts such as       where the first values are the word lengths and the following values are the number of times that words of that length appear 
 reads the output file and parses the summation of lengths the word count and the lengths squared to perform a quick calculation of the standard deviation 
 validate that source parameters look sane 
 write to the status file the status file will contain a string describing the exit status of the test it will be success if the test returned success return code  a numerical code if it returned a non zero status or in progress or timed out 
 runs the specified command and saves each line of the command s output to the given list and each line of the command s stderr to the other list 
 creates a new outputbufferthread to consume the given inputstream 
 reads and returns the full contents of the specified file 
 given a list of files computes and returns an md checksum of the full contents of all files 
 computes and returns an md checksum of the contents of all files in the input maven fileset 
 inheritdoc 
 delete an object see link delete path boolean 
 check whether the path is a valid path 
 delete a directory from aliyun oss 
 upload an empty file as an oss object using single upload 
 upload a file as an oss object using single upload 
 upload a file as an oss object using multipart upload 
 clean up all objects matching the prefix 
 skip the requested number of bytes or fail if there are no enough bytes left this allows for the possibility that link inputstream skip long may not skip as many bytes as requested most likely because of reaching eof 
 create credential provider specified by configuration or create default credential provider if not specified 
 this assumes that there are two types of files file dir
 this method writes all the valid top level directories into the srcwriter for indexing this method is a little tricky example for an input with parent path home user and sources as home user source dir home user source dir this will output dir means that source is a dir with dir and dir as children and and 
 archive the given source paths into the dest
 the main driver for creating the archives it takes at least three command line parameters the parent path the src and the dest it does an lsr on the source paths the mapper created archuves and the reducer creates the archive index 
 instantiate this calls link getcredentials to fail fast on the innerrole credential retrieval 
 create a committer this constructor binds the destination directory and configuration but does not update the work path that must be calculated by the implementation it is omitted here to avoid subclass methods being called too early 
 init the output filesystem and path testing only allows mock fs to cheat 
 get the destination fs creating it on demand if needed 
 get the destination as an sa filesystem casting it 
 if the job requires a success marker on a successful job create the file link commitconstants success while the classic committers create a  byte file the sguard committers put up a the contents of a link successdata file 
 base job setup deletes the success marker todo do we need this 
 internal job commit operation where the s requests are made potentially in parallel 
 the internal job abort operation can be overridden in tests this must clean up operations it is called when a commit fails as well as in an link abortjob jobcontext jobstatus state call the base implementation calls link cleanup jobcontext boolean 
 cleanup the job context including aborting anything pending 
 commit a single pending commit exceptions are caught and converted to an outcome 
 load all single pending commits in the directory all load failures are logged and then added to list of files which would not load 
 abort the multipart commit supplied this is the lower level operation which doesn t generate an outcome instead raising an exception 
 enumerate all pending files in a dir tree abort 
 upload all the data in the local file returning the information needed to commit the work 
 verify that the path is a magic one 
 get the sa fs of a path 
 load an instance from a file then validate it 
 deserialize via java serialization api deserialize the instance and then call link validate to verify that the deserializeddata is valid 
 load an instance from a file then validate it 
 deserialize via java serialization api deserialize the instance and then call link validate to verify that the deserializeddata is valid 
 build the destination path of the object 
 load an instance from a file then validate it 
 create a task committer 
 build a qualified temporary path for the multipart upload commit information in the cluster filesystem path is built by link getmultipartuploadcommitsdirectory filesystem configuration string 
 committer for a single task attempt 
 get the work path for a task 
 get the list of pending uploads for this job attempt 
 get the list of pending uploads for this job attempt swallowing exceptions 
 wait for all the futures to complete there s a small sleep between each iteration enough to yield the cpu 
 execute a function translating any exception into an ioexception 
 execute an operation with no result 
 execute an operation and ignore all raised ioexceptions log at info 
 execute a void operation with retry processing 
 execute a void operation with the default retry callback invoked 
 execute a function with the default retry callback invoked 
 execute a function with retry processing uses link once string string operation as the innerinvocation mechanism before retry logic is performed 
 execute a function with retry processing and no translation and the default retry callback 
 execute a function with retry processing aws sdk exceptions are not translated this is method which the others eventually invoke 
 execute an operation any exception raised is simply caught and logged at debug 
 execute an operation any exception raised is caught and logged at debug the result is only non empty if the operation succeeded
 create a filestatus iterator against a path with a given list object request 
 create a filestatus iterator against a path with a given list object request 
 inheritdoc 
 create an iterator over file status entries 
 report whether or not there is new data available if there is data in the local filtered list return true else request more data util that condition is met or there is no more remote listing data lastly return true if the code providedstatusiterator has left items 
 try to retrieve another batch note that for the initial batch link objectlistingiterator does not generate a request it simply returns the initial set 
 list outstanding multipart uploads package private safilesystem and tests are the users of this 
 get next listing first call this returns initial set possibly empty obtained from s subsequent calls my block on io or fail 
 an sa output stream which uploads partitions in a separate pool of threads different link sadatablocks blockfactory instances can control where data is buffered 
 the flush operation does not trigger an upload that awaits the next block being full what it does do is call code flush on the current block leaving it to choose how to react 
 writes a byte to the destination if this causes the buffer to reach its limit the actual upload is submitted to the threadpool 
 writes a range of bytes from to the memory buffer if this causes the buffer to reach its limit the actual upload is submitted to the threadpool and the remainder of the array is written to memory recursively 
 start an asynchronous upload of the current block 
 init multipart upload assumption this is called from a synchronized block note that this makes a blocking https request to the far end so can take time and potentially fail 
 close the stream this will not return until the upload is complete or the attempt to perform the upload has failed exceptions raised in this method are indicative that the write has failed and data is at risk of being lost 
 atomically enter a state verifying current state 
 write a series of bytes from the buffer from the offset returns the number of bytes written only valid in the state code writing base class verifies the state but does no writing 
 flush the output only valid in the state code writing in the base class this is a no op
 switch to the upload state and return a stream for uploading base class calls link enterstate deststate deststate tomanage the state machine 
 read in data 
 create a temp file and a link diskblock instance to manage it 
 flush operation will flush to disk 
 called after a new filesystem instance is constructed 
 get the region of a bucket 
 opens an fsdatainputstream at the indicated path 
 create an fsdataoutputstream at the indicated path with write progress reporting retry policy retrying translated on the getfilestatus probe no data is uploaded to s in this call so retry issues related to that 
 inheritdoc 
 renames path src to path dst can take place on local fs or remote dfs warning s does not support renames this method does a copy which can take s some time to execute with large files and directories since there is no progressable passed in this can time out jobs note this implementation differs with other s drivers specifically fails if src is a file and dst is a directory fails if src is a directory and dst is a file fails if the parent of dst does not exist or is a file fails if dst is a directory that is not empty 
 the inner rename operation see link rename path path forthe description of the operation this operation throws an exception on any failure which needs to be reported and downgraded to a failure retries retry translated assuming all operations it is called do so for safely consider catch and handle amazonclientexception because this is such a complex method there s a risk it could surface 
 low level call to get at the object metadata 
 entry point to an operation increments the statistic verifies the fs is active 
 delete an object this is the low level internal call which does not update the metastore increments the code object delete requests and writeoperation statistics this call does not create any mock parent entries retry policy retry untranslated delete considered idempotent 
 delete an object also updating the metastore this call does not create any mock parent entries retry policy retry untranslated delete considered idempotent 
 a helper method to delete a list of keys on a s backend retry policy retry untranslated delete considered idempotent 
 delete a path this operation is at least code o files with added overheads to enumerate the path it is also not atomic 
 delete an object see link delete path boolean this call does not create any fake parent directory that is left to the caller 
 create a fake directory if required that is it is not the root path and the path does not exist retry policy retrying untranslated 
 create a fake parent directory if required that is it parent is not the root path and does not yet exist 
 list the statuses of the files directories in the given path if the path is a directory 
 make the given path and all non existent parents into directories has the semantics of unix code mkdir p existence of the directory hierarchy is not an error 
 make the given path and all non existent parents into directories see link mkdirs path fspermission 
 return a file status object that represents the path 
 internal version of link getfilestatus path 
 raw code getfilestatus that talks direct to s used to implement link innergetfilestatus path boolean and for direct management of empty directory blobs retry policy retry translated 
 raw version of link filesystem exists path which uses s only sguard metadatastore if any will be skipped retry policy retrying translated 
 the src file is on the local disk add it to fs at the given dst name this version doesn t need to create a temporary file to calculate the md sadly this doesn t seem to be used by the shell cp delsrc indicates if the source should be removed
 the src file is on the local disk add it to fs at the given dst name this version doesn t need to create a temporary file to calculate the md sadly this doesn t seem to be used by the shell cp delsrc indicates if the source should be removed
 execute a put via the transfer manager blocking for completion updating the metastore afterwards if the waiting for completion is interrupted the upload will be aborted before an code interruptedioexception is thrown 
 delete mock parent directories which are no longer needed retry policy retrying exceptions swallowed 
 create a fake directory always ending in retry policy retrying translated 
 increments the statistic link statistic invocation glob status inheritdoc 
 override superclass so as to add statistic collection inheritdoc 
 override superclass so as to add statistic collection inheritdoc 
 override superclass so as to add statistic collection inheritdoc 
 override superclass so as to add statistic collection inheritdoc 
 inheritdoc this implementation is optimized for s which can do a bulk listing off all entries under a path in one single operation thus there is no need to recursively walk the directory tree instead a link listobjectsrequest is created requesting a windowed listing of all entries under the given path this is used to construct an code objectlistingiterator instance iteratively returning thesequence of lists of elements under the path this is then iterated over in a code filestatuslistingiterator which generates link safilestatus instances one per listing entry these are then translated into link locatedfilestatus instances this is essentially a nested and wrapped set of iterators with some generator classes an architecture which may become less convoluted using lambda expressions 
 override superclass so as to add statistic collection inheritdoc 
 inheritdoc s optimized directory listing the initial operation performs the first bulk listing extra listings will take place when all the current set of results are used up 
 seek without raising any exception this is for use in code finally clauses
 adjust the stream to a specific position 
 perform lazy seek and adjust stream to correct position for reading 
 handle an ioe on a read by attempting to re open the stream the filesystem s readexception count will be incremented 
 inheritdoc this updates the statistics on read operations started and whether or not the read operation completed that is returned the exact number of bytes requested 
 forcibly reset the stream by aborting the connection the next code read operation will trigger the opening of a new httpsconnection this is potentially very inefficient and should only be invoked in extreme circumstances it logs at info for this reason 
 subclass code readfully operation which only seeks at the startof the series of operations seeking back at the end this is significantly higher performance if multiple read attempts are needed to fetch the data as it does not break the http connection to maintain thread safety requirements this operation is synchronized for the duration of the sequence inheritdoc 
 create the aws credentials from the providers the uri and the key link constants aws credentials provider in the configuration 
 create an aws credential provider from its class by using reflection the class must implement one of the following means of construction which are attempted in order a public constructor accepting java net uri and org apache hadoop conf configuration a public static method named getinstance that accepts no arguments and returns an instance of com amazonaws auth awscredentialsprovider or a public default constructor 
 return the access key and secret for s api use credentials may exist in configuration within credential providers or indicated in the userinfo of the name uri param 
 get a password from a configuration including jceks files handling both the absolute key and bucket override 
 get a password from a configuration or if a value is passed in pick that up instead 
 get a password from a configuration or if a value is passed in pick that up instead 
 returns the public constructor of code cl specified by the list of code args or code null if code cl has no public constructor thatmatches that specification 
 returns the public static method of code cl that accepts no argumentsand returns code returntype specified by code methodname or code null if code cl has no public static method that matches thatspecification 
 apply an operation to every link locatedfilestatus in a remoteiterator 
 map an operation to every link locatedfilestatus in a remoteiterator returning a list of the results 
 map an operation to every link locatedfilestatus in a remoteiterator returning a list of the all results which were not empty 
 list located files and filter them as a classic listfiles path filter would do 
 get any sse key from a configuration credential provider this operation handles the case where the option has been set in the provider or configuration to the option code old sa server side encryption key ioexceptions raised during retrieval are swallowed 
 get the server side encryption algorithm this includes validation of the configuration checking the state of the encryption key given the chosen algorithm 
 close the closeable objects and ignore any exception or null pointers this is the slfj equivalent of that in code ioutils 
 for paths that are handed in directly we assert they are in consistent format with checkpath for paths that are supplied embedded in filestatus we attempt to fill in missing scheme and host when this dirlistingmetadata is associated with one 
 performs one time initialization of the metadata store via configuration this initialization depends on the configuration object to get aws credentials dynamodbfactory implementation class dynamodb endpoints dynamodb table names etc after initialization this metadata store does not explicitly relate to any s bucket which be nonexistent this is used to operate the metadata store directly beyond the scope of the safilesystem integration e g command line tools generally callers should use link initialize filesystem with an initialized code safilesystem instance without a filesystem to act as a reference point the configuration itself must declare the table name and region in the link constants sguard ddb table name key and link constants sguard ddb region key respectively 
 helper method to issue a batch write request to dynamodb the retry logic here is limited to repeating the write operations until all items have been written there is no other attempt at recovery retry throttling is handled internally 
 put the current thread to sleep to implement exponential backoff depending on retrycount if max retries are exceeded throws an exception instead 
 create a table if it does not exist and wait for it to become active if a table with the intended name already exists then it uses that table otherwise it will automatically create the table if the config link org apache hadoop fs sa constants sguard ddb table create key isenabled the dynamodb table creation api is asynchronous this method wait for the table to become active after sending the creation request so overall this method is synchronous and the table is guaranteed to exist after this method returns successfully 
 get the version mark item in the existing dynamodb table as the version marker item may be created by another concurrent thread or process we sleep and retry a limited times before we fail to get it this does not include handling any failure other than item not found so this method is tagged as onceraw 
 create a table wait for it to become active then add the version marker 
 parse dynamodb region from either m option or a s path this function should only be called from link init or link destroy 
 recursively import every path under path 
 compare the metadata of the directory with the same path on s and the metadata store respectively if one of them is null consider the metadata of the directory and all its subdirectories are missing from the source pass the filestatus obtained from s and metadata store to avoid one round trip to fetch the same metadata twice because the filestatus hve already been obtained from liststatus listchildren operations 
 compare both metadata store and s on the same path 
 convert a path to a uri catching any code urisyntaxexception and converting to an invalid args exception 
 main entry point calls code system exit on all execution paths 
 this completes a multipart upload to the destination key via code finalizemultipartupload retry policy retrying translated retries increment the code errorcount counter 
 abort multipart uploads under a path limited to the first few hundred 
 abort a multipart commit operation 
 extract the login details from a uri 
 canonicalize the given uri this strips out login information 
 write a file read it back validate the dataset overwrites the file if it is present
 execute a sequence of rename operations 
 execute a sequence of rename operations where the source data is read only to the client calling rename this will cause the inner delete operations to fail whose outcomes are explored multiple files are created in parallel for some renames so exploring the outcome on bulk delete calls including verifying that a multiobjectdeleteexception is translated to an accessdeniedexception the exception raised is accessdeniedexception from single and multi delete calls it happens after the copy not ideal but well we can t pretend it s a filesystem forever 
 write some csv data to a local file 
 have a directory with full r w permissions but then remove write access underneath and try to delete it 
 wrap a set of statements with a policy and bind the configuration s code assumed role policy option to it 
 clean up a directory waits for consistency if needed
 teardown waits for the consistency delay and resets failure count so fs is stable before the superclass teardown is called this should clean things up better 
 wait a multiple of the inconsistency delay for things to stabilize no op if the consistent client is used 
 assert that there are pending mpus 
 count the number of mpus under a path 
 verify that the specified dir has the code success markerand that it can be loaded the contents will be logged and returned 
 create a random job id using the fork id as part of the number 
 create a committer for a task 
 write some text out 
 create a new job sets the task attempt id and output dir asks for a success marker 
 start a job with a committer optionally write the test data always register the job to be aborted quietly in teardown this is from an oo purity perspective the wrong kind of method to do it s setting things up mixing functionality registering for teardown its aim is simple though a common body of code for starting work in test cases 
 start a job with a committer optionally write the test data always register the job to be aborted quietly in teardown this is from an oo purity perspective the wrong kind of method to do it s setting things up mixing functionality registering for teardown its aim is simple though a common body of code for starting work in test cases 
 execute work as part of a test after creating the job after the execution link abortjobquietly jobdata iscalled for abort cleanup 
 execute work as part of a test against the created job after the execution link abortjobquietly jobdata iscalled for abort cleanup 
 verify that recovery doesn t work for these committers 
 verify the output of the directory that includes the code part m  file existence and contents as well as optionally the success marker 
 look for the partfile subdir of the output dir 
 full test of the expected lifecycle start job task write commit task commit job 
 simulate a failure on the first job commit expect the second to succeed 
 override point the failure expected on the attempt to commit a failed job 
 simulate a failure on the first job commit expect the second to succeed 
 this looks at what happens with concurrent commits however the failure condition it looks for subdir under subdir is the kind of failure you see on a rename based commit what it will not detect is the fact that both tasks will each commit to the destination directory that is whichever commits last wins there s no way to stop this instead it is a requirement that the task commit operation is only executed when the committer is happy to commit only those tasks which it knows have succeeded and abort those which have not 
 commit the file with before and after checks on the dest and magic values failures can be set they ll be reset after the commit 
 commit to a write to code magicfile which is expected tobe saved to code destfile failures can be set they ll be reset after the commit 
 get a method relative path 
 create files in the attempt path that should be found by code gettaskoutput 
 this originally verified that empty files weren t put they are now 
 run tasks return the uploaded dataset the upload data is extracted from the link results field this is resetbefore the operation 
 basic test of running one trivial task 
 more involved test including detecting blocking when at capacity 
 create a temp file with some text 
 set up some invalid credentials verify login is rejected 
 it is possible to list the contents of a directory up to the actual end of the nested directories this is due to how sa mocks the directories and how prefixes work in s 
 much like the above list encrypted directory test you cannot get the metadata of an object without the correct encryption key 
 much like trying to access a encrypted directory an encrypted file cannot have its metadata read since both are technically an object 
 it is possible to delete directories without the proper encryption key and the hierarchy above it 
 main test case for upload part listing and iterator paging 
 helper function for other test cases does a single rename operation and validates the aftermath 
 tests that after renaming a directory the original directory and its contents are indeed missing and the corresponding new paths are visible 
 tests a circular sequence of renames to verify that overwriting recently deleted files and reading recently created files from rename operations works as expected 
 tests that deleted files immediately stop manifesting in list operations even when the effect in s is delayed 
 tests that rename immediately after files in the source directory are deleted results in exactly the correct set of destination files and none of the source files 
 similar to link testconsistentliststatusafterput this tests that the fs listlocatedstatus call will return consistent list 
 tests that the safs listfiles call will return consistent file list 
 in liststatus when sguard is enabled the full listing for a directory is written back to the metadatastore before the listing is returned currently this write back behavior occurs when fs sa metadatastore authoritative is true this test validates this behavior 
 create the test filesystem if the test fs sa name property is not set this will trigger a junit failure multipart purging is enabled 
 test assumption that sguard is is not enabled 
 run a sguardtool command from a varags list catch any raised exitexception and verify the status code matches that expected 
 create file either on s or in metadata store 
 run uploads cli command and assert the reported count listed or deleted matches 
 helper function for verifying descendantsiterator and metadatastorelistfilesiterator behavior 
 test that we can get the whole sub tree by iterating descendantsiterator the tree is similar to or same as the example in code comment 
 test that we can get the correct subset of the tree with metadatastorelistfilesiterator the tree is similar to or same as the example in code comment 
 test that the metadatastore differentiates between the same path in two different buckets 
 modifies paths input array and returns it 
 start the in memory dynamodblocal server and initializes s file system 
 this tests that after initialize using an safilesystem object the instance should have been initialized successfully and tables are active 
 this tests that after initialize using a configuration object the instance should have been initialized successfully and tables are active 
 test that for a large batch write request the limit is handled correctly 
 delete the version marker and verify that table init fails 
 set the version value to a different number and verify that table init fails 
 test that inittable fails with ioexception when table does not exist and table auto creation is disabled 
 test cases about root directory as it is not in the dynamodb table 
 test that when moving nested paths all its ancestors up to destination root will also be created here is the directory tree before move testmovepopulateancestors    a 牋    b 牋    src 牋    dir 牋 牋    dir 牋    file txt    c    d    dest as part of rename a b src d c dest sa will enumerate the subtree at a b src this test verifies that after the move the new subtree at dest is reachable from the root i e c and c d exist in the table dynamodbmetadatastore depends on this property to do recursive delete without a full table scan 
 recursively create a directory tree 
 assume that the huge file exists skip the test if it does not 
 cleanup delete the files 
 attempts to trigger a deadlock that would happen if any bounded resource pool became saturated with control tasks that depended on other tasks that now can t enter the resource pool to get completed 
 test rate at which we can create deeply nested files from a single thread 
 caution if this test starts failing please make sure that the link org apache hadoop fs sa constants max threads configuration is notset too low alternatively consider reducing the scale test operation count parameter in getoperationcount 
 open the fs and the test data the input stream is always set up here 
 open the test file with the read buffer specified in the setting link key read buffer size use the code normal policy
 open the test file with the read buffer specified in the setting link key read buffer size this includes the link requirecsvtestdata assumption soif called before any fs op will automatically skip the test if the csv file is absent 
 execute a decompression  line read with the given input policy 
 execute a seekread sequence 
 execute the random io code readfully pos bytes sequence defined by link random io sequence the stream is closed afterwards that s used in the timing too
 interrupted ioes are not retryable 
 test that npes aren t retried also verify that the catch counts are incremented while the retry count isn t 
 very that the old key is picked up via the properties 
 create a configuration with no defaults and bonded to a file provider so that link setprovideroption configuration string string can be used to set a secret 
 helper method 
 construct a uri raises an rte if it won t parse this allows it to be used in static constructors 
 execute the file operation parallel using threads all threads works on a single working set of files stored in input contents the synchronization between multiple threads is achieved through retrieving atomic index value from the array once thread gets the index it retrieves the file and initiates the file operation the advantage with this method is that file operations doesn t get serialized due to any thread also the input copy is not changed such that caller can reuse the list for other purposes this implementation also considers that failure of operation on single file is considered as overall operation failure all threads bail out their execution as soon as they detect any single thread either got exception or operation is failed 
 method for the uri and configuration object necessary to create a storage session with an azure session it parses the scheme to ensure it matches the storage protocol supported by this file system 
 connect to azure storage using anonymous credentials 
 connect to azure storage using account key credentials 
 connect to azure storage using shared access signature credentials 
 establish a session with azure blob storage based on the target uri the method determines whether or not the uri target contains an explicit account or an implicit default cluster wide account 
 checks if the given rawdir belongs to this account container and if so returns the canonicalized path for it otherwise return null 
 take a comma separated list of directories from a configuration variable and transform it to a set of directories 
 stores an empty blob that s linking to the temporary file where re we re uploading the initial data 
 if the blob with the given key exists and has a link in its metadata to a temporary file see storeemptylinkfile this method returns the key to that temporary file otherwise returns null 
 build up a metadata list of blobs in an azure blob directory this method uses a in order first traversal of blob directory structures to maintain the sorted order of the blob names 
 api implementation to delete a blob in the back end azure storage 
 api implementation to delete a blob in the back end azure storage 
 checks whether an explicit file folder exists this is used by redo of atomic rename there was a bug apache jira hadoop  during atomic rename if process crashes after an inner directory has been renamed but still there are file under that directory to be renamed then after the process comes again it tries to redo the renames it checks whether the directory exists or not by calling filesystem exist but filesystem exists will treat that directory as implicit directory and return true as file exists under that directory so it will try try to rename that directory and will fail as the corresponding blob does not exist so this method explicitly checks for the blob 
 changes the permission status on the given key 
 get a lease on the blob identified by key this lease will be renewed indefinitely by a background thread 
 writes the specified byte to this output stream the general contract for write is that one byte is written to the output stream the byte to be written is the eight low order bits of the argument b the  high order bits of b are ignored 
 writes length bytes from the specified byte array starting at offset to this output stream 
 flushes this output stream and forces any buffered output bytes to be written out if any data remains in the payload it is committed to the service data is queued for writing and forced out to the service before the call returns 
 force all data in the output stream to be written to azure storage wait to return until this is complete 
 force all data in the output stream to be written to azure storage wait to return until this is complete 
 force all data in the output stream to be written to azure storage wait to return until this is complete close the access to the stream and shutdown the upload thread pool if the blob was created its lease will be released any error encountered caught in threads and stored will be rethrown here after cleanup 
 this is shared between upload block runnable and commitblocklist the method captures retry logic
 write block list the method captures retry logic
 execute command 
 execute command 
 block compaction process block compaction is only enabled when the number of blocks exceeds activatecompactionblockcount the algorithm searches for the longest segment b e where e b   b  b e   maxblocksize such that size b  size b   size bn  maximum block size it then downloads the blocks in the sequence concatenates the data to form a single block uploads this new block and updates the block list to replace the sequence of blocks with the new block 
 prepare block upload command and queue the command in thread pool executor 
 prepare block list commit command and queue the command in thread pool executor 
 gets the read position of the stream 
 sets the read position of the stream 
 gets the number of bytes that can be read or skipped over without performing a network operation 
 closes this stream and releases any system resources associated with it 
 reads up to len bytes of data from the input stream into an array of bytes 
 reads the next byte of data from the stream 
 skips over and discards n bytes of data from this input stream 
 suspends the current storage operation as necessary to reduce throughput 
 implementation to generate sas key for a container
 helper method that creates a cloudstorageaccount instance based on sas key for accountname
 implementation for generation of relative path blob sas uri 
 write to disk the information needed to redo folder rename in json format the file name will be code wasb foldername renamepending json the file format will be code formatversion   operationtime oldfoldername newfoldername filelist here s a sample formatversion   operationutctime        oldfoldername user ehans foldertorename newfoldername user ehans renamedfolder filelist innerfile innerfile 
 execute a folder rename this is the execution path followed when everything is working normally see redo for the alternate execution path for the case where we re recovering from a folder rename failure 
 clean up after execution of rename 
 recover from a folder rename failure by redoing the intended work as recorded in the renamepending json file 
 return the size of the remaining available bytes if the size is less than or equal to link integer max value otherwise return link integer max value this is to match the behavior of dfsinputstream available which some clients may rely on hbase write ahead log reading in particular 
 puts in the authority of the default file system if it is a wasb file system and the given uri s authority is null 
 param requestingaccessforpath the path to the ancestor parent subtree file that needs to bechecked before granting access to originalpath
 this optional operation is not yet supported 
 create an azure blob and return an output stream to use to write data to it 
 this is the version of the create call that is meant for internal usage this version is not public facing and does not perform authorization checks it is used by the public facing create call and by folderrenamepending to create the internal renamepending json file 
 delete file or folder with authorization checks most of the code is duplicate of the actual delete implementation and will be merged once the performance and funcional aspects are guaranteed not to regress existing delete semantics 
 delete the specified file or folder the parameter skipparentfolderlastmodifiedtimeupdate is used in the case of atomic folder rename redo in that case there is a lease on the parent folder so without reworking the code modifying the parent folder update time will fail because of a conflict with the lease since we are going to delete the folder soon anyway so accurate modified time is not necessary it s easier to just skip the modified time update 
 gets list of contents that can be deleted based on authorization check calls performed on the sub tree for the foldertodelete 
 checks if a given path exists in the filesystem calls getfilestatusinternal and has the same costs as the public facing exists call this internal version of the exists call does not perform authorization checks and is used internally by various filesystem operations that need to check if the parent ancestor path exist the idea is to avoid having to configure authorization policies for these internal calls 
 inner implementation of link getfilestatus path return a file status object that represents the path 
 retrieve the status of a given path if it is a file or of all the contained files if it is a directory 
 update the last modified time of the parent folder of the file identified by key 
 if the source is a page blob folder prepare to rename this folder atomically this means to get exclusive access to the source folder and record the actions to be performed for this rename in a rename pending file this code was designed to meet the needs of hbase which requires atomic rename of write ahead log wal folders for correctness before calling this method the caller must ensure that the source is a folder for non page blob directories prepare the in memory information needed but don t take the lease or write the redo file this is done to limit the scope of atomic folder rename to hbase at least at the time of writing this code 
 performs sticky bit check on source folder for rename operation 
 implements recover and delete move and delete behaviors for handling dangling files blobs whose upload was interrupted 
 looks under the given root path for any blob that are left dangling meaning that they are place holder blobs that we created while we upload the data to a temporary blob but for some reason we crashed in the middle of the upload and left them there if any are found we move them to the destination given 
 looks under the given root path for any blob that are left dangling meaning that they are place holder blobs that we created while we upload the data to a temporary blob but for some reason we crashed in the middle of the upload and left them there if any are found we delete them 
 helper method to extract the actual data size of a page blob this typically involves  service requests one for page ranges another for the last page s data 
 return the size of the remaining available bytes if the size is less than or equal to link integer max value otherwise return link integer max value this is to match the behavior of dfsinputstream available which some clients may rely on hbase write ahead log reading in particular 
 check our buffer and download more from the server if needed if data is not available in the buffer method downloads maximum page blob download size mb or if there is less then mb left all remaining pages if we are on the last page method will return true even if we reached the end of stream 
 skips over and discards n bytes of data from this input stream 
 skip over n bytes within the current buffer or just over skip the whole buffer if n is greater than the bytes remaining in the buffer 
 skip over n bytes within the current page or just over skip the whole page if n is greater than the bytes remaining in the page 
 gets the number of bytes remaining within the current page in the buffer 
 closes this output stream and releases any system resources associated with this stream if any data remains in the buffer it is committed to the service 
 extend the page blob file if we are close to the end 
 flushes this output stream and forces any buffered output bytes to be written out if any data remains in the buffer it is committed to the service data is queued for writing but not forced out to the service before the call returns 
 writes b length bytes from the specified byte array to this output stream 
 writes length bytes from the specified byte array starting at offset to this output stream 
 writes the specified byte to this output stream the general contract for write is that one byte is written to the output stream the byte to be written is the eight low order bits of the argument b the  high order bits of b are ignored 
 writes the data to the buffer and triggers writes to the service as needed 
 force all data in the output stream to be written to azure storage wait to return until this is complete 
 renew the delegation token 
 cancel the delegation token 
 start a keep alive thread that will continue to renew the lease until it is freed or the process dies 
 recursively check if a given path and its child paths have colons in their names it returns true if none of them has a colon or this path does not exist and false otherwise 
 helper method to make remote http get request 
 return a path bonded to this method name unique to this fork during parallel execution 
 return a blob path bonded to this method name unique to this fork during parallel execution 
 acquires a short lease on the given blob in this test account 
 releases the lease on the container 
 creates a test account that goes against the storage emulator 
 implementation of the runnable interface the run method is a tight loop which repeatedly updates the blob with a  mb block 
 test that by default we don t store the blob level md 
 test that we get blob level md storage and validation if we specify that in the configuration 
 test that by default we check block level md 
 test that we don t check block level md if we specify that in the configuration 
 writes data to the given stream of the given size flushing every x bytes 
 writes data to the given file of the given size flushing every x bytes measure performance of that and return it 
 writes data to a block blob of the given size flushing every x bytes measure performance of that and return it 
 writes data to a page blob of the given size flushing every x bytes measure performance of that and return it 
 runs the benchmark over a small  kb file flushing every  bytes 
 runs the benchmark for the given file size and flush frequency 
 runs the benchmark for the given file size and flush frequency from the command line 
 validates the implementation of inputstream marksupported 
 validates the implementation of inputstream marksupported 
 validates the implementation of inputstream mark and reset for version  of the block blob input stream 
 validates the implementation of inputstream mark and reset for version  of the block blob input stream 
 validates the implementation of seekable seektonewsource which should return false for version  of the block blob input stream 
 validates the implementation of seekable seektonewsource which should return false for version  of the block blob input stream 
 validates the implementation of inputstream skip and ensures there is no network io for version  of the block blob input stream 
 validates the implementation of inputstream skip and ensures there is no network io for version  of the block blob input stream 
 validates the implementation of seekable seek and ensures there is no network io for forward seek 
 validates the implementation of seekable seek and ensures there is no network io for forward seek 
 validates the implementation of seekable seek seekable getpos and inputstream available 
 validates the implementation of seekable seek seekable getpos and inputstream available 
 validates the implementation of inputstream skip seekable getpos and inputstream available 
 validates the implementation of inputstream skip seekable getpos and inputstream available 
 ensures parity in the performance of sequential read for version  and version  of the block blob input stream 
 ensures parity in the performance of sequential read after reverse seek for version  of the block blob input stream 
 tests a basic single threaded read scenario for page blobs 
 tests a basic single threaded seek scenario for page blobs 
 test a basic single thread seek scenario for block blobs 
 tests a basic single threaded read scenario for block blobs 
 tests basic single threaded setpermission scenario 
 tests basic single threaded setpermission scenario 
 test basic single threaded liststatus scenario 
 test basic single threaded liststatus scenario 
 test to validate correct exception is thrown for multithreaded read scenario for block blobs 
 test to validate correct exception is thrown for multithreaded seek scenario for block blobs 
 tests basic multi threaded setpermission scenario 
 tests basic multi threaded setpermission scenario 
 tests basic multi threaded setowner scenario 
 tests basic multi threaded liststatus scenario 
 test to validate correct exception is thrown for multithreaded read scenario for page blobs 
 test to validate correct exception is thrown for multithreaded seek scenario for page blobs 
 validate contract for filesystem create when overwrite is true and there are concurrent callers of filesystem delete an existing file should be overwritten even if the original destination exists but is deleted by an external agent during the create operation 
 validate contract for filesystem delete when invoked concurrently one of the threads should successfully delete the file and return true all other threads should return false 
 tests the rename file operation to ensure that when there are multiple attempts to rename a file to the same destination only one rename operation is successful hadoop  
 tests fs delete function to delete a blob when another blob is holding a lease on it delete if called without a lease should fail if another process is holding a lease and throw appropriate exception this is a scenario that would happen in hmaster startup when it tries to clean up the temp dirs while the hmaster process which was killed earlier held lease on the blob when doing some ddl operation
 for a set of different file sizes write some random data to a page blob read it back and compare that what was read is the same as what was written 
 write data to a page blob open it seek and then read a range of data then compare that the data read from that range is the same as the data originally written 
 test malformed json response 
 test valid json response failure response code 
 tests that we can connect to fully qualified accounts outside of blob core windows net
 creates a file and writes a single byte with the given value in it 
 reads the file given and makes sure that it s a single byte file with the given value in it 
 tests the cases when the uri is specified with no authority i e wasb path to file 
 tests the cases when the scheme specified is wasbs 
 tests the basic functionality of the class 
 if there is a folder to be renamed inside a parent folder then when you list the parent folder you should only see the final result after the rename 
 there is a nested folder and file under the folder to be renamed and the process crashes after the nested folder has been renamed but not the file then when you list the parent folder pending renames should be redone apache jira hadoop 
 test the situation where a rename pending file exists but the rename is really done this could happen if the rename process died just before deleting the rename pending file it exercises a non standard code path in redo 
 tests that wasb stamped the version in the container metadata if it does a write operation to a pre existing container 
 tests that wasb works well with an older version container with asv era version and metadata 
 tests that wasb understands the old style asv metadata and changes it when it gets the chance 
 setup up permissions to allow a recursive delete for cleanup purposes 
 recursive delete for teardown finally operations setting the permissions to do the delete before invoking filesystem delete exceptions are caught and logged at error 
 test to check rename fails when sticky bit is set on parent of source directory and the user is not owner of parent or the source directory 
 test to check rename returns false when sticky bit is set on parent of source parent directory and the source does not exist
 negative test to verify file delete fails when sticky bit is set on parent and non owner user performs delete
 test to verify delete fails for child files and folders when non owner user performs delete and stickybit is set on parent
 test delete scenario where sticky bit check leaves files folders not owned by a specific user intact and the files owned by him her are deleted
 test delete scenario where sticky bit is set and the owner of parent directory can delete child files folders which he does not own this is according to the sticky bit behaviour specified in hdfs permission guide which is as follows the sticky bit can be set on directories preventing anyone except the superuser directory owner or file owner from deleting or moving the files within the directory
 negative test case for owner does not match current user
 negative test for setowner when authorization is enabled 
 test for setowner when authorization is enabled and the user is specified in chown allowed user list 
 test for setowner when authorization is enabled and the userlist is specified as 
 test for setowner throws for illegal setup of chown allowed testsetownersucceedsforauthorisedusers 
 test to ensure that the internal renamepending mechanism does not make authorization calls 
 this method expects a failure while invoking setpermission call and validates whether the failure is as expected
 this method expects a success while invoking setpermission call and validates whether the new permissions are set
 tests running starting multiple threads all doing various file system operations against the same fs 
 tests that we upload consistently if we flush after every little bit of data 
 tests that we upload consistently if we flush after every medium sized bit of data 
 tests that we upload consistently if we flush after every large chunk of data 
 makes sure the data in the given input is what i d expect 
 checks that the data in the given file is what i d expect 
 checks that the data in the current temporary upload blob is what i d expect 
 tests the given scenario for uploading a file while flushing periodically and making sure the data is always consistent with what i d expect 
 tests that we recover files properly
 tests that we delete dangling files properly
 called after a new filesystem instance is constructed 
 gets the acl of a file or directory 
 param args
 this is the function that re groups values for a key into sub groups based on a secondary key input tag 
 join the list of the value lists and collect the results 
 perform the actual join recursively 
 build listing function creates the input listing that distcp uses to perform the copy the build listing is a sequence file that has relative path of a file in the key and the file status information of the source file in the value for instance if the source path is tmp data and the traversed path is tmp data dir dir file then the sequence file would contain key dir dir file and value filestatus tmp data dir dir file file would also contain directory entries meaning if tmp data dir dir file is the only file under tmp data the resulting sequence file would contain the following entries key dir and value filestatus tmp data dir key dir dir and value filestatus tmp data dir dir key dir dir file and value filestatus tmp data dir dir file cases requiring special handling if source path is a file tmp file contents of the file will be as follows target does not exist key value filestatus tmp file target is file key value filestatus tmp file target is dir key file value filestatus tmp file 
 public factory method with which the appropriate copylisting implementation may be retrieved 
 public constructor creates distcp object with specified input parameters e g source paths target location etc 
 implementation of tool run orchestrates the copy of source file s to target location by  creating a list of files to be copied to target  launching a map only job to copy the files delegates to execute 
 implements the core execution creates the file list for copy and launches the hadoop job to do the copy 
 create and submit the mapreduce job 
 create job object for submitting it with all the configuration
 create input listing by invoking an appropriate copy listing implementation also add delegation tokens for each path to job s credential store
 create input listing based on snapshot diff report 
 implementation of copylisting buildlisting iterates over all source paths mentioned in the input file 
 inheritdoc 
 concat chunk files for the same file into one iterate through copy listing identify chunk files for the same file concat them into one 
 concat the passed chunk files into one and rename it the targetfile 
 implementation of the mapper map does the copy 
 implementation of inputformat getsplits this method splits up the copy listing file into chunks and assigns the first batch to different tasks 
 implementation of inputformat createrecordreader 
 implementation of recordreader getprogress 
 implementation of retriablecommand doexecute this is the actual copy implementation 
 implementation of inputformat getsplits returns a list of inputsplits such that the number of bytes to be copied for all the splits are approximately equal 
 the parse method parses the command line options and creates a corresponding options object 
 loads a list of filter patterns for use in shouldcopy 
 get a path with its scheme and authority 
 write a single file directory to the sequence file 
 build a copy list based on the snapshot diff report any file directory changed or created will be in the list deleted files directories will not be in the list since they are handled by link org apache hadoop tools distcpsync sync an item can be created modified and renamed in which case the target path is put into the list 
 collect the list of literal to be copied and write to the sequence file in essence any file or directory that need to be copied or sync ed is written as an entry to the sequence file with the possible exception of the source root when either update sync or overwrite switch is specified and if the the source root is a directory then the source root entry is not written to the sequence file because only the contents of the source directory need to be copied in this case see link org apache hadoop tools util distcputils getrelativepath forhow relative path is computed see computesourcerootpath method for how the root path of the source is computed 
 preserve attribute on file matching that of the file status being sent as argument barring the block size all the other attributes are preserved by this function
 converts filestatus to a list of copylistingfilestatus the resulted list contains either one copylistingfilestatus per chunk of file blocks if file size exceeds blocksize blocksperchunk and there are more blocks in the file than blocksperchunk or a single copylistingfilestatus for the entire file if file size is too small to split if preserving acls populates the copylistingfilestatus with the acls if preserving xattrs populates the copylistingfilestatus with the xattrs 
 the execute method invokes doexecute until either  doexecute succeeds or  the command may no longer be retried e g runs out of retry attempts 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 read bytes starting from the specified position this requires rawstream is an instance of link positionedreadable 
 executes a test using a file system sub tree with multiple nesting levels 
 executes a test using multiple large files 
 this test covers the case where the copyreadexception is double wrapped and the mapper should be able to ignore this nested read exception 
 if a single file is being copied to a location where the file of the same name already exists then the file shouldn t be skipped 
 test the sync returns false in the following scenarios  the source target dir are not snapshottable dir  the source target does not have the given snapshots  changes have been made in target
 test the basic functionality 
 similar test with testsync but the to snapshot is specified as 
 test a case where there are multiple source files with the same name 
 test a case where multiple level dirs are renamed 
 test a case with different delete and rename sequences 
 test a case where there is a cycle in renaming dirs 
 test a case where rename a dir then create a new dir with the same name and sub dir 
 test a case where create a dir then mv a existed dir into it 
 test a case where the source path is relative 
 test the sync returns false in the following scenarios  the source target dir are not snapshottable dir  the source target does not have the given snapshots  changes have been made in target
 test the basic functionality 
 test the case that current is snapshotted as s 
 test a case where there are multiple source files with the same name 
 test a case where multiple level dirs are renamed 
 test a case with different delete and rename sequences 
 test a case where there is a cycle in renaming dirs 
 test a case where rename a dir then create a new dir with the same name and sub dir 
 test a case where create a dir then mv a existed dir into it 
 test a case where create a dir then mv a existed dir into it the difference between this one and testsync is this one also creates a snapshot s  in between s and s 
 restarts the cluster with acls enabled or disabled 
 restarts the cluster with xattrs enabled or disabled 
 test methods run end execute of distcp class silple copy file
 test main method of distcp method should to call system exit 
 runs distcp from src to dst preserving xattrs asserts the expected exit code 
 path owner group permission e g user foo foo bar  
 inheritdoc 
 inheritdoc 
 run a fileoperation 
 this is the main driver for recursively changing files properties 
 write checksum for image file pulled from mdutils internals awkward to reuse existing tools utils 
 tests setting replication of provided files 
 verify that the specified file has a valid provided location 
 this is to be called before any other method of distributedcacheemulator checks if emulation of distributed cache load is needed and is feasible sets the flags generatedistcachedata and emulatedistributedcache to the appropriate values gridmix does not emulate distributed cache load if the specific gridmix job type doesn t need emulation of distributed cache load or the trace is coming from a stream instead of file or the distributed cache dir where distributed cache data is to be generated by gridmix is on local file system or execute permission is not there for any of the ascendant directories of lt iopath gt till root this is because for emulation of distributed cache load distributed cache files created under lt iopath distributedcache gt should be considered by hadoop as public distributed cache files creation of pseudo local file system fails for    and  generation of distributed cache data is also disabled 
 create distributed cache directories also create a file that contains the list of distributed cache files that will be used as distributed cache files for all the simulated jobs 
 create the list of unique distributed cache files needed for all the simulated jobs and write the list to a special file 
 for the job to be simulated identify the needed distributed cache files by mapping original cluster s distributed cache file paths to the simulated cluster s paths and add these paths in the map code distcachefiles jobstory should contain distributed cache related properties like link mrjobconfig cache files link mrjobconfig cache file visibilities link mrjobconfig cache files sizes link mrjobconfig cache file timestamps link mrjobconfig classpath files link mrjobconfig cache archives link mrjobconfig cache archives visibilities link mrjobconfig cache archives sizes link mrjobconfig cache archives timestamps link mrjobconfig classpath archives link mrjobconfig cache symlink 
 write the list of distributed cache files in the decreasing order of file sizes into the sequence file this file will be input to the job link generatedistcachedata also validates if generate option is missing and distributed cache files are missing 
 if gridmix needs to emulate distributed cache load then configure distributed cache files of a simulated job by mapping the original cluster s distributed cache file paths to the simulated cluster s paths and setting these mapped paths in the job configuration of the simulated job configure local fs based distributed cache files through the property tmpfiles and hdfs based distributed cache files through the property link mrjobconfig cache files 
 gather a collection of files at least as large as minsize 
 re generate cache of input filestatus objects 
 param split description of input sources 
 publish the data statistics 
 write random bytes at the path lt inputdir gt if needed 
 write random bytes in the distributed cache files that will be used by all simulated jobs of current gridmix run if files are to be generated do this as part of the mapreduce job link generatedistcachedata job name 
 create each component in the pipeline and start it 
 param conf gridmix configuration
 create gridmix output directory setup things for emulation of various loads if needed 
 setup gridmix for emulation of distributed cache load this includes generation of distributed cache files if needed 
 generate random text data that can be compressed if the record is marked compressible via link fileoutputformat compress only then the random data will be text data else link gridmixrecord writerandom dataoutput int will be invoked 
 wait for the reader thread to exhaust the job trace 
 wait for the monitor to halt assuming shutdown or abort have been called note that since submission may be sporatic this will hang if no form of shutdown has been requested 
 enqueue the job to be submitted per the deadline associated with it 
 continue running until all queued jobs have been submitted to the cluster 
 creating a pseudo local file is nothing but validating the file path actual data of the file is generated on the fly when client tries to open the file for reading 
 validate if the path provided is of expected format of pseudo local file system based files 
 see create path for details
 see create path for details
 serial in this scenario method waits on notification that a submitted job is actually completed logic is simple     while true wait till previousjob is completed break submit newjob previousjob   newjob   
 wait until the service completes it is assumed that either a link shutdown or link abort has been requested 
 stress submits the job in stress mode while jt is overloaded wait if not overloaded get number of slots available keep submitting the jobs till total jobs is sufficient to load the jt that is submit sigma no of maps job   no of slots available 
 methods to generate the home directory for dummy users 
 test link randomtextdatamapper via link compressionemulationutil 
 test if link randomtextdatagenerator can generate random text data with the desired compression ratio this involves using link compressionemulationutil to configure the mr job for generating the random text data with the desired compression ratio running the mr job test link randomtextdatagenerator s output and match the output size compressed with the expected compression ratio 
 test compression ratio with multiple compression ratios 
 test compressible link gridmixrecord 
 test link compressionemulationutil getpossiblydecompressedinputstream path configuration long and link compressionemulationutil getpossiblycompressedoutputstream path configuration 
 test of link filequeue can identify compressed file and providereaders to extract uncompressed data only if input compression is enabled 
 validate the dist cache files generated by generatedistcachedata job 
 runs setupgeneratedistcachedata on a new distrbutedcacheemulator and and returns the jobconf fills the array sortedfilesizes that can be used for validation validation of exit code from setupgeneratedistcachedata is done 
 validate generatedistcachedata job if it creates dist cache files properly 
 validate setupgeneratedistcachedata by validating permissions of the distributed cache directories and content of the generated sequence file this includes validation of dist cache file paths and their file sizes 
 test if distributedcacheemulator s setup of generatedistcachedata is working as expected 
 test the configuration property for disabling enabling emulation of distributed cache load 
 test method configuredistcachefiles
 test link totalheapusageemulatorplugin 
 test task level java heap options configuration in link gridmixjob 
 expands a file compressed using code gzip 
 tests the reading of traces in gridmix these traces are generated by rumen and are in the json format the traces can optionally be compressed and uncompressed traces can also be passed to gridmix via its standard input stream the testing is effected via junit assertions 
 test link executionsummarizer 
 tests high ram job properties configuration 
 test if a file on pseudolocalfs of a specific size can be opened and read validate the size of the data read test the read methods of link pseudolocalfs randominputstream 
 validate if file status is obtained for correctly formed file paths on pseudolocalfs and also verify if appropriate exception is thrown for invalid file paths 
 test pseudo local file system methods like getfilestatus create open exists for valid file paths and invalid file paths 
 test link resourceusagematcher 
 test link loadjob resourceusagematcherrunner 
 test link cumulativecpuusageemulatorplugin 
 validate the behavior of link roundrobinuserresolver for differentuser resource files like empty user resource file non existent user resource file user resource file with valid content
 release logic
 release the method using the exception as a cause
 finalizer does release the stream but also logs at warn level including the uri at fault
 build a properties instance bound to the configuration file using the filesystem uri as the source of the information 
 create a swift rest client instance 
 make an http get request to swift to get a range of data in the object 
 returns object length 
 get the length of the remote object 
 get the path contents as an input stream warning this input stream must be closed to avoid keeping http connections open 
 returns object location as byte 
 create the uri needed to query the location of an object 
 find objects under a prefix 
 find objects in a directory 
 find objects in a location 
 copy an object this is done by sending a copy method to the filesystem which is required to handle this webdav level extension to the base http operations 
 uploads file as input stream to swift the data stream will be closed after the request 
 deletes object from swift the result is true if this operation did the deletion 
 issue a head request 
 issue a put request 
 authenticate to openstack keystone as well as returning the access token the member fields link token link endpointuri and link objectlocationuri are set up for re use this method is re entrant if more than one thread attempts to authenticate neither will block but the field values with have those of the last caller 
 create default container if it doesn t exist for hadoop swift integration non reentrant as this should only be needed once 
 create a container if it already exists do nothing 
 trigger an initial auth operation if some of the needed fields are missing 
 pre execution actions to be performed by methods currently this logs the operation at trace authenticates the client if needed 
 performs the http request validates the response code and returns the received data http status codes are converted into exceptions 
 performs the http request validates the response code and returns the received data http status codes are converted into exceptions 
 exec a get request and return the input stream of the response 
 create an instance against a specific fs uri 
 converts swift path to uri to make request this is public for unit testing
 encode the url this extends link urlencoder encode string string with a replacement of  with  
 convert a swift path to a uri relative to the current endpoint 
 add the headers to the method and the auth token which must be set 
 set the auth key header of the method to the token id supplied 
 execute a method in a new httpclient instance if the auth failed authenticate then retry the method 
 ensures that an object reference passed as a parameter to the calling method is not null 
 create the parent directories as an optimization the entire hierarchy of parent directories is not polled instead the tree is walked up from the last to the first creating directories until one that exists is found this strategy means if a file is created in an existing directory one quick poll suffices there is a big assumption here that all parent directories of an existing directory also exists 
 internal implementation of directory creation 
 should mkdir create this directory if the directory is root false if the entry exists and is a directory false if the entry exists and is a file exception else true
 param permission currently ignored 
 opens an fsdatainputstream at the indicated path 
 delete a file this method is abstract in hadoop  x in  x it is non abstract and deprecated
 upload a file input stream of a specific length 
 upload part of a larger file 
 tell the swift server to expect a multi part upload by submitting a  byte file with the x object manifest header
 get the metadata of an object
 get the http headers in case you really need the low level metadata
 get the metadata of an object
 get the object as an input stream
 get the input stream starting from a specific point 
 list a directory this is o n for the number of objects in this path 
 list all elements in this directory
 create a directory
 try to find the specific server s on which the data lives
 deletes object from swift
 deletes a directory from swift this is not recursive
 does the object exist
 rename through copy and delete this is a consequence of the swift filesystem using the path as the hash into the distributed hash table the ring of filenames because of the nature of the operation it is not atomic 
 copy an object then if the copy worked delete it if the copy failed the source object is not deleted 
 take a hadoop path and return one which uses the uri prefix and authority of this fs it doesn t make a relative path absolute
 builds a hadoop path from a swift path inserting the uri authority of this fs instance
 insert a throttled wait if the throttle delay gt 
 delete the entire tree this is an internal one with slightly different behavior if an entry is missing a link filenotfoundexception israised this lets the caller distinguish a file not found with other reasons for failure so handles race conditions in recursive directory deletes better the problem being addressed is caller a requests a recursive directory of directory dir caller b requests a delete of a file dir file between caller a enumerating the files contents and requesting a delete of dir file we want to recognise the special case directed file is no longer there and not convert that into a failure
 re open the buffer
 close the stream after this the stream is not usable unless and until it is re opened which can happen on some of the buffer ops this method is thread safe and idempotent 
 seek to an offset if the data is already in the buffer move to it
 create an output stream
 flush the local backing stream this does not trigger a flush of data to the remote blobstore 
 close the stream this will trigger the upload of all locally cached data to the remote blobstore 
 write to the backup stream guarantees backupstream is open blockoffset  len lt filepartsize 
 upload a single partition this deletes the local backing file and re opens it to create a new one 
 returns the response body of the httpresponse if any as an array of bytes if response body is not available or cannot be read returns null note this will cause the entire response body to be buffered in memory a malicious server may easily exhaust all the vm memory it is strongly recommended to use getresponseasstream if the content length of the response is unknown or reasonably large 
 returns the response body of the httpresponse if any as a link string if response body is not available or cannot be read returns null the string conversion on the data is done using utf  note this will cause the entire response body to be buffered in memory a malicious server may easily exhaust all the vm memory it is strongly recommended to use getresponseasstream if the content length of the response is unknown or reasonably large 
 create a path tuple of container path where the container is chosen from the host of the uri 
 get the test uri
 write a file and read it in validating the result optional flags control whether file overwrite operations should be enabled and whether the file should be deleted afterwards if there is a mismatch between what was written and what was expected a small range of bytes either side of the first error are logged to aid diagnosing what problem occurred whether it was a previous file or a corrupting of the current file this assumes that two sequential runs to the same path use datasets with different character moduli 
 write a file optional flags control whether file overwrite operations should be enabled
 touch a file fails if it is already there
 assert that a file exists and whose link filestatus entrydeclares that this is a file and not a symlink or directory 
 assert that a path exists but make no assertions as to the type of that entry
 create a file using the standard link data bytes 
 rename a path
 get a string describing the outcome of a rename by listing the dest path and its parent along with some covering text
 rename to success
 test on concurrent file system changes
 tests functionality for big files  gb upload
 helper method to create instances of object using the class name specified in the configuration object 
 check if recurrenceid is null or resourceskylines is null 
 check if pipelineid is null or resourceovertime is null 
 converts string date to unix timestamp note that we assume the time in the logs has the same time zone with the machine which runs the link rmsinglelineparser 
 parse the log file directory 
 runs the actual anonymization tool 
 constructor 
 constructor 
 constructor 
 param args string arguments see code usage 
 parse the job configuration file as an input stream and return a link properties collection the input stream will not be closed afterreturn from the call 
 initialized the link statepool this api also reloads the previously persisted state note that the link statepool should be initialized onlyonce 
 persists the current state to the state directory the state will be persisted to the latest file in the state directory 
 throws ioexception there should be files in the directory named by  test build data rumen histogram test there will be pairs of files inputxxx json and goldxxx json we read the input file as a histogramrawtestdata in json then we create a histogram using the data field and then a loggeddiscretecdf using the percentiles and scale field finally we read the corresponding goldxxx json as a loggeddiscretecdf and deepcompare them 
 register with rm
 parse workload from a sls trace file 
 parse workload from a rumen trace file 
 parse workload information from synth generator trace files 
 parse the sls trace file return each host name
 parse the input node file return each host name
 package metrics information for one tracked queue app only support fairscheduler currently
 convert the byte buffer to a string an specific character encoding
 creates a new instance of csvrecordoutput 
 serialize a record without a tag
 deserialize a record without a tag
 param s
 converts a csv serialized representation of buffer to a new buffer
 the main driver for dumptypedbytes 
 dump given list of files to standard output as typed bytes 
 param name path in jar for this jar element must not start with 
 test program 
 this method creates a streaming job from the given argument list the created object can be used and or submitted to a jobtracker for execution by a job agent such as jobcontrol
 this is the method that actually intializes the job conf and submits the job to the jobtracker
 return path to the created jar file or null if no files are necessary 
 split a utf  byte array into key and value assuming that the delimilator is at splitpos 
 split a utf  byte array into key and value assuming that the delimilator is at splitpos 
 split a utf  byte array into key and value assuming that the delimilator is at splitpos 
 reads a typed bytes sequence and converts it to a java object the first byte is interpreted as a type code and then the right number of subsequent bytes are read depending on the obtained type 
 reads a typed bytes sequence the first byte is interpreted as a type code and then the right number of subsequent bytes are read depending on the obtained type 
 reads a type byte and returns the corresponding link type 
 skips a type byte 
 reads the bytes following a type bytes code 
 reads the raw bytes following a custom code 
 reads the raw bytes following a type bytes code 
 reads the byte following a type byte code 
 reads the raw byte following a type byte code 
 reads the boolean following a type bool code 
 reads the raw bytes following a type bool code 
 reads the integer following a type int code 
 reads the raw bytes following a type int code 
 reads the long following a type long code 
 reads the raw bytes following a type long code 
 reads the float following a type float code 
 reads the raw bytes following a type float code 
 reads the double following a type double code 
 reads the raw bytes following a type double code 
 reads the raw bytes following a type string code 
 reads the vector following a type vector code 
 reads the raw bytes following a type vector code 
 reads the header following a type vector code 
 reads the list following a type list code 
 reads the raw bytes following a type list code 
 reads the map following a type map code 
 reads the raw bytes following a type map code 
 reads the header following a type map code 
 writes a java object as a typed bytes sequence 
 writes a raw sequence of typed bytes 
 writes a raw sequence of typed bytes 
 writes a bytes array as a typed bytes sequence using a given typecode and length 
 writes a bytes array as a typed bytes sequence using a given typecode 
 writes a bytes array as a typed bytes sequence 
 writes a bytes buffer as a typed bytes sequence 
 writes a byte as a typed bytes sequence 
 writes a boolean as a typed bytes sequence 
 writes an integer as a typed bytes sequence 
 writes a long as a typed bytes sequence 
 writes a float as a typed bytes sequence 
 writes a double as a typed bytes sequence 
 writes a string as a typed bytes sequence 
 writes a vector as a typed bytes sequence 
 writes a vector header 
 writes a list as a typed bytes sequence 
 writes a list header 
 writes a list footer 
 writes a map as a typed bytes sequence 
 writes a map header 
 print prewritelines to stderr pausing sleep ms between each output then consume stdin and echo it to stdout then write postwritelines to stderr 
 runs a streaming job with the given arguments
 run the job with the indicating the input format key should be emitted 
 run the job the default way the input format key is not emitted 
 check if mapper reducer with empty nonempty input works properly if reporting is done using lines like reporter status and reporter counter before map reduce method is called validate the task s log of stderr if messages are written to stderr before map reduce is called also validate job output 
 run a streaming job with the given script as mapper and validate run another streaming job with the given script as reducer and validate 
 parse the string argument as a link reservationid 
 get the value for a specified resource no information about the units is returned 
 set the resourceinformation object for a particular resource 
 set the value of a resource in the resourceinformation object the unit of the value is assumed to be the one in the resourceinformation object 
 creates an instance of link configurationprovider using givenconfiguration 
 parses source tags from expression sourcetags numofallocations 
 parses a placement constraint specification a placement constraint spec is a composite expression which is composed by multiple sub constraint expressions delimited by with following syntax tag n p tag n p tagn nn pn where tagn nn is a key value pair to determine the source allocation tag and the number of allocations such as foo  and where pn can be any form of a valid constraint expression such as in node foo bar notin node foo bar   and notin node foo notin node bar 
 from a given configuration get all entries representing requested resources entries that match the prefix resourcename   value units pattern 
 dump out contents of cwd and the environment to stdout for debugging
 parse command line options
 main run function for the application master
 param args command line arguments 
 parse command line options
 main run function for the client
 monitor the submitted application for completion kill application if time expires 
 checks the events and idprefix published for an entity 
 monitor the submitted application for completion kill application if time expires 
 try to connect to the host port a failure to connect within the specified timeout is a failure 
 validate the config files 
 add a component under the slider name entry 
 add a service under a path for the current user 
 look up an external rest api
 resolve a service record then return an external rest api exported it 
 set an entire map full of values
 verify that the given directory is not present
 verify that a user has write access to a directory it does this by creating then deleting a temp file
 register all files under a fs path as a directory to push out
 submit a jar containing a specific class returning the resource to be mapped in
 submit a local file to the filesystem references by the instance s cluster filesystem
 submit the am tar gz resource referenced by the instance s cluster filesystem also update the providerresources object with the new resource 
 clone by converting to json and back again this is much less efficient than any java clone process 
 deserialize from a byte array
 load from a hadoop filesystem
 save to a hadoop filesystem
 save an instance to a file
 write the json as bytes then close the file
 save the config to a destination file in the format of this outputter
 save the content the default saves the asstring value to the output stream
 find a containing jar
 find a containing jar
 get a random open port
 see if a port is available for listening on by trying to listen on it and seeing if that works or fails 
 given a source folder create a tar gz file
 setup api service web request 
 launch yarn service application 
 stop yarn service application 
 start yarn service application 
 save service configuration 
 decommission a yarn service 
 change number of containers associated with a service 
 retrieve service status through rest api 
 wait for check to return true for each  ms see also link waitfor java util function supplier int and link waitfor java util function supplier int int 
 wait for check to return true for each checkeverymillis ms see also link waitfor java util function supplier int int 
 wait for check to return true for each checkeverymillis ms in the main loop this method will log the message waiting in main loop for each loginterval times iteration to confirm the thread is alive 
 create a new instance of appadminclient 
 wait for check to return true for each  ms see also link waitfor java util function supplier int and link waitfor java util function supplier int int 
 wait for check to return true for each checkeverymillis ms see also link waitfor java util function supplier int int 
 tells the heartbeat and handler threads to stop and waits for them to terminate 
 registers this application master with the resource manager on successful registration starts the heartbeating thread 
 prints the application attempt report for an application attempt id 
 prints the container report for an container id 
 kills applications with the application id as appid
 prints the application report for an application id 
 updates priority of an application with the given id 
 prints the node report for node id 
 handle resources of two different formats  updatenoderesource nodeid memsize vcores overcommittimeout  updatenoderesource nodeid resourcetypes overcommittimeout incase of no of args is  or  nd arg should contain only numbers to satisfy the st format otherwise nd format flow continues 
 look up and return the resource manager s principal this method automatically does the host replacement in the principal and correctly handles ha resource manager configurations 
 perform the host replacement in the code principal returning the result correctly handles ha resource manager configurations 
 wait until the container reaches a state n times 
 test printing of help messages
 start resource manager server
 throw link runtimeexception inside a thread of link resourcemanager with ha enabled and check if the link resourcemanager is transited to standby state 
 throw link runtimeexception inside a thread of link resourcemanager with ha disabled and check link rmcriticalthreaduncaughtexceptionhandler instance used link exitutil class to avoid jvm exit through code system exit  
 check if reaching to maximum of retries 
 poll timelineserviceaddress for maximum of retries times if it is null 
 create a proxy to the resourcemanager for the specified protocol 
 close all the proxy objects which have been opened over the lifetime of this proxy provider 
 creates a executor and invokes all proxies concurrently 
 currently used by client and am only create a proxy for the specified protocol for non ha this is a direct connection to the resourcemanager address when ha is enabled the proxy handles the failover between the resourcemanagers as well 
 currently used by nodemanagers only create a proxy for the specified protocol for non ha this is a direct connection to the resourcemanager address when ha is enabled the proxy handles the failover between the resourcemanagers as well 
 helper method to create failoverproxyprovider 
 utility method that unwraps and returns appropriate exceptions 
 initialize the logwriter must be called just after the instance is created 
 returns the owner of the application 
 returns acls for the application an empty map is returned if no acls are found 
 read the next key and return the value stream 
 get a containerlogsreader to read the logs for the specified container 
 writes all logs for a single container to the provided writer 
 writes all logs for a single container to the provided writer 
 keep calling this till you get a link eofexception for getting logs ofall types for a single container 
 keep calling this till you get a link eofexception for getting logs ofall types for a single container for the specific bytes 
 keep calling this till you get a link eofexception for getting logs ofall types for a single container 
 keep calling this till you get a link eofexception for getting logs ofthe specific types for a single container 
 keep calling this till you get a link eofexception for getting logs ofthe specific types for a single container 
 verify and create the remote log directory 
 return the remote application log directory 
 get all available log files under remote app log directory 
 get all available log files under remote app log directory 
 get all available log files under remote app log directory 
 parse start index from html 
 parse end index from html 
 parse log start time from html 
 parse log end time from html 
 output container log 
 create the container log file under given local directory nodeid and return the printstream object 
 add multiple node labels to repository
 add more labels to nodes
 remove multiple node labels from repository
 remove labels from nodes labels being removed most be contained by these nodes
 replace labels to nodes
 write the tokenidentifier to the output stream 
 param classes list of classes which have static fieldstatemachinefactory of type statemachinefactory
 returns a boolean to denote whether a cache file is visible to all public or not
 returns true if all ancestors of the specified path have the execute permission set for all users i e that other users can traverse the directory hierarchy to the given path 
 checks for a given path whether the other permissions on it imply the permission in the passed fsaction
 localize files 
 copy source path to destination with localization rules 
 recursively change permissions of all files dirs on path based on resource visibility change to  or  for dirs  or  for files 
 construct the processinfo using the process pid and procfs rooted at the specified directory and return the same it is provided mainly to assist testing purposes returns null on failing to read from procfs 
 update memory related information
 create the resourcecalculatorprocesstree rooted to specified process from the class name and configure it if class name is null this method will try and return a process tree plugin available for this system 
 serialize a pojo object into a json string not in a pretty format
 given iso formatted string with format yyyy mm dd t hh mm ss sssz return epoch time for local time zone 
 parses string of process info lines into processinfo objects
 generate a specific hamlet implementation from a spec 
 this method generate record instance by calling newintance using reflection add register the generated value to typevaluecache
 verify that the rpc layer is always created using the correct ugi from the rmproxy it should always use the ugi from creation in subsequent uses even outside of a doas 
 bad user user owner is trying to read logs without access
 try to read bad logs
 reading from logs should succeed and they should be shown in the aggregatedlogsblock html 
 log files was deleted 
 utility function to create container log file and upload it into remote file system 
 throw code yarnruntimeexception inside thread andcheck code yarnuncaughtexceptionhandler instance
 throw code error inside thread andcheck code yarnuncaughtexceptionhandler instance used code exitutil class to avoid jvm exit through code system exit  
 throw code outofmemoryerror inside thread andcheck code yarnuncaughtexceptionhandler instance used code exitutil class to avoid jvm exit through code runtime getruntime halt  
 get pid from a pid file 
 a basic test that creates a few process directories and writes stat files verifies that the cpu time and memory is correctly computed 
 tests that cumulative memory is computed only for processes older than a given age 
 verifies procfsbasedprocesstree checkpidpgrpidformatch in case of constructprocessinfo returning null by not writing stat file for the mock process
 test the correctness of process tree dump 
 is the root process alive used only in tests 
 write stat files under the specified pid directories with data setup in the corresponding processstatinfo objects
 deserialize from a byte array
 deserialize from a byte array optionally checking for a marker string if the marker parameter is supplied and not empty then its presence will be verified before the json parsing takes place it is a fast fail check if not found an link invalidrecordexception exception will beraised
 create a full path from the registry root and the supplied subdir
 get a single uri endpoint
 get the address urls guranteed to return at least one address 
 validate the record by checking for null fields and other invalid conditions
 list service records directly under a path
 extract all service records under a list of stat operations this non atomic action skips entries that are too short or simply not matching 
 extract all service records under a list of stat operations this non atomic action skips entries that are too short or simply not matching 
 start the service this is where the curator instance is started 
 create a path if it does not exist the check is poll  create there s a risk that another process may create the same path before the create operation is executed propagated to the zk node polled 
 stat the file 
 get the acls of a path 
 probe for a path existing 
 verify a path exists 
 create a directory it is not an error if it already exists 
 recursively make a path 
 create a path with given data byte  is used for a path without data 
 update the data for a path 
 create or update an entry 
 delete a directory directory tree it is not an error to delete a path that does not exist 
 list all children of a path 
 read data on a path 
 init the service this sets up security based on the configuration
 init security after this operation the link systemacls list is valid 
 generate a base  encoded digest of the idpasswordpair pair
 create a digest id from an id pass pair
 initializes the dns record type to descriptor mapping based on the provided service record 
 create an application txt record descriptor 
 create an application srv record descriptor 
 create an application cname record descriptor 
 create an application aaaa record descriptor 
 create an application a record descriptor 
 initializes the descriptor parameters 
 return the ipv mapped address for the provided ipv address utilized to create corresponding aaaa records 
 creates an application associated dns record descriptor the endpoint is leverated to create an associated application api record 
 return the dns name associated with the api endpoint 
 initializes the dns record type to descriptor mapping based on the provided service record 
 create a container txt record descriptor 
 creates a container ptr record descriptor 
 creates a container aaaa ipv record descriptor 
 creates a container a ipv record descriptor 
 initializes the descriptor parameters 
 initializes the registry 
 initializes the registry based on available parameters in the hadoop configuration 
 signs zone records if necessary dnssec enabled zones may not have their ns and soa records signed if they were initialized from master files 
 initializes the reverse lookup zone mapping ip to name 
 create the zones based on the zone count 
 create the zone and its related zone associated dns records ns soa 
 load the required public private keys create the zone dnskey record and sign the zone level records 
 process a tcp request 
 monitor the tcp socket for inbound requests 
 open the tcp listener 
 create the thread callable monitoring the tcp listener 
 create the thread monitoring the socket for inbound udp requests 
 process an inbound udp request 
 create and udp listener socket 
 add the answer section to the response 
 add the nxt record to the authority section of the response 
 perform a zone transfer 
 register dns records based on the provided service record 
 delete the dns records generated by the provided service record 
 performs operations required to setup the dns registry instance e g sets up a path listener to react to service record creation deletion and invoke the appropriate registry method 
 iterates thru the supplied service records executing the provided registry command 
 process the service record parsing the information and creating the required dns records 
 given a baseip range and index return the network address for the reverse zone 
 initialize the service including choosing a path for the data
 startup start zk it is only after this that the binding information is valid 
 start the service including creating base directories with permissions
 start an async operation to create the home path for a user if it does not exist
 create the home path for a user if it does not exist this uses link inituserregistryasync string and then waits for theresult the code path is the same as the async operation this just picks up and relays converts exceptions
 recursive operation to purge all matching records under a base path uses a depth first search a match is on id and persistence policy or if policy    any match if a record matches then it is deleted without any child searches deletions will be asynchronous if a callback is provided the code is designed to be robust against parallel deletions taking place in such a case it will stop attempting that part of the tree this avoid the situation of more than  purge happening in parallel and one of the purge operations deleteing the node tree above the other 
 create a service entry with the sample endpoints and put it at the destination
 create a service entry with the sample endpoints
 add some endpoints
 log out from a context if non null exceptions are caught and logged
 all class initialization for this test class
 sets up the kdc and a set of principals in the jaas file
 log in defaulting to the client context
 start the secure zk instance using the test method name as the path as the entry is saved to the link securezk field itis automatically stopped after the test case 
 returns log file s name as well as current file size for a container 
 deserializes an object from a byte array created with link write object 
 read entity from a db iterator if no information is found in the specified fields for this entity return null 
 retrieves a list of entities satisfying given parameters 
 put a single entity if there is an error add a timelineputerror to the given response 
 for a given key value pair that has been written to the db write additional entries to the db for each primary filter 
 get the unique start time for a given entity as a byte array that sorts the timestamps in reverse order see link genericobjectmapper writereverseorderedlong long 
 get the unique start time for a given entity as a long 
 get the unique start time for a given entity as a byte array that sorts the timestamps in reverse order see link genericobjectmapper writereverseorderedlong long if the start time doesn t exist set it based on the information provided should only be called when a lock has been obtained on the entity 
 checks db for start time and returns it if it exists if it doesn t exist writes the suggested start time if it is not null this is only called when the start time is not found in the cache so it adds it back into the cache if it is found should only be called when a lock has been obtained on the entity 
 discards entities with start timestamp less than or equal to the given timestamp 
  versioning timeline store major minor for e g           etc  any incompatible change of ts store is a major upgrade and any compatible change of ts store is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade timeline store or remove incompatible old state 
  versioning timeline state store major minor for e g           etc  any incompatible change of ts store is a major upgrade and any compatible change of ts store is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade timeline store or remove incompatible old state 
 initialize the state storage
 start the state storage for use
 shutdown the state storage 
 read entity from a db iterator if no information is found in the specified fields for this entity return null 
 retrieves a list of entities satisfying given parameters 
 put a single entity if there is an error add a timelineputerror to the given response 
 for a given key value pair that has been written to the db write additional entries to the db for each primary filter 
 get the unique start time for a given entity as a byte array that sorts the timestamps in reverse order see link genericobjectmapper writereverseorderedlong long 
 get the unique start time for a given entity as a long 
 get the unique start time for a given entity as a byte array that sorts the timestamps in reverse order see link genericobjectmapper writereverseorderedlong long if the start time doesn t exist set it based on the information provided 
 checks db for start time and returns it if it exists if it doesn t exist writes the suggested start time if it is not null this is only called when the start time is not found in the cache so it adds it back into the cache if it is found 
 discards entities with start timestamp less than or equal to the given timestamp 
  versioning timeline store major minor for e g           etc  any incompatible change of ts store is a major upgrade and any compatible change of ts store is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade timeline store or remove incompatible old state 
 get the timeline entities that the given user have access to the meaning of each argument has been documented with link timelinereader getentities 
 get the single timeline entity that the given user has access to the meaning of each argument has been documented with link timelinereader getentity 
 get the events whose entities the given user has access to the meaning of each argument has been documented with link timelinereader getentitytimelines 
 store the timeline entities into the store and set the owner of them to the given user 
 add or update an domain if the domain already exists only the owner and the admin can update it 
 get a single domain of the particular id if callerugi is not the owner or the admin of the domain null will be returned 
 get all the domains that belong to the given owner if callerugi is not the owner or the admin of the domain empty list is going to be returned 
 create a proxy to the resourcemanager for the specified protocol 
 create a proxy for the specified protocol in the context of federation for non ha this is a direct connection to the resourcemanager address when ha is enabled the proxy handles the failover between the resourcemanagers as well 
 it splits a list of non localized resource requests among sub clusters 
 return a projection of this any link resourcerequest that belongs tothis sub cluster this is done based on the count of the containers that require locality in each sublcuster if any or based on the weights and headroom 
 converts the policy into a byte array representation in the input link bytebuffer 
 a utilize method to instantiate a policy manager class given the type class name from link subclusterpolicyconfiguration 
 get amrmproxy policy from state store using default queue and configuration as fallback 
 this default implementation validates the link federationpolicyinitializationcontext then checks whether it needs to reinstantiate the class null or mismatching type and reinitialize the policy 
 this default implementation validates the link federationpolicyinitializationcontext then checks whether it needs to reinstantiate the class null or mismatching type and reinitialize the policy 
 common functionality to instantiate a reinitialize a link configurablefederationpolicy 
 this method provides a wrapper of all policy functionalities for routing internally it manages configuration changes and policy init reinit 
 get the subcluster for an application 
 put an application 
 get the current information for a subcluster from zookeeper 
 put the subcluster information in zookeeper 
 get the queue policy from zookeeper 
 put the subcluster information in zookeeper 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link addapplicationhomesubclusterrequest for adding a new application is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link updateapplicationhomesubclusterrequest for updating an application is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link getapplicationhomesubclusterrequest for querying application s information is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link deleteapplicationhomesubclusterrequest for deleting an application is valid or not 
 validate if the applicationhomesubcluster info are present or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link subclusterregisterrequest forregistration a new subcluster is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link subclusterderegisterrequest forderegistration a subcluster is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link subclusterheartbeatrequest forheartbeating a subcluster is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link getsubclusterinforequest for queryingsubcluster s information is valid or not 
 validate if all the required fields on link subclusterinfo are presentor not code capability will be empty as the corresponding code resourcemanager is in the process of initialization duringregistration 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link getsubclusterpolicyconfigurationrequest for querying policy sinformation is valid or not 
 quick validation on the input to check some obvious fail conditions fail fast check if the provided link setsubclusterpolicyconfigurationrequest for adding a new policy isvalid or not 
 validate if the subclusterpolicyconfiguration is valid or not 
 returns the sql federationstatestore connections to the pool 
 returns the sql federationstatestore connections to the pool 
 get the list of known applications in the registry 
 for testing delete all application records in registry 
 write update the uam token for an application and a sub cluster 
 load the information of one application from registry 
 remove an application from registry 
 returns the link subclusterinfo for the specified link subclusterid 
 updates the cache with the central link federationstatestore and returnsthe link subclusterinfo for the specified link subclusterid 
 returns the link subclusterpolicyconfiguration for the specified queue 
 allocate opportunistic containers 
 create a new uam and register the application without specifying uamid and appid we will ask for an appid from rm and use it as the uamid 
 launch a new uam in the resource manager 
 re attach to an existing uam in the resource manager 
 force kill the uam 
 sends the specified heart beat request to the resource manager and invokes the callback asynchronously with the response 
 launch and initialize an unmanaged am first it creates a new application on the rm and negotiates a new attempt id then it waits for the rm application attempt state to reach yarnapplicationattemptstate launched after which it returns the am rm token 
 monitor the submitted application and attempt until it reaches certain states 
 gets the amrmtoken of the unmanaged am 
 helper method for client calling applicationmasterprotocol allocate that handles re register if rm fails over 
 helper method for client calling applicationmasterprotocol finishapplicationmaster that handles re register if rm fails over 
 authorizes the current request and returns the amrmtokenidentifier for the current application 
 initialiaze a main memory link federationstatestorefacade used fortesting wiht a mock resolver 
 if register is slow async allocate requests in the meanwhile should not throw or be dropped 
 recover from nm state store called after serviceinit before servicestart 
 this is called by the ams started on this node to register with the rm this method does the initial authorization and then forwards the request to the application instance specific intercepter chain 
 this is called by the ams started on this node to unregister from the rm this method does the initial authorization and then forwards the request to the application instance specific intercepter chain 
 this is called by the ams started on this node to send heart beat to rm this method does the initial authorization and then forwards the request to the application instance specific pipeline which is a chain of request intercepter objects one application request processing pipeline is created per am instance 
 initializes the request intercepter pipeline for the specified application 
 authorizes the request and returns the application specific request processing pipeline 
 sends the heart beats to the home rm and the secondary sub cluster rms that are being used by the application 
 sends the finish application master request to all the resource managers used by the application 
 in federation the heart beat request needs to be sent to all the sub clusters from which the am has requested containers this method splits the specified allocaterequest from the am and creates a new request for each sub cluster rm 
 this methods sends the specified allocaterequests to the appropriate sub cluster resource managers 
 this method ensures that unmanaged ams are created for each of the specified sub cluster specified in the input and registers with the corresponding resource managers 
 recover an already existing container this is a blocking call and returns only when the container exits note that the container must have been activated prior to this call 
 this method writes out the launch environment of a container to the default container launch script for the default container script path see link containerlaunch container script 
 this method writes out the launch environment of a container to a specified path 
 start a list of containers on this nodemanager 
 increase resource of a list of containers on this nodemanager 
 update resource of a list of containers on this nodemanager 
 stop a list of containers running on this nodemanager 
 get a list of container statuses running on this nodemanager
 reinitialize a container using a new launch context if the retryfailurecontext is not provided the container is terminated on failure note auto commit is true by default this also means that the rollback context is purged as soon as the command to start the new process is sent the container moves to running state 
 rollback the last reinitialization if possible 
 commit last reinitialization after which no rollback will be possible 
 tries to tail and fetch tail size in bytes of data from the error log errorlog filename is not fixed and depends upon app hence file name pattern is used 
 cleanup the container cancels the launch if launch has not started yet or signals the executor to not execute the process if not already done so also sends a sigterm followed by a sigkill to the process if the process id is available 
 send a signal to the container 
 loop through for a time bounded interval waiting to read the process id from a file generated by a running process 
 set stdout for the shell script
 set stderr for the shell script
 executes a privileged operation it is up to the callers to ensure that each privileged operation s parameters are constructed correctly the parameters are passed verbatim to the container executor binary 
 create cgroup handler object 
 mount cgroup or use existing mount point based on configuration 
 if tasks file is empty delete the cgroup 
 get number of requested gpus from resource 
 bootstrap tc configuration
 get the status of the docker container this runs a docker inspect to get the status if the container no longer exists docker inspect throws an exception and the nonexistent status is returned 
 execute the docker inspect command to retrieve the docker container s status 
 set a dns friendly hostname 
 signal the docker container signals are used to check the liveliness of the container as well as to stop kill the container the following outlines the docker container signal handling if the null signal is sent run kill  on the pid this is used to check if the container is still alive which is necessary for reacquiring containers on nm restart if sigterm sigkill is sent attempt to stop and remove the docker container if the docker container exists and is running execute docker stop if any other signal is sent signal the container using docker kill 
 reap the docker container 
 initialize the java security policy directory either creates the directory if it doesn t exist or clears the contents of the directory if already created 
 prior to environment from being written locally need to generate policy file which limits container access to a small set of directories additionally the container run command needs to be modified to include flags to enable the java security manager with the generated policy the java sandbox will be circumvented if the user is a member of the group specified in value yarnconfiguration yarn container sandbox whitelist group and ifthey do not include the jvm flag value nmcontainerpolicyutils security flag 
 deletes policy files for container specified by parameter additionally this method will age off any stale policy files generated by link javasandboxlinuxcontainerruntime 
 write new policy file to policyoutstream which will include read access to localize resources optionally a default policyfilepath can be specified to append a custom policy implementation to the new policy file
 modify command to enable the java security manager and specify java policy file will modify the passed commands to strip any existing java security configurations expects a java command to be the first and only executable provided in enforcing mode in passive mode any commands with  or will not be modified 
 checks that the original remote file is either owned by the user who started the app or public 
 get gpu device information from system this need to be called after initialize please note that this only works on nix platform so external caller need to make sure this 
 get list of gpu devices usable by yarn 
 do initialize when gpu requested
 write out the wrapper script for the container launch script this method will create the script at the configured wrapper script path 
 return a randomly chosen application directory from a list of local storage directories the probability of selecting a directory is proportional to its size 
 initialize the local directories for a particular user mkdir local dir usercache user 
 initialize the local cache directories for a particular user local dir usercache user local dir usercache user appcache local dir usercache user filecache 
 initialize the local directories for a particular user local dir usercache user appcache appid 
 create application log directories on all disks 
 create application log directories on all disks 
 create any non existent directories and parent directories updating the list of valid directories if necessary 
 method which initializes the timertask and its interval time 
 terminate the timer
 stop the thread that does the node resource utilization monitoring 
 periodically monitor the resource utilization of the node 
  versioning scheme major minor for e g           etc  any incompatible change of state store is a major upgrade and any compatible change of state store is a minor upgrade  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade nm state or remove incompatible old state 
 initialize the state storage 
 start the state storage for use 
 shutdown the state storage 
 route register call to the corresponding distributed scheduling method viz registerapplicationmasterfordistributedscheduling and return response to the caller after stripping away distributed scheduling information 
 route allocate call to the allocatefordistributedscheduling method and return response to the caller after stripping away distributed scheduling information 
 if tasks file is empty delete the cgroup 
 get the process id from specified file path parses each line to find a valid number and returns the first one found 
 finds the local directories that logs for the given container are stored on 
 finds the log file with the given filename for the given container 
 returns the contents of a container s log file in plain text only works for containers that are still in the nodemanager s memory so logs are no longer available after the corresponding application is no longer running 
 this helper method will invoke the specified function in parallel for each end point in the specified list using a thread pool and return the responses received from the function it implements the logic required for dispatching requests in parallel and waiting for the responses if any of the function call fails or times out it will ignore and proceed with the rest so the responses returned can be less than the number of end points specified
 we override this method here to return a mock rm instances the base class returns the proxy to the real rm which will not work in case of stand alone test cases 
 verify correct container request events sent to localizer 
 verify container launch when all resources already cached 
 verify servicedata correctly sent 
 see if environment variable is forwarded using sanitizeenv 
 test that script exists with non zero exit code when command fails 
 test that script exists with non zero exit code when command fails 
 test that script exists with non zero exit code when command fails 
 create an empty mtab file no cgroups are premounted
 create simulated cgroups mount point 
 tests whether mtab parsing works as expected with a valid hierarchy set 
 tests whether mtab parsing works as expected with the specified hierarchy 
 tests whether mtab parsing works as expected with an empty hierarchy set 
 tests whether mtab parsing works as expected with a hierarchy set 
 tests whether mtab parsing works as expected with the specified hierarchy 
 if verifyaccess fails the upload should fail
 if rename fails the upload should fail
 if verifyaccess uploadfile rename and notification succeed the upload should succeed
 if verifyaccess uploadfile and rename succed but it receives a nay from scm the file should be deleted
 if resource is public verifyaccess should succeed
 if the localpath does not exists getactualpath should get to one level down
 create the given number of log files under the container log directory 
 verify if the application log aggregator configured with given log retention period and the recovered log initialization time of the application uploads and deletes the set of log files as expected 
 run a test to submit values for actual memory usage and see if the histogram comes out correctly 
 submit both a guaranteed and an opportunistic container each of which requires more resources than available at the node and make sure they are both queued 
 starts one opportunistic container that takes up the whole node s resources and submit two more that will be queued 
 starts one guaranteed container that takes us the whole node s resources and submit more opportunistic containers than the opportunistic container queue can hold opportunistic containers that cannot be queue should be killed 
 submit two opportunistic and one guaranteed containers the resources requests by each container as such that only one can run in parallel thus the opportunistic container that started running will be killed for the guaranteed container to start once the guaranteed container finishes its execution the remaining opportunistic container will be executed 
  submit a long running guaranteed container to hog all nm resources  submit  opportunistic containers all of which will be queued  update the queue limit to   ensure only  containers remain in the queue and  are de queued 
  submit a long running guaranteed container to hog all nm resources  submit  opportunistic containers both of which will be queued  send stop container to one of the queued containers  ensure container is removed from the queue 
 submit three opportunistic containers that can run concurrently and one guaranteed that needs to kill two of the opportunistic for it to run 
 submit four opportunistic containers that can run concurrently and then two guaranteed that needs to kill exactly two of the opportunistic for it to run make sure only  are killed 
 start running one guaranteed container and queue two opportunistic ones try killing one of the two queued containers 
 starts one opportunistic container that takes up the whole node s resources and submit one more that will be queued now promote the queued opportunistic container which should kill the current running opportunistic container to make room for the promoted request 
 prepare a launch context for container upgrade and request the container manager to re initialize a running container using the new launch context 
 prepare and start an initial container this container will be subsequently re initialized for upgrade it also waits for the container to start and returns the pid of the running container 
 creates a script to run a container that will run forever unless stopped by external means 
 verifies that if for some reason nm fails to start containermanager rpc server rm is oblivious to nm s presence the behaviour is like this because otherwise nm will report to rm even if all its servers are not started properly rm will think that the nm is alive and will retire the nm only after nm expiry interval see mapreduce  
 test completed containerstatus get back up when heart beat lost and will be sent via next heart beat 
 this is to avoid race condition in the test case nodestatusupdater heartbeat thread after sending the heartbeat needs some time to process the response and then go wait state but in the test case once the main test thread returns back after resourcetracker waittillheartbeat we proceed with next sendoutofbandheartbeat before heartbeat thread is blocked on wait 
 return the ha status of this rm this includes the current state and whether the rm is ready to become active 
 get the rmnodelabelsmappingprovider which is used to provide node labels 
 refresh the nodes gracefully
  versioning scheme major minor for e g           etc  any incompatible change of state store is a major upgrade and any compatible change of state store is a minor upgrade  if theres s no version treat it as current version info  within a minor upgrade say   to   overwrite the version info and proceed as normal  within a major upgrade say   to   throw exception and indicate user to use a separate upgrade tool to upgrade rm state 
 returns the rm delegation token data from the link datainputstream as a link rmdelegationtokenidentifierdata it can handle both the current and old non protobuf formats 
 checks if parent znode has no leaf nodes and if it does not have removes it 
 remove application node and its attempt nodes 
 get alternate path for app id if path according to configured split index does not exist we look for path based on all possible split indices 
 returns leaf znode path based on node name and passed split index if the passed flag createparentifnotexists is true also creates the parent znode if it does not exist 
 returns leaf app node path based on app id and configured split index if the passed flag createparentifnotexists is true also creates the parent app node if it does not exist 
 returns leaf delegation token node path based on sequence number and configured split index if the passed flag createparentifnotexists is true also creates the parent znode if it does not exist the sequence number is padded to be at least  digits wide to ensure consistency with the split indexing 
 returns leaf delegation token node path based on sequence number and passed split index if the passed flag createparentifnotexists is true also creates the parent znode if it does not exist the sequence number is padded to be at least  digits wide to ensure consistency with the split indexing 
 get alternate path for delegation token if path according to configured split index does not exist we look for path based on all possible split indices 
 first sets entitlement of queues to zero to prevent new app submission then move all apps in the set of queues to the parent plan queue s default reservation queue if move is enabled finally cleanups the queue by killing any apps if move is disabled or move failed and removing the queue
 performs the actual allocation for a reservationdefinition within a plan 
 quick validation on the input to check some obvious fail conditions fail fast the input and returns the appropriate link plan associated withthe specified link queue or throws an exception message illustrating thedetails of any validation check failures
 quick validation on the input to check some obvious fail conditions fail fast the input and returns the appropriate link plan associated withthe specified link queue or throws an exception message illustrating thedetails of any validation check failures
 quick validation on the input to check some obvious fail conditions fail fast the input and returns the appropriate link plan associated withthe specified link queue or throws an exception message illustrating thedetails of any validation check failures 
 quick validation on the input to check some obvious fail conditions fail fast the input and returns the appropriate link plan associated withthe specified link queue or throws an exception message illustrating thedetails of any validation check failures
 merges the range start to end of two code rlesparseresourceallocation using a given code rleoperator 
 updateapplicationpriority will invoke scheduler api to update the new priority to rm and statestore 
 movetoqueue will invoke scheduler api to perform move queue operation 
 constructor for rmnminfo registers the bean with jmx 
 get appplacementallocator related to the placement type requested 
 this methods to change capacity for a queue and adjusts its absolutecapacity
 remove the specified child queue 
 schedule on all nodes by starting at a random point 
 initialized the queues 
 parse the queue from the configuration 
 asynchronously called from scheduler to apply queue management changes
 during the placement phase allocation tags are added to the node if the constraint is satisfied but depending on the order in which the algorithm sees the request it is possible that a constraint that happened to be valid during placement of an earlier seen request might not be valid after all subsequent requests have been placed for eg assume nodes n n n n and n consider the  constraints  foo anti affinity with foo  bar anti affinity with foo and  requests req numallocations    alloctags   foo req numallocations    alloctags   bar if req is seen first the algorithm can place the  containers in n n n and n and when it gets to req it will see that  nodes with the foo tag and will place on n but if req is seem first then bar will be placed on any node since no node currently has foo and when it gets to req since foo has not anti affinity with bar the algorithm can end up placing foo on a node with bar violating the second constraint to prevent the above we need a validation step after the placements for a batch of requests are made for each req we remove its tags from the node and try to see of constraints are still satisfied if the tag were to be added back on the node when applied to the example above after req and req are placed we remove the bar tag from the node and try to add it back on the node this time constraint satisfaction will fail since there is now a foo tag on the node and bar cannot be added the algorithm will then retry placing req on another node 
 check if given tag exists on node 
 returns true if all child constraints are satisfied 
 returns true as long as any of child constraint is satisfied 
 returns true if the placement constraint for a given scheduling request is currently satisfied by the specific scheduler node this method first validates the constraint specified in the request if not specified then it validates application level constraint if exists otherwise it validates the global constraint if exists this method only checks whether a scheduling request can be placed on a node with respect to the certain placement constraint it gives no guarantee that asked allocations can be eventually allocated because it doesn t check resource that needs to be further decided by a scheduler 
 loads a queue from a queue element in the configuration file 
 updates the allocation list from the allocation config file this file is expected to be in the xml format specified in the design doc 
 parses a resource config value of a form like   mb or  mb  vcores if no units are given megabytes are assumed 
 builds a queueplacementpolicy from an xml element 
 build a simple queue placement policy from the allow undeclared pools and user as default queue configuration options 
 param requestedqueue the queue explicitly requested 
 returns link schedulingpolicy instance corresponding to the link schedulingpolicy passed as a string the policy can be fair forfairsharepolicy fifo for fifopolicy or drf for dominantresourcefairnesspolicy for a custom link schedulingpolicy s in the rm classpath the policy should be canonical class name of the link schedulingpolicy 
 utility method to validate a resource request by insuring that the requested memory vcore is non negative and not greater than max
 create the actual applicationsubmissioncontext to be submitted to the rm from the information provided by the user 
 wait until an application has reached a specified state the timeout is  seconds 
 wait until an attempt has reached a specified state the timeout is  seconds 
 wait until an attempt has reached a specified state the timeout can be specified by the parameter 
 wait until an attempt has reached a specified state the timeout is  seconds 
 wait until an attempt has reached a specified state the timeout can be specified by the parameter 
 wait until a container has reached a specified state the timeout is  seconds 
 wait until a container has reached a specified state the timeout is specified by the parameter 
 wait until a container has reached a specified state the timeout is  seconds 
 wait until a container has reached a specified state the timeout is specified by the parameter 
 wait until a node has reached a specified state the timeout is  seconds 
 recommend to use launcham or use sendamlaunched like  wait rmappattempt scheduled  send node heartbeat  sendamlaunched
 note nm nodeheartbeat is explicitly invoked don t invoke it before calling launcham
 wait until an app removed from scheduler the timeout is  seconds 
 when failing over new active rm should read from current state of store including any updates when the new active rm was in standby 
 when failing over if rm stopped and removed a queue that rm has in memory failing over to rm should not throw an exception 
 test case verifies the order of applications activated after rm restart app  and app  submitted and scheduled and running with a priority  and  respectively app  submitted and scheduled with a priority  this is not activated since amresourcelimit is reached rm restarted app  get activated nevertheless of amresourcelimit app  and app  put in pendingorderingpolicy after nm registration app  is activated expected output app  must get activated since app  was running earlier 
 make sure scheduler skips nms which haven t heartbeat for a while 
 verify that you can t place queues at the same level as the root queue in the allocations file 
 verify that you can t include periods as the queue name in the allocations file 
 verify that you can t have the queue name with whitespace only in the allocations file 
 verify that you can t have the queue name with just a non breaking whitespace in the allocations file 
 verify that defaultqueueschedulingmode can t accept fifo as a value 
 verify that you can t have dynamic user queue and reservable queue on the same queue
 test fair shares when max resources are set but are too high to impact the shares 
 test fair shares when max resources are set and are low enough to impact the shares 
 test the child max resource settings 
 test if we compute the maximum am resource correctly 
 the test verifies that zero fairshare queues because of zero tiny weight can get resources for the am 
 the test verifies container gets reserved when not over maxamshare reserved container gets unreserved when over maxamshare container doesn t get reserved when over maxamshare reserved container is turned into an allocation and superfluously reserved container gets unreserved  create three nodes node is g node is g and node is g  app allocated g on node and app allocated g on node  app reserved g on node and node  app allocated g on node which makes app over maxamshare  remove app to make node have g available resource  app unreserved its container on node because it is over maxamshare  app allocated g on node after app unreserved its container  remove app  app failed to reserve a g container on node due to amshare limit  app allocated g on node  remove app and app  app reserved g on node and node  app failed to allocate a g container on node and node because app reserved node and node  remove app  app turned the g reservation into an allocation on node  app unreserved its container on node app allocated g on node 
 submit application to code queue and take over the entire cluster submit application with larger containers to code queue thatrequires preemption from the first application 
 setup the cluster for starvation testing  create fs allocation file  create and start mockrm  add two nodes to the cluster  submit an app that uses up all resources on the cluster
 test to verify that resourcerequests recovery back to the right app attempt after a container gets killed at acquired state yarn  
 validate that application tokens are unusable after the application finishes 
 validate master key roll over and that tokens are usable even after master key roll over 
 basic idea of the test  create tokens  mark one of them to be renewed in  seconds instead of  hours  register them for renewal  sleep for  seconds  count number of renewals should  initial ones  one extra  register another token for  seconds  cancel it immediately  sleep and check that the  seconds renew didn t happen totally  renewals  check cancellation
 basic idea of the test  register a token for  seconds with no cancel at the end  cancel it immediately  sleep and check that the  seconds renew didn t happen totally  renewals  check cancellation
 basic idea of the test  setup token keep alive  create tokens  register them for renewal to be cancelled on app complete  complete app  verify token is alive within the keep alive time  verify token has been cancelled after the keep alive time
 basic idea of the test  setup token keep alive  create tokens  register them for renewal to be cancelled on app complete  complete app  verify token is alive within the keep alive time  send an explicity keep alive request  verify token keep alive time is renewed  verify token has been cancelled after the renewed keep alive time 
 decommissioning using a pre configured include hosts file
 decommissioning using a pre configured exclude hosts file
 graceful decommission node with no running application 
 graceful decommission node with running application 
 decommissioning using a post configured include hosts file
 decommissioning using a post configured exclude hosts file
 remove a node from all lists and check if its forgotten 
 tests the successlog with two ip addresses 
 test that neutral mode plays well with all other transitions 
 helper method to test that neutral mode plays well with other transitions 
 helper method to test that neutral mode plays well with an active transition 
 helper method to test that neutral mode plays well with a standby transition 
 helper method to test that neutral mode plays well with itself 
 helper method to test that neutral mode does not race with an active transition 
 helper method to test that neutral mode does not race with an active transition 
 test to verify the following rm ha transitions to the following states  standby should be a no op  active active services should start  active should be a no op while active submit a couple of jobs  standby active services should stop  active active services should start  stop the rm all services should stop and rm should not be ready to become active
 test uam work preserving restart when the keepcontainersacrossattempt flag is on we allow uam to directly register again and move on without getting the applicationalreadyregistered exception 
 testing inconsistent state between abstractyarnscheduler applications and rmcontext applications
 test per user resources and resourcesused elements in the web services xml
 this method is used when a reservationid is required attempt to use rest api if authentication is not enabled ensure that the response status is unauthorized and generate a reservationid because downstream components require a reservationid for testing 
 yarn router forwards every getnewapplication requests to any rm during this operation there will be no communication with the state store the router will forward the requests to any subcluster the router will retry to submit the request on numsubmitretries different subclusters the subclusters are randomly chosen from the active ones possible failures and behaviors client identical behavior as code clientrmservice router the client will timeout and resubmit resourcemanager the router will timeout and contacts another rm statestore not in the execution 
 today in yarn there are no checks of any applicationid submitted base scenarios the client submits an application to the router the router selects one subcluster to forward the request the router inserts a tuple into statestore with the selected subcluster e g sc and the appid the state store replies with the selected subcluster e g sc the router submits the request to the selected subcluster in case of state store failure the client submits an application to the router the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid due to the state store down the router times out and it will retry depending on the federationfacade settings the router replies to the client with an error message if state store fails after inserting the tuple identical behavior as code clientrmservice in case of router failure scenario  crash before submission to the resourcemanager the client submits an application to the router the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid the router crashes the client timeouts and resubmits the application the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid because the tuple is already inserted in the state store it returns the previous selected subcluster e g sc the router submits the request to the selected subcluster e g sc scenario  crash after submission to the resourcemanager the client submits an application to the router the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid the router submits the request to the selected subcluster the router crashes the client timeouts and resubmit the application the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid the state store replies with the selected subcluster e g sc the router submits the request to the selected subcluster e g sc when a client re submits the same application to the same rm it does not raise an exception and replies with operation successful message in case of client failure identical behavior as code clientrmservice in case of resourcemanager failure the client submits an application to the router the router selects one subcluster to forward the request the router inserts a tuple into state store with the selected subcluster e g sc and the appid the router submits the request to the selected subcluster the entire subcluster is down all the rms in ha or the master rm is not reachable the router times out the router selects a new subcluster to forward the request the router update a tuple into state store with the selected subcluster e g sc and the appid the state store replies with ok answer the router submits the request to the selected subcluster e g sc 
 the yarn router will forward to the respective yarn rm in which the am is running possible failures and behaviors client identical behavior as code clientrmservice router the client will timeout and resubmit the request resourcemanager the router will timeout and the call will fail state store the router will timeout and it will retry depending on the federationfacade settings if the failure happened before the select operation 
 the yarn router will forward to the respective yarn rm in which the am is running possible failure client identical behavior as code clientrmservice router the client will timeout and resubmit the request resourcemanager the router will timeout and the call will fail state store the router will timeout and it will retry depending on the federationfacade settings if the failure happened before the select operation 
 yarn router forwards every getnewapplication requests to any rm during this operation there will be no communication with the state store the router will forward the requests to any subcluster the router will retry to submit the request on numsubmitretries different subclusters the subclusters are randomly chosen from the active ones possible failures and behaviors client identical behavior as code rmwebservices router the client will timeout and resubmit resourcemanager the router will timeout and contacts another rm statestore not in the execution 
 creates and performs a rest call to a specific webservice 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subcluster and  good one 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subcluster and a good one 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of getnewapplication in case the cluster is composed of only  bad subcluster and  good one 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of submitapplication in case the cluster is composed of only  bad subcluster and a good one 
 this test validates the correctness of getapps in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of getapps in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of getapps in case the cluster is composed of only  bad subcluster and a good one 
 this test validates the correctness of getnode in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of getnode in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of getnode in case the cluster is composed of only  bad subcluster and a good one 
 this test validates the correctness of getnodes in case the cluster is composed of only  bad subcluster 
 this test validates the correctness of getnodes in case the cluster is composed of only  bad subclusters 
 this test validates the correctness of getnodes in case the cluster is composed of only  bad subcluster and a good one 
 this test validates the correctness of getnodes in case the cluster is composed of only  bad subcluster the excepted result would be a clustermetricsinfo with all its values set to  
 this test validates the correctness of getclustermetrics in case the cluster is composed of only  bad subclusters the excepted result would be a clustermetricsinfo with all its values set to  
 this test validates the correctness of getclustermetrics in case the cluster is composed of only  bad subcluster and a good one the good subcluster provided a clustermetricsinfo with appssubmitted set to its subclusterid the expected result would be appsubmitted equals to its subclusterid subclusterid in this case is an integer 
 this test validates the correctness of link rmwebserviceprotocol get inside router 
 this test validates the correctness of link rmwebserviceprotocol getclusterinfo inside router 
 this test validates the correctness of link rmwebserviceprotocol getclustermetricsinfo inside router 
 this test validates the correctness of link rmwebserviceprotocol getnodes inside router 
 this test validates the correctness of link rmwebserviceprotocol getnodes inside router 
 this test validates the correctness of link rmwebserviceprotocol getnode inside router 
 this test validates the correctness of link rmwebserviceprotocol getactivities inside router 
 this test validates the correctness of link rmwebserviceprotocol getappactivities inside router 
 this test validates the correctness of link rmwebserviceprotocol getappstatistics inside router 
 this test validates the correctness of link rmwebserviceprotocol dumpschedulerlogs inside router 
 this test validates the correctness of link rmwebserviceprotocol createnewapplication inside router 
 this test validates the correctness of link rmwebserviceprotocol submitapplication inside router 
 this test validates the correctness of link rmwebserviceprotocol getapps inside router 
 this test validates the correctness of link rmwebserviceprotocol getapp inside router 
 this test validates the correctness of link rmwebserviceprotocol getappattempts inside router 
 this test validates the correctness of link rmwebserviceprotocol getappstate inside router 
 this test validates the correctness of link rmwebserviceprotocol updateappstate inside router 
 this test validates the correctness of link rmwebserviceprotocol getapppriority inside router 
 this test validates the correctness of link rmwebserviceprotocol updateapplicationpriority inside router 
 this test validates the correctness of link rmwebserviceprotocol getappqueue inside router 
 this test validates the correctness of link rmwebserviceprotocol updateappqueue inside router 
 this test validates the correctness of link rmwebserviceprotocol getapptimeouts inside router 
 this test validates the correctness of link rmwebserviceprotocol getapptimeout inside router 
 this test validates the correctness of link rmwebserviceprotocol updateapplicationtimeout inside router 
 this test validates the correctness of link rmwebserviceprotocol createnewreservation inside router 
 this test validates the correctness of link rmwebserviceprotocol submitreservation inside router 
 this test validates the correctness of link rmwebserviceprotocol updatereservation inside router 
 this test validates the correctness of link rmwebserviceprotocol deletereservation inside router 
 this test validates the correctness of link rmwebserviceprotocol getnodetolabels inside router 
 this test validates the correctness of link rmwebserviceprotocol getclusternodelabels inside router 
 this test validates the correctness of link rmwebserviceprotocol getlabelsonnode inside router 
 this test validates the correctness of link rmwebserviceprotocol getlabelstonodes inside router 
 this test validates the correctness of link rmwebserviceprotocol getlabelstonodes inside router 
 this test validates the correctness of link rmwebserviceprotocol addtoclusternodelabels inside router 
 this test validates the correctness of link rmwebserviceprotocol removefromclusernodelabels inside router 
 this test validates the correctness of link rmwebserviceprotocol replacelabelsonnodes inside router 
 this test validates the correctness of link rmwebserviceprotocol replacelabelsonnode inside router 
 this test validates the correctness of link webservices getappattempt inside router 
 this test validates the correctness of link webservices getcontainers inside router 
 get the number of applications in the system 
 sweeps and processes the shared cache area to clean up stale and orphaned files 
 processes a single shared cache resource directory 
 the in memory store bootstraps itself from the shared cache entries that exist in hdfs 
 create an instance of the appchecker service via reflection based on the link yarnconfiguration scm app checker class parameter 
 in an ha cluster go through all the rms and find the active rm in a non ha cluster return the index of the only rm 
 wait for all the nodemanagers to connect to the resourcemanager 
 run a series of tests using different nmtokens a configuration is provided for managing creating of the tokens and rpc 
 this tests a malice user getting a proper token but then messing with it by tampering with containerid resource etc his her containers should be rejected 
 this tests whether a containerid is serialized deserialized with epoch 
 make local dirs fail inaccessible and verify if nodemanager can recognize the disk failures properly and can update the list of local dirs accordingly with good disks also verify the overall health status of the node 
 make log dirs fail inaccessible and verify if nodemanager can recognize the disk failures properly and can update the list of log dirs accordingly with good disks also verify the overall health status of the node 
 make a local and log directory inaccessible during initialization and verify those bad directories are recognized and removed from the list of available local and log directories 
 wait for the nodemanger to go for the disk health check at least once 
 verify if the nodemanager could identify disk failures 
 prepare directory for a failure replace the given directory on the local filesystem with a regular file with the same name this would cause failure of creation of directory in diskchecker checkdir with the same name 
 simulates a nm heartbeat using the simulated nodestatus fixture verify both the rmnode and schedulernode have been updated with the new utilization 
 trigger the nm to send a heartbeat using the simulated nodestatus fixture verify both the rmnode and schedulernode have been updated with the new utilization 
 verify both the rmnode and schedulernode have been updated with the test fixture utilization data 
 main function for entity log cleaner this method performs depth first search from a given dir path for all application log dirs once found it will decide if the directory should be cleaned up and then clean them 
 get the application state 
 write timeline entities to a file system
 handles entity writes these writes are synchronous and are written to the backing storage without buffering batching if any entity already exists it results in an update of the entity this method should be reserved for selected critical entities and events for normal voluminous writes one should use the async method link putentitiesasync timelineentities usergroupinformation 
 handles entity writes in an asynchronous manner the method returns as soon as validation is done no promises are made on how quickly it will be written to the backing storage or if it will always be written to the backing storage multiple writes to the same entities may be batched and appropriate values updated and result in fewer writes to the backing storage 
 parses date range which can be a single date or in the format startdate enddate where either of start or end date may not exist 
 return a single entity for a given entity type and uid which is a delimited string containing clusterid userid flow name flowrun id and app id 
 return a set of entities that match the given parameters if userid flow name and flowrun id which are optional query parameters are not specified they will be queried based on app id and cluster id from the flow context information stored in underlying storage implementation if number of matching entities are more than the limit most recent entities till the limit is reached will be returned 
 return a single entity for given uid which is a delimited string containing clusterid userid flow name flowrun id app id entity type and entityid 
 return a single entity of the given entity type and id if userid flowname and flowrun id which are optional query parameters are not specified they will be queried based on app id and cluster id from the flow context information stored in underlying storage implementation 
 return a single flow run for given uid which is a delimited string containing clusterid userid flow name and flowrun id 
 return a single flow run for the given user cluster flow name and run id 
 return a list of flow runs for given uid which is a delimited string containing clusterid userid and flow name 
 return a set of flows runs for the given cluster user and flow name 
 return a list of active flows for a given cluster id 
 return a single app for given uid which is a delimited string containing clusterid userid flow name flowrun id and app id 
 return a single app for given cluster id and app id if userid flow name and flowrun id which are optional query parameters are not specified they will be queried based on app id and cluster id from the flow context information stored in underlying storage implementation 
 return a list of apps for given uid which is a delimited string containing clusterid userid flow name and flowrun id if number of matching apps are more than the limit most recent apps till the limit is reached will be returned 
 returns a set of available entity types for a given app id if userid flow name and flow run id which are optional query parameters are not specified they will be queried based on app id and cluster id from the flow context information stored in underlying storage implementation 
 parse the passed filters represented as strings and convert them into a link timelineentityfilters object 
 parse the passed filters represented as strings and convert them into a link timelineentityfilters object 
 parse the passed fields represented as strings and convert them into a link timelinedatatoretrieve object 
 parse a delimited string and convert it into a set of strings for instance if delimiter is then the string should be represented as value value value 
 parse relation filters 
 parses config and info filters 
 parses metric filters 
 parses confstoretrieve and metricstoretrieve 
 matches relatesto 
 matches isrelatedto 
 matches config filters 
 matches info filters 
 matches event filters 
 matches metric filters 
 common routine to match different filters iterates over a filter list and calls routines based on filter type 
 unit test for poc yarn  
 writes  timeline entities belonging to one flow run through the link hbasetimelinewriterimpl checks the flow run table contents the first entity has a created event metrics and a finish event the second entity has a created event and this is the entity with smallest start time this should be the start time for the flow run the third entity has a finish event and this is the entity with the max end time this should be the end time for the flow run the fourth entity has a created event which has a start time that is greater than min start time the test also checks in the flow activity table that one entry has been made for all of these  application entities since they belong to the same flow run 
 write  application entity and checks the record for today in the flow activity table 
 writes  applications each with a different run id and version for the same cluster user flow they should be getting inserted into one record in the flow activity table with  columns one per run id 
 writes two application entities of the same flow run each application has two metrics slot millis and hdfs bytes read each metric has values at two timestamps checks the metric values of the flow in the flow run table flow metric values should be the sum of individual metric values that belong to the latest timestamp for that metric
 create a new connection to the rm application history server to fetch application reports 
 create a direct connection to rm instead of a remote connection when the proxy is running as part of the rm also create a remote connection to application history server if it is enabled 
 get a proxied uri for the original uri 
 create a uri form a no scheme url such as is returned by the am 
 download link and have it be the response 
 the action against the http method 
 locate the tracking uri for the application based on the reported tracking uri if the reported uri is invalid redirect to the history server or rm app page if the uri is valid covert it into a usable uri object with a schema if the returned uri is null that means there was a redirect 
 fetch the application report from the rm 
 this method is used by java object deserialization to fill in the transient link trackinguriplugins field see link objectinputstream defaultreadobject do not remove yarn isn t currently serializing this class but findbugs complains in its absence 
 test main method of webappproxyserver
 set the bsp algorithm class for the job 
 some methods rely on having a recent job profile object refresh it if necessary
 returns immediately whether the whole job is done yet or not 
 true if job completed successfully 
 blocks until the job is finished
 tells the service to get the state of the current job 
 submit a job to the bsp system this returns a handle to the link runningjob which can be used to track the running job 
 read a splits file into a list of raw splits
 monitor a job and print status in real time as progress is made and tasks fail 
 get an runningjob object to track an ongoing job returns null if the id does not correspond to any known job 
 dump a list of currently running jobs
 dump a list of all jobs submitted 
 starts the bsp master process 
 starts the bsp master process
 bsppeer constructor bsppeer acts on behalf of clients performing bsp tasks 
 return all the splits in the specified set of paths
 read the binary representation of the counter
 write the binary representation of the counter
 write the set of groups the external format is groups groupname group i e the number of groups followed by  or more groups where each group is of the form groupdisplayname counters false  true counter where each counter is of the form name false  true displayname value
 read a set of groups 
 convert a stringified counter representation into a counter object note that the counter can be recovered if its stringified using link makeescapedcompactstring 
 command for bspmaster or groomserver to execute public abstract void execute throws exception 
 splits files returned by link liststatus bspjob when they re too big numsplits will be ignored by the framework 
 this function identifies and returns the hosts that contribute most for a given split for calculating the contribution rack locality is treated on par with host locality so hosts from racks that contribute the most are preferred over hosts on racks that contribute less
 remove the tip and update all relevant state 
 a child task had a fatal error kill the task 
 interpret a given string as a composite expression code func   func   tbl class   see java lang class forname java lang string path   see org apache hadoop fs path path java lang string reads expression from the mapred join expr property anduser supplied join types from mapred join define lt ident gt types paths supplied to tbl are given as input paths to the inputformat class listed 
 build a compositeinputsplit from the child inputformats by assigning the ith split from each child to the ith composite split 
 construct a compositerecordreader for the children of this inputformat as defined in the init expression the outermost join need only be composable not necessarily a composite mandating tuplewritable isn t strictly correct 
 inheritdoc 
 write the next value into key value as accepted by the operation associated with this set of recordreaders 
 if key provided matches that of this composite give joincollector iterator over values it may emit 
 close all child rrs 
 inheritdoc 
 return the node type registered for the particular identifier by default this is a cnode for any composite node and a wnode for quot wrapped quot nodes user nodes will likely be composite nodes 
 let the first actual define the inputformat and the second define the bsp input dir property 
 parse a list of comma separated nodes 
 given an expression and an optional comparator build a tree of inputformats using the comparator to sort keys 
 inheritdoc 
 for a given recordreader rr occupy position id in collector 
 skip key value pairs with keys less than or equal to the key provided 
 add an iterator to the collector at the position occupied by this recordreader over the values in this stream paired with the key provided ie register a stream of values from this source matching k with a collector 
 write key value pair at the head of this stream to the objects provided get next key value pair from proxied rr 
 decompresses a bspcompressedbundle and returns the corresponding bspmessagebundle 
 schedule job to designated groomserver s immediately 
 utility function to get byte array out of writable
 utility function to read writable object value from byte array 
 read value stored in the zookeeper node 
 writes data into the zookeeper node if the path does not exist the zookeeper node is created recursively and the value is stored in the node 
 clears all sub children of node bsproot
 clears all sub children of node rooted at path 
 read a log file from start to end positions the offsets may be negative in which case they are relative to the end of the file for example reader taskid kind   is the entire file and reader taskid kind   is the last  bytes 
 wrap a command in a shell to capture stdout and stderr to files if the taillength is  the entire output will be saved 
 wrap a command in a shell to capture stdout and stderr to files setup commands such as setting memory limit can be passed which will be executed before exec if the taillength is  the entire output will be saved 
 get the logs via http 
 build working environment and launch bsppeer processes 
 log process s stream 
 write the object to the byte stream handling text as a special case 
 create a status server on the given port the jsp scripts are taken from src main webapp 
 add a context
 start the server does not wait for the server to start 
 connect to the server and set up the io streams it then sends a header to the server 
 configure the client and connect to server
 receive a response this method is called with the received response message whenever new data is received from a server 
 make a call passing param to the ipc server running at address which is servicing the protocol protocol with the ticket credentials rpctimeout as timeout and conf as configuration for this connection returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 make a call passing param to the ipc server defined by remoteid returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 makes a set of calls in parallel each parameter is sent to the corresponding address when all values are available or have timed out or errored the collected results are returned in an array the array contains nulls for calls that timed out or errored 
 get a connection from the pool or create a new one and add it to the pool connections to a given connectionid are reused 
 param in
 param out
 get a proxy connection to a remote server
 get a proxy connection to a remote server
 get a proxy connection to a remote server
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object with the default socketfactory
 get versionedprotocol
 expert make multiple parallel calls to a set of servers 
 construct a server for a protocol implementation instance listening on a port and address 
 construct a server for a protocol implementation instance listening on a port and address 
 construct a server for a protocol implementation instance listening on a port and address with a secret manager 
 process a recieved message from client this method is called with the received message whenever new data is received from a client 
 send response data to client
 reads the connection header following version
 reads the received data create call object 
 setup response for the ipc call 
 read a byte from the stream send a ping if timeout on read retries if no failure is detected until a byte is read 
 read bytes into a buffer starting from offset off send a ping if timeout on read retries if no failure is detected until a byte is read 
 initiates a call by sending the parameter to the remote server note this is not called from the connection thread but by other threads 
 make a call passing param to the ipc server running at address returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 make a call passing param to the ipc server running at address with the ticket credentials returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 make a call passing param to the ipc server running at address which is servicing the protocol protocol with the ticket credentials and rpctimeout as timeout returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 make a call passing param to the ipc server running at address which is servicing the protocol protocol with the ticket credentials rpctimeout as timeout and conf as configuration for this connection returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 make a call passing param to the ipc server defined by remoteid returning the value throws exceptions if there are network problems or if the remote code threw an exception 
 deprecated use link call writable inetsocketaddress class usergroupinformation configuration instead
 deprecated use link call writable inetsocketaddress class usergroupinformation configuration instead
 makes a set of calls in parallel each parameter is sent to the corresponding address when all values are available or have timed out or errored the collected results are returned in an array the array contains nulls for calls that timed out or errored 
 get a connection from the pool or create a new one and add it to the pool connections to a given connectionid are reused 
 if this remote exception wraps up one of the lookuptypes then return this exception unwraps any ioexception 
 get a proxy connection to a remote server
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object that implements the named protocol talking to a server at the named address 
 construct a client side proxy object with the default socketfactory
 expert make multiple parallel calls to a set of servers 
 expert make multiple parallel calls to a set of servers 
 construct a server for a protocol implementation instance listening on a port and address 
 construct a server for a protocol implementation instance listening on a port and address 
 construct a server for a protocol implementation instance listening on a port and address with a secret manager 
 a convenience method to bind to a given address and report better exceptions if the address is not a valid host 
 constructs a server listening on the named port and address parameters passed must be of the named class the handlercount determines the number of handler threads that will be used to process calls 
 setup response for the ipc call 
 called for each call 
 authorize the incoming client connection 
 this is a wrapper around link writablebytechannel write bytebuffer if the amount of data is large it writes to channel in smaller chunks this is to avoid jdk from creating many direct buffers as the size of buffer increases this also minimizes extra copies in nio layer as a result of multiple write operations required to write a large buffer 
 this is a wrapper around link readablebytechannel read bytebuffer if the amount of data is large it writes to channel in smaller chunks this is to avoid jdk from creating many direct buffers as the size of bytebuffer increases there should not be any performance degredation 
 helper for link channelread readablebytechannel bytebuffer and link channelwrite writablebytechannel bytebuffer only one of readch or writech should be non null 
 download log file 
 read template file contents
 read template file contents
 configure plugins directory for monitoring groomserver 
 load jar from specified path 
 constructor for udp client setting up configuration and open datagramsocket 
 the heartbeat function signifying its existence 
 udp supervisor 
 load jar from plugin directory for executing task 
 start the child process to handle the task for us peer is not available now start child by using only the configuration e g pipespartitioner no peer is available at this time 
 start the child process to handle the task for us 
 clean up the child process and socket if exist 
 create a proxy object that will speak the binary protocol on a socket upward messages are passed on the specified handler and downward downward messages are public methods on this object 
 create a proxy object that will speak the binary protocol on a socket upward messages are passed on the specified handler and downward downward messages are public methods on this object 
 close the connection and shutdown the handler thread 
 write the given object to the stream if it is a intwritable longwritable floatwritable doublewritable text or byteswritable write it directly otherwise write it to a buffer and then write the length and data to the stream 
 read the given object from stream if it is a intwritable longwritable floatwritable doublewritable text or byteswritable read it directly otherwise read it to a buffer and then write the length and data to the stream 
 submit a job to the cluster all of the necessary modifications to the job to run under pipes are made to the configuration 
 submit a pipes job based on the command line arguments 
 add the files to hdfs
 add the jars from the given hdfs paths to the classpath
 gets the canonical hostname of this machine 
 gets a new inetsocketaddress from the given peername peername must contain a colon to distinct between host and port 
 create a socket address with the given host and port the hostname might be replaced with another host that was set via link netutils addstaticresolution string string the value of hadoop security token service use ip will determine whether the standard java host resolver is used or if the fully qualified resolver is used 
 checks if code host is a local host name and return link inetaddress corresponding to that address 
 this is a drop in replacement for link socket connect socketaddress int in the case of normal sockets that don t have associated channels this just invokes socket connect endpoint timeout if socket getchannel returns a non null channel connect is implemented using hadoop s selectors this is done mainly to avoid sun s connect implementation from creating thread local selectors since hadoop does not have control on when these are closed and could end up taking all the available file descriptors 
 like link netutils connect socket socketaddress int but also takes alocal address and port to bind the socket to 
 same as getinputstream socket socket getsotimeout from documentation for link getinputstream socket long returns inputstream for the socket if the socket has an associated socketchannel then it returns a link socketinputstream with the giventimeout if the socket does not have a channel link socket getinputstream is returned in the later case the timeoutargument is ignored and the timeout set with link socket setsotimeout int applies for reads any socket created using socket factories returned by link netutils must use this interface instead of link socket getinputstream 
 returns inputstream for the socket if the socket has an associated socketchannel then it returns a link socketinputstream with the giventimeout if the socket does not have a channel link socket getinputstream is returned in the later case the timeoutargument is ignored and the timeout set with link socket setsotimeout int applies for reads any socket created using socket factories returned by link netutils must use this interface instead of link socket getinputstream 
 same as getoutputstream socket  timeout of zero implies write will wait until data is available from documentation for link getoutputstream socket long returns outputstream for the socket if the socket has an associated socketchannel then it returns a link socketoutputstream with the giventimeout if the socket does not have a channel link socket getoutputstream is returned in the later case thetimeout argument is ignored and the write will wait until data is available any socket created using socket factories returned by link netutils must use this interface instead of link socket getoutputstream 
 returns outputstream for the socket if the socket has an associated socketchannel then it returns a link socketoutputstream with the giventimeout if the socket does not have a channel link socket getoutputstream is returned in the later case thetimeout argument is ignored and the write will wait until data is available any socket created using socket factories returned by link netutils must use this interface instead of link socket getoutputstream 
 read byte array written with a writableableutils vint prefix 
 read byte array written with a writableableutils vint prefix ioexception is converted to a runtimeexception 
 write byte array with a writableableutils vint prefix 
 write byte array to out with a vint length prefix 
 write a printable representation of a byte array non printable characters are hex escaped in the format x x eg x x etc
 creates a link groomserverthread call start on the returned thread to make it run 
 this is the method that adds the classed to the repository
 this is a driver for the example programs it looks at the first command line argument and tries to find an example program with that name if it is found it calls the main method in that class with the rest of the command line arguments 
 create an instance using class literal name and object values supplied 
 create an instance with corresponded class and object values supplied constructor
 create an instance with corresponded class and object values supplied constructor
 unpack a jar file into a directory 
 run a hama job jar if the main class is not in the jar s manifest then it must be provided on the command line 
 create a new input stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 same as socketinputstream socket getchannel socket getsotimeout create a new input stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 performs one io and returns number of bytes read or written it waits up to the specified timeout if the channel is not read before the timeout sockettimeoutexception is thrown 
 the contract is similar to link socketchannel connect socketaddress with a timeout 
 this is similar to link doio bytebuffer int except that it does notperform any io it just waits for the channel to be ready for io as specified in ops 
 waits on the channel with the given timeout using one of the cached selectors it also removes any cached selectors that are idle for a few seconds 
 takes one selector from end of lru list of free selectors if there are no selectors awailable it creates a new selector also invokes trimidleselectors 
 create a new ouput stream with the given timeout if the timeout is zero it will be treated as infinite timeout the socket s channel will be configured to be non blocking 
 transfers data from filechannel using link filechannel transferto long long writablebytechannel similar to readfully this waits till requested amount of data is transfered 
 parse zookeeper configuration from hama xml config and run a quorumpeer this method checks if we are in localmode to prevent zookeeper from starting 
 parse zookeeper configuration from hama xml config and run a quorumpeer 
 runs a shutdownable zookeeper main server this does not work with multiple quorums 
 make a properties object holding zookeeper config equivalent to zoo cfg if there is a zoo cfg in the classpath simply read it in otherwise parse the corresponding config options from the hama xml configs and generate the appropriate zookeeper properties 
 parse zookeeper s zoo cfg injecting hama configuration variables in this method is used for testing so we can pass our own inputstream 
 return the zk quorum servers string given zk properties returned by makezkprops
 param basedir
 function parses command line in standart form 
 important this can be a bottle neck problem can be here core writableutil convertspmvoutputtodensevector 
 each peer reads input dense vector 
 method which actually starts spmv 
 function parses command line in standart form 
 spmv produces a file which contains result dense vector in format of pairs of integer and double the aim of this method is to convert spmv output to format usable in subsequent computation dense vector it can be usable for iterative solvers important currently it is used in spmv it can be a bottle neck because all input needs to be stored in memory 
 the text file essentially should look like vertex id t n tab separated vertex id vertex value pairs e g  t  t  t   t  t  etc 
 simple test multiplying                           
 creates a new graph job with the given configuration and an exampleclass the exampleclass is used to determine the user s jar to distribute in the cluster this constructor sets the vertex id class to link text the vertex value class to link intwritable and the edge value class to link intwritable 
 do the main logic of a superstep namely checking if vertices are active feeding compute with messages and controlling combiners aggregators we iterate over our messages and vertices in sorted order that means that we need to seek the first vertex that has the same id as the iterated message 
 seed the vertices first with their own values in compute this is the first superstep after the vertices have been loaded 
 loads vertices into memory of each peer 
 parses the messages in every superstep and does actions according to flags in the messages 
 reads output the list of output records can be restricted to maxlines 
 reads input text files and writes it to a sequencefile 
 converting given inputs into compatible output and save
 inheritdoc 
 read a line 
 the user overrides the compute method which will be executed at each active vertex in every superstep
 test create  input vectors of dimension two input vectors         k    maxiterations    resulting center should be   
 connects to cm sets up container launch context for shell command and eventually dispatches the container start request to the cm 
 main entry point after a container has launched 
 invokes admin createnamespace and admin createtable to create a namespace with a table that has one column family 
 invokes admin disabletable admin deletetable and admin deletenamespace to disable delete table and delete namespace 
 invokes admin createnamespace and admin createtable to create a namespace with a table that has one column family 
 invokes admin disabletable admin deletetable and admin deletenamespace to disable delete table and delete namespace 
 add a column family to an existing table synchronous operation use link addcolumnfamilyasync tablename columnfamilydescriptor instead because itreturns a link future from which you can learn whether success or failure 
 modify an existing column family on a table synchronous operation use link modifycolumnfamilyasync tablename columnfamilydescriptor instead because itreturns a link future from which you can learn whether success or failure 
 compact all regions on the region server asynchronous operation in that this method requests that a compaction run and then it returns it does not wait on the completion of compaction it can take a while 
 turn the load balancer on or off 
 invoke the balancer will run the balancer and if regions to move it will go ahead and do the reassignments can not run for various reasons check logs 
 invoke the balancer will run the balancer and if regions to move it will go ahead and do the reassignments if there is region in transition force parameter of true would still run balancer can not run for other reasons check logs 
 turn region normalizer on or off 
 enable disable the catalog janitor 
 ask for a scan of the catalog table 
 enable disable the cleaner chore 
 get whole cluster status containing status about hbase version cluster id primary backup master s master s coprocessors live dead regionservers balancer regions in transition 
 get whole cluster metrics containing status about hbase version cluster id primary backup master s master s coprocessors live dead regionservers balancer regions in transition 
 return current master server name
 return current backup master list
 return current live region servers list
 get link regionmetrics of all regions hosted on a regionserver 
 helper that delegates to getclustermetrics getmastercoprocessornames 
 helper that delegates to getclustermetrics getmastercoprocessornames 
 take a snapshot without waiting for the server to complete that snapshot asynchronous only a single snapshot should be taken at a time or results may be undefined 
 execute a distributed procedure on a cluster 
 get the info port of the current master if one is available 
 turn the split or merge switches on or off 
 query the current state of the switch 
 add a new replication peer for replicating data to slave cluster 
 list dead region servers 
 the submitted task may be not accomplished at all if there are too many running tasks or other limits 
 log as much info as possible and if there is something to replay submit it again after a back off sleep 
 return pid of the current process if it can be extracted from jvm name or null 
 close the previous scanner and create a new scannercallable for the next scanner marked as protected only because testclientscanner need to override this method 
 contact the servers to load more link result s in the cache 
 param timetolive time to live of cell contents in seconds 
 retrieve a table implementation for accessing a table the returned table is not thread safe a new instance should be created for each using thread this is a lightweight operation pooling or caching of the returned table is neither required nor desired the caller is responsible for calling link table close on the returnedtable instance since    this method no longer checks table existence an exception will be thrown if the table does not exist only when the first operation is attempted 
 create a new connection instance using default hbaseconfiguration connection encapsulates all housekeeping for a connection to the cluster all tables and interfaces created from returned connection share zookeeper connection meta cache and connections to region servers and masters the caller is responsible for calling link connection close on the returned connectioninstance typical usage connection connection   connectionfactory createconnection table table   connection gettable tablename valueof mytable try table get finally table close connection close 
 create a new connection instance using the passed conf instance connection encapsulates all housekeeping for a connection to the cluster all tables and interfaces created from returned connection share zookeeper connection meta cache and connections to region servers and masters the caller is responsible for calling link connection close on the returned connectioninstance typical usage connection connection   connectionfactory createconnection conf table table   connection gettable tablename valueof mytable try table get finally table close connection close 
 create a new connection instance using the passed conf instance connection encapsulates all housekeeping for a connection to the cluster all tables and interfaces created from returned connection share zookeeper connection meta cache and connections to region servers and masters the caller is responsible for calling link connection close on the returned connectioninstance typical usage connection connection   connectionfactory createconnection conf table table   connection gettable tablename valueof mytable try table get finally table close connection close 
 create a new connection instance using the passed conf instance connection encapsulates all housekeeping for a connection to the cluster all tables and interfaces created from returned connection share zookeeper connection meta cache and connections to region servers and masters the caller is responsible for calling link connection close on the returned connectioninstance typical usage connection connection   connectionfactory createconnection conf table table   connection gettable tablename valueof table try table get finally table close connection close 
 create a new connection instance using the passed conf instance connection encapsulates all housekeeping for a connection to the cluster all tables and interfaces created from returned connection share zookeeper connection meta cache and connections to region servers and masters the caller is responsible for calling link connection close on the returned connectioninstance typical usage connection connection   connectionfactory createconnection conf table table   connection gettable tablename valueof table try table get finally table close connection close 
 return true if the master is running throws an exception otherwise
 search the hbase meta table for the hregionlocation info that contains the table and row we re seeking 
 create a stub try once only it is not typed because there is no common type to protobuf services nor their interfaces let the caller do appropriate casting 
 create a stub against the master retry if necessary 
 creates a short circuit connection that can bypass the rpc layer serialization deserialization networking etc when talking to a local server 
 get a unique key for the rpc stub to the given server 
 sleep for an expected amount of time this is nearly a copy of what the sleeper does but with the ability to know if you got interrupted while sleeping 
 advanced use only add an existing delete marker to this delete object 
 get up to the specified number of versions of each column 
 deprecated since   version and will be removed in   version use link gettabledescriptor tablename connection rpcretryingcallerfactory rpccontrollerfactory int int 
 delete tables matching the passed in pattern and wait on completion warning use this method carefully there is no prompting and the effect is immediate consider using link listtables java util regex pattern and link deletetable tablename 
 inheritdoc 
 param sn
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 compact a table asynchronous operation 
 compact an individual region asynchronous operation 
 inheritdoc 
 invoke region normalizer can not run for various reasons check logs 
 merge two regions synchronous operation note it is not feasible to predict the length of merge therefore this is for internal testing only 
 merge two regions asynchronous operation 
 merge two regions asynchronous operation 
 merge two regions asynchronous operation 
 split one region synchronous operation note it is not feasible to predict the length of split therefore this is for internal testing only 
 split one region synchronous operation 
 if the input is a region name it is returned as is if it s an encoded region name the corresponding region is found from meta and its region name is returned if we can t find any region in meta matching the input as either region name or encoded region name the input is returned as is we don t throw unknown region exception 
 check if table exists or not
 do a get with a timeout against the passed in future 
 param tablename
 roll the log writer i e when using a file system based write ahead log start writing log messages to a new file note that when talking to a version   hbase deployment the rolling is asynchronous this method will return as soon as the roll is requested and the return value will always be null additionally the named region server may schedule store flushes at the request of the wal handling the roll request when talking to a   or older hbase deployment the rolling is synchronous and the return value may be either null or a list of encoded region names 
 execute restore clone snapshot and wait for the server to complete blocking to check if the cloned table exists use link istableavailable it is not safe tocreate an htable instance to this table before it is available 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 return the table descriptor
 connect to peer and check the table descriptor on peer create the same table on peer when not exist throw an exception if the table already has replication enabled on any of the column families throw an exception if the table exists on peer cluster but descriptors are not same 
 set the table s replication switch if the table s replication switch is already not set 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 get the corresponding start keys and regions for an arbitrary range of keys 
 the underlying link htable must not be closed link table getscanner byte has other usage details 
 the underlying link htable must not be closed link table getscanner byte byte has other usage details 
 override that changes the link java util concurrent callable call exception from link exception to link ioexception it also does setup of an rpccontroller and calls through to the rpccall method which callers are expected to implement if rpccontroller is an instance of payloadcarryingrpccontroller we will set a timeout on it 
 return cellvisibility associated with cells in this mutation 
 produces a json object sufficient for description of a query in a debugging or logging context 
 produces a string representation of this operation it defaults to a json representation but falls back to a string representation of the fingerprint and details in the case of a json encoding failure 
 see link getmasterregionsizes connection rpccontrollerfactory rpcretryingcallerfactory int 
 see link getmasterquotastates connection rpccontrollerfactory rpcretryingcallerfactory int 
 see link getregionserverquotasnapshot clusterconnection rpccontrollerfactory int servername 
 override that changes exception from link exception to link ioexception it also does setup of an rpccontroller and calls through to the unimplemented call payloadcarryingrpccontroller method implement this method to add your rpc invocation 
 gets the start key from the specified region name 
 param bytes a pb regioninfo serialized with a pb magic prefix 
 serializes given regioninfo s as a byte array use this instead of link regioninfo tobytearray regioninfo whenwriting to a stream and you want to use the pb mergedelimitedfrom w o the delimiter pb reads to eof which may not be what you want link parsedelimitedfrom byte int int canbe used to read back the instances 
 parses an regioninfo instance from the passed in stream presumes the regioninfo was serialized to the stream with link todelimitedbytearray regioninfo 
 parses all the regioninfo instances from the passed in stream until eof presumes the regioninfo s were serialized to the stream with odelimitedbytearray 
 override that changes call exception from link exception to link ioexception also does set up of the rpccontroller 
 append the replicable table cf config of the specified peer
 remove some table cfs from table cfs config of the specified peer
 set the replicable table cf config of the specified peer
 loads the latest version of the specified column into the provided bytebuffer does not clear or flip the buffer 
 poll for the first completed task whether it is a success or execution exception 
 poll for the first successfully completed task whose completed order is in startindex endindex exclusive range
 poll for the nth completed task index starts from  the st  the second 
 get nbrows rows how many rpcs are made is determined by the link scan setcaching int setting or hbase client scanner caching in hbase site xml 
 this builds the implementation of link retryingcallerinterceptor that wespecify in the conf and returns the same to use link preemptivefastfailinterceptor set hbase client enable fast fail mode to true hbase client fast fail interceptor impl is defaulted to link preemptivefastfailinterceptor 
 param reload force reload of server location
 create a link rowmutations with the specified mutations 
 add a link put operation to the list of mutations
 add a link delete operation to the list of mutations
 currently only supports link put and link delete mutations 
 get the good or the remote exception if any throws the donotretryioexception 
 two responsibilities if the call is already completed by another replica stops the retries set the location to the right region depending on the replica 
 algo we put the query into the execution pool after x ms if we don t have a result we add the queries for the secondary replicas we take the first answer when done we cancel what s left cancelling means removing from the pool if the actual call was not started interrupting the call if it has started client side we need to take into account a call is not executed immediately after being put into the pool a call is a thread let s not multiply the number of thread by the number of replicas server side if we can cancel when it s still in the handler pool it s much better as a call can take some i o globally the number of retries timeout and so on still applies but it s per replica not global we continue until all retries are done or all timeouts are exceeded 
 securely bulk load a list of hfiles using client protocol 
 gets the link org apache hadoop hbase htabledescriptor table descriptor for this table 
 test for the existence of columns in the table as specified by the get this will return true if the get matches one or more keys false if not this is a server side call so it prevents any data from being transfered to the client 
 test for the existence of columns in the table as specified by the gets this will return an array of booleans each value will be true if the related get matches one or more keys false if not this is a server side call so it prevents any data from being transferred to the client 
 extracts certain cells from a given row 
 puts some data in the table 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the put if the passed value is null the check is for the lack of column ie non existance 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the put if the passed value is null the check is for the lack of column ie non existence the expected value argument of this call is on the left and the current value of the cell is on the right side of the comparison operator ie eg greater operator means expected value  existing add the put 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the put if the passed value is null the check is for the lack of column ie non existence the expected value argument of this call is on the left and the current value of the cell is on the right side of the comparison operator ie eg greater operator means expected value  existing add the put 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the delete if the passed value is null the check is for the lack of column ie non existance 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the delete if the passed value is null the check is for the lack of column ie non existence the expected value argument of this call is on the left and the current value of the cell is on the right side of the comparison operator ie eg greater operator means expected value  existing add the delete 
 atomically checks if a row family qualifier value matches the expected value if it does it adds the delete if the passed value is null the check is for the lack of column ie non existence the expected value argument of this call is on the left and the current value of the cell is on the right side of the comparison operator ie eg greater operator means expected value  existing add the delete 
 see link incrementcolumnvalue byte byte byte long durability the link durability is defaulted to link durability sync wal 
 atomically increments a column value if the column value already exists and is not a big endian long this could throw an exception if the column value does not yet exist it is initialized to amount and written to the specified column setting durability to link durability skip wal means that in a failscenario you will lose any increments that have not been flushed 
 the input should be created by link tobytearray 
 deprecated as of release    this will be removed in hbase    use link setcoprocessor string instead
 deprecated as of release    this will be removed in hbase    use link setcoprocessor coprocessordescriptor instead
 deprecated as of release    this will be removed in hbase    use link setcoprocessor coprocessordescriptor instead
 add a table coprocessor to this table the coprocessor type must be org apache hadoop hbase coprocessor regionobserver or endpoint it won t check if the class can be loaded or not whether a coprocessor is loadable or not will be determined when a region is opened 
 add a table coprocessor to this table the coprocessor type must be org apache hadoop hbase coprocessor regionobserver or endpoint it won t check if the class can be loaded or not whether a coprocessor is loadable or not will be determined when a region is opened 
 add a table coprocessor to this table the coprocessor type must be org apache hadoop hbase coprocessor regionobserver or endpoint it won t check if the class can be loaded or not whether a coprocessor is loadable or not will be determined when a region is opened 
 for hadoop ipc internal call do not use we have to parse the hostname to recreate the exception the input is the one generated by link getmessage 
 a way to filter based on the column family column qualifier and or the column value return code is described below this allows filters to filter only certain number of columns then terminate without matching ever column if filterrowkey returns true filtercell needs to be consistent with it filtercell can assume that filterrowkey has already been called for the row if your filter returns returncode next row it should return returncode next row until link reset is called just in case the caller callsfor the next row concrete implementers can signal a failure condition in their code by throwing an link ioexception 
 filters that do not filter by row key can inherit this implementation that never filters anything ie returns false inheritdoc 
 parses the filterstring and constructs a filter using it 
 parses the filterstring and constructs a filter using it 
 constructs a filter object given a simple filter expression 
 param bytes a pb regioninfo serialized with a pb magic prefix 
 parses an hregioninfo instance from the passed in stream presumes the hregioninfo was serialized to the stream with link todelimitedbytearray 
 parses all the hregioninfo instances from the passed in stream until eof presumes the hregioninfo s were serialized to the stream with link todelimitedbytearray 
 encapsulate the ugly casting and runtimeexception conversion in private method 
 encapsulate the ugly casting and runtimeexception conversion in private method 
 get a connection from the pool or create a new one and add it to the pool connections to a given host port are reused 
 returns the parameter passed to link run object or code null if a null value waspassed when used asynchronously this method will block until the link run object method has been called 
 reads the call from the queue write them on the socket 
 handle connection failures if the current number of retries is equal to the max number of retries stop retrying and throw the exception otherwise backoff n seconds and try connecting again this method is only called from inside setupiostreams which is synchronized hence the sleep is synchronized the locks will be retained 
 if multiple clients with the same principal try to connect to the same server at the same time the server assumes a replay attack is in progress this is a feature of kerberos in order to work around this what is done is that the client backs off randomly and tries to initiate the connection again the other problem is to do with ticket expiry to handle that a relogin is attempted the retry logic is governed by the link shouldauthenticateoverkrb method in case when theuser doesn t have valid credentials we don t need to retry from cache or ticket in such cases it is prudent to throw a runtime exception when we receive a saslexception from the underlying authentication implementation so there is no retry from other high level for eg hcm or hbaseadmin 
 write the rpc header code 
 write the connection header 
 initiates a call by sending the parameter to the remote server note this is not called from the connection thread but by other threads 
 puts cellscanner cells into a cell block using passed in codec and or compressor 
 puts cellscanner cells into a cell block using passed in codec and or compressor 
 param codec to use for cellblock
 param codec to use for cellblock
 write out header param and cell block if there is one 
 lists all of the table regions currently in meta deprecated keep there until some test use this 
 performs a full scan of hbase meta for regions 
 performs a full scan of hbase meta for regions 
 performs a full scan of hbase meta for tables 
 performs a full scan of hbase meta 
 gets the region info and assignment for the specified region 
 returns the hregionlocation from meta for the given region
 returns the hregionlocation from meta for the given region
 returns an hri parsed from this regionname not all the fields of the hri is stored in the name so the returned object should only be used for the fields in the regionname 
 gets the result in hbase meta for the specified region 
 get regions from the merge qualifier of the specified merged region
 checks if the specified table exists looks at the hbase meta table hosted on the specified server 
 lists all of the regions currently in meta 
 gets all of the regions of the specified table do not use this method to get meta table regions use methods in metatablelocator instead 
 gets all of the regions of the specified table do not use this method to get meta table regions use methods in metatablelocator instead 
 do not use this method to get meta table regions use methods in metatablelocator instead 
 do not use this method to get meta table regions use methods in metatablelocator instead 
 param connection connection we re using
 performs a scan of meta table for given table starting from given row 
 performs a scan of meta table 
 return get closest metatable region row to passed row
 fetch table state for given table from meta table
 fetch table states from meta table
 updates state in meta
 count regions in hbase meta for passed table 
 generates and returns a put containing the region into for the catalog table
 put the passed p to the hbase meta table 
 param t table to use
 put the passed ps to the hbase meta table 
 delete the passed d from the hbase meta table 
 delete the passed deletes from the hbase meta table 
 deletes some replica columns corresponding to replicas for the passed rows
 execute the passed mutations against hbase meta table 
 adds daughter region infos to hbase meta row for the specified region note that this does not add its daughter s as different rows but adds information about the daughters in the same row as the parent use link splitregion connection regioninfo regioninfo regioninfo servername int if you want to do that 
 adds a single hbase meta row for the specified new region and its daughters note that this does not add its daughter s as different rows but adds information about the daughters in the same row as the parent use link splitregion connection regioninfo regioninfo regioninfo servername int if you want to do that 
 adds a hbase meta row for each of the specified new regions initial state for new regions is closed 
 adds a hbase meta row for each of the specified new regions initial state for new regions is closed 
 merge the two regions into one in an atomic operation deletes the two merging regions in hbase meta and adds the merged region with the information of two merging regions 
 splits the region into two in an atomic operation offlines the parent region with the information that it is split into two and also adds the daughter regions does not add the location information to the daughter regions since they are not open yet 
 update state of the table in meta 
 remove state for table from meta
 performs an atomic multi mutate operation against the given table 
 updates the location of the specified region in hbase meta to be the specified server hostname and startcode uses passed catalog tracker to get a connection to the server hosting hbase meta and makes edits to that region 
 updates the location of the specified region to be the specified server connects to the specified server which should be hosting the specified catalog region name to perform the edit 
 deletes the specified region from meta 
 deletes the specified regions from meta 
 deletes the specified regions from meta 
 overwrites the specified regions from hbase meta deletes old rows for the given regions and adds new ones regions added back have state closed 
 deletes merge qualifiers for the specified merged region 
 convert a protocol buffer get to a client get
 convert a protocol buffer mutate to a put 
 convert a protocol buffer mutate to a put 
 convert a protocol buffer mutate to a delete
 convert a protocol buffer mutate to an append
 convert a protocol buffer mutate to an increment
 convert a mutaterequest to mutation
 convert a protocol buffer mutate to a get 
 convert a client scan to a protocol buffer scan
 convert a protocol buffer scan to a client scan
 create a protocol buffer get based on a client get 
 create a protocol buffer mutate based on a client mutation
 create a protocol buffer mutationproto based on a client mutation does not include data understanding is that the cell will be transported other than via protobuf 
 create a protocol buffer mutationproto based on a client mutation does not include data understanding is that the cell will be transported other than via protobuf 
 convert a protocol buffer comparator to a bytearraycomparable
 convert a protocol buffer filter to a client filter
 convert a stringified protocol buffer exception parameter to a java exception
 open a quotaretriever with no filter all the quota settings will be returned 
 open a quotaretriever with the specified filter 
 fetches the computed size of all snapshots against tables in a namespace for space quotas 
 fetches any persisted hbase snapshot sizes stored in the quota table the sizes here are computed relative to the table which the snapshot was created from a snapshot s size will not include the size of files which the table still refers these sizes in bytes are what is used internally to compute quota violation for tables and namespaces 
 create a hbasesaslrpcclient for an authentication method
 protect a key by encrypting it with the secret key of the given subject the configuration must be set up correctly for key alias resolution 
 unwrap a key by decrypting it with the secret key of the given subject the configuration must be set up correctly for key alias resolution 
 unwrap a wal key by decrypting it with the secret key of the given subject the configuration must be set up correctly for key alias resolution 
 helper to create an encyption context 
 helper for link unwrapkey configuration string byte which automatically uses theconfigured master and alternative keys rather than having to specify a key type to unwrap with the configuration must be set up correctly for key alias resolution 
 do client side sasl authentication with server via the given inputstream and outputstream
 convert a protocol buffer get to a client get
 convert a protocol buffer mutate to a put 
 convert a protocol buffer mutate to a put 
 convert a protocol buffer mutate to a delete
 convert a protocol buffer mutate to an append
 convert a protocol buffer mutate to an increment
 convert a mutaterequest to mutation
 convert a client scan to a protocol buffer scan
 convert a protocol buffer scan to a client scan
 create a protocol buffer get based on a client get 
 create a protocol buffer mutate based on a client mutation
 create a protocol buffer mutationproto based on a client mutation does not include data understanding is that the cell will be transported other than via protobuf 
 create a protocol buffer mutationproto based on a client mutation does not include data understanding is that the cell will be transported other than via protobuf 
 a helper to get the all the online regions on a region server using admin protocol 
 a helper to get the list of files of a column family on a given region using admin protocol 
 param m message to get delimited pb serialization of with pb magic prefix 
 convert a protocol buffer cellvisibility bytes to a client cellvisibility
 convert a protocol buffer authorizations bytes to a client authorizations
 this version of protobuf s mergedelimitedfrom avoid the hard coded mb limit for decoding buffers
 contain serviceexception inside here take a callable that is doing our pb rpc and run it 
 create a protocol buffer mutaterequest for a conditioned put
 create a protocol buffer mutaterequest for a conditioned delete
 create a protocol buffer multi request for a list of actions propagates actions original index the passed in multirequestbuilder will be populated with region actions 
 create a protocol buffer multirequest with no data for a list of actions data is carried otherwise than via protobuf this means it just notes attributes whether to write the wal etc and the presence in protobuf serves as place holder for the data which is coming along otherwise note that get is different it does not contain data and is always carried by protobuf we return references to the data by adding them to the passed in data param propagates actions original index the passed in multirequestbuilder will be populated with region actions 
 get the results from a protocol buffer multiresponse
 param w writable
 put a bunch of writables as bytes all into the one byte array 
 set bytes into the passed writable by calling its link writable readfields java io datainput 
 copy one writable to another copies bytes using data streams 
 copy one writable to another copies bytes using data streams 
 get a zookeeper instance and wait until it connected before returning 
 this test simulates multiple regions on  servers we should have  multi requests and  threads  per server this whatever the number of regions 
 test and make sure we could use a special pause setting when retry with callqueuetoobigexception see hbase 
 tests the case where all replicas of a region throw an exception it should not cause a hang but the exception should propagate to the client
 make sure that we validate the snapshot name and the table name before we pass anything across the wire
 for running a few tests of methods herein 
 writes the cell s key part as it would have serialized in a keyvalue the format is lt  bytes rk len gt lt rk gt lt  byte cf len gt lt cf gt lt qualifier gt lt  bytes timestamp gt lt  byte type gt 
 writes the row from the given cell to the output stream excluding the common prefix
 write rowkey excluding the common part 
 write this cell to an outputstream in a link keyvalue format keyvalue format lt  bytes keylength gt lt  bytes valuelength gt lt  bytes rowlength gt lt row gt lt  byte columnfamilylength gt lt columnfamily gt lt columnqualifier gt lt  bytes timestamp gt lt  byte keytype gt lt value gt lt  bytes tagslength gt lt tags gt 
 return whether to show hbase configuration in servlet
 get the password from the configuration instance using the getpassword method if it exists if not then fall back to the general get method for configuration elements 
 generates a link configuration instance by applying the zookeeper cluster keyto the base configuration note that additional configuration properties may be needed for a remote cluster so it is preferable to use link createclusterconf configuration string string 
 generates a link configuration instance by applying property overrides prefixed bya cluster profile key to the base configuration override properties are extracted by the link subset configuration string method then the merged on top of the baseconfiguration and returned 
 writes the complete contents of this byte buffer output stream to the specified output stream argument 
 decompresses data from the given stream using the configured compression algorithm it will throw an exception if the dest buffer does not have enough space to hold the decompressed data 
 override because certain implementation calls def end which causes problem when resetting the stream for reuse 
 re implement because the relative method in jdk is invisible 
 re implement because the relative method in jdk is invisible 
 encrypts input data the result composes of msg padding if needed mac and sequence num 
 decrypts input data the input composes of msg padding if needed mac and sequence num the result is msg 
 return the md digest of the concatenation of the supplied arguments 
 return the md digest of the concatenation of the supplied arguments 
 return the sha  digest of the concatenation of the supplied arguments 
 return the sha  digest of the concatenation of the supplied arguments 
 return a  bit key derived from the concatenation of the supplied arguments using pbkdfwithhmacsha at   iterations 
 return a  bit key derived from the concatenation of the supplied arguments using pbkdfwithhmacsha at   iterations 
 encrypt a block of plaintext the encryptor s state will be finalized it should be reinitialized or returned to the pool 
 encrypt a block of plaintext
 encrypt a stream of plaintext given an encryptor the encryptor s state will be finalized it should be reinitialized or returned to the pool 
 encrypt a stream of plaintext given a context and iv
 decrypt a block of ciphertext read in from a stream with the given cipher and context the decryptor s state will be finalized it should be reinitialized or returned to the pool 
 decrypt a block of ciphertext from a stream given a context and iv
 decrypt a stream of ciphertext given a decryptor
 decrypt a stream of ciphertext given a context and iv
 encrypts a block of plaintext with the symmetric key resolved for the given subject
 decrypts a block of ciphertext with the symmetric key resolved for the given subject
 writes id in bytes 
 find the size of compressed data assuming that buffer will be compressed using given algorithm 
 estimate size after second stage of compression e g lzo 
 do the encoding but do not cache the encoded data 
 inheritdoc 
 inheritdoc 
 inheritdoc 
 read bytes starting from the specified position this requires rawstream is an instance of link positionedreadable 
 reads a magic record of the length link magic length from the givenstream and expects it to match this block type 
 compress tags one by one and writes to the outputstream 
 uncompress tags from the inputstream and writes to the destination array 
 uncompress tags from the inputstream and writes to the destination buffer 
 helper methods to write the dictionary data to the outputstream
 helper methods to write the dictionary data to the outputstream
 write keyvalue format into the provided byte array 
 param in where to read bytes from creates a byte array to hold the keyvaluebacking bytes copied from the steam 
 create a keyvalue reading length from in
 write out a keyvalue in the manner in which we used to when keyvalue was a writable 
 write out a keyvalue in the manner in which we used to when keyvalue was a writable but do not require a link dataoutput just take plain link outputstream named oswrite so does not clash with link write keyvalue dataoutput 
 create a keyvalue for the specified row family and qualifier that would be smaller than all other possible keyvalues that have the same row family qualifier used for seeking 
 create a keyvalue reading from the raw inputstream named createkeyvaluefrominputstream so doesn t clash with link create datainput 
 param in where to read bytes from creates a byte array to hold the keyvalue backing bytes copied from the steam 
 create a keyvalue reading length from in
 made into a static method so as to reuse the logic within valueandtagrewritebytebufferextendedcell
 write rowkey excluding the common part 
 writes the row from the given cell to the output stream excluding the common prefix
 writes the cell s key part as it would have serialized in a keyvalue the format is lt  bytes rk len gt lt rk gt lt  byte cf len gt lt cf gt lt qualifier gt lt  bytes timestamp gt lt  byte type gt 
 writes the cell to the given outputstream
 sets the given timestamp to the cell iff current timestamp is link hconstants latest timestamp 
 sets the given timestamp to the cell iff current timestamp is link hconstants latest timestamp 
 writes the row from the given cell to the output stream
 writes the family from the given cell to the output stream
 writes the qualifier from the given cell to the output stream
 writes the qualifier from the given cell to the output stream excluding the common prefix
 writes the value from the given cell to the output stream
 writes the tag from the given cell to the output stream
 returns the code user instance within current execution context 
 executes the given action as the login user
 log in the current process using the given configuration keys for the credential file and login principal this is only applicable when running on secure hadoop see org apache hadoop security securityutil login configuration string string string on regular hadoop without security features this will safely be ignored 
 return the username for the current logged in user 
 param hostandport string in form of lt hostname gt lt port gt 
 check that the object does not exist already there are two reasons for creating the objects only once  with k regions the table names take mb  equals becomes much faster as it s resolved with a reference and an int comparison 
 parse a number and enforce a range 
 param hostandport formatted as lt hostname gt lt port gt 
 we allocate a number of byte buffers as the capacity in order not to out of the array bounds for the last byte see link bytebufferarray multiple we will allocate one additional buffer with capacity  
 put in buffer integer using  bit encoding for each written byte  bits are used to store value  bit is used to indicate whether there is next bit 
 copy the data to the output stream and update position in buffer 
 copy data from a buffer to an output stream does not update the position in the buffer 
 copy data from a buffer to an output stream does not update the position in the buffer 
 read integer from stream coded in  bits and increment position 
 read long which was written to fitinbytes bytes and increment position 
 copy the given number of bytes from the given stream and put it at the current position of the given buffer updating the position in the buffer 
 read byte array written with a writableableutils vint prefix ioexception is converted to a runtimeexception 
 write byte array with a writableableutils vint prefix 
 this method will convert utf encoded bytes into a string if the given byte array is null this method will return null 
 this method will convert utf encoded bytes into a string if the given byte array is null this method will return null 
 converts a string to a utf  byte array 
 writes a string as a fixed size field padded with zeros 
 reads a fixed size field and interprets it as a string padded with zeros 
 equivalent of link class forname string which also returns classes forprimitives like boolean etc 
 in hbase  we found that concurrenthashmap get is much faster than computeifabsent if the value already exists so here we copy the implementation of link concurrentmap computeifabsent object java util function function it uses get and putifabsent to implement computeifabsent and notice that the implementation does not guarantee that the supplier will only be executed once 
 return the number of bytes that large input files should be optimally be split into to minimize i o time use reflection to search for getdefaultblocksize path f if the method doesn t exist fall back to using getdefaultblocksize 
 create the specified file on the filesystem by default this will apply the umask in the configuration if it is enabled use the fs configured buffer size or  if not set use the default replication use the default block size not track progress 
 verifies root directory path is a valid uri with a scheme
 checks for the presence of the wal log root path using the provided conf object in the given path if it exists this method removes it and returns the string representation of remaining relative path 
 param c configuration
 returns the wal region directory based on the given table name and region name
 returns the table directory under the walrootdir for the specified table name
 param conf must not be null
 calls fs liststatus and treats filenotfoundexception as non fatal this would accommodates differences between hadoop versions
 log the current state of the filesystem from a certain root directory
 recursive helper to log the state of the fs
 attempt to use builder api via reflection to create a file with the given parameters and replication enabled 
 attempt to use builder api via reflection to create a file with the given parameters and replication enabled 
 attempt to use builder api via reflection to create a file with the given parameters and replication enabled will not attempt to enable replication when passed an hfilesystem 
 attempt to use builder api via reflection to create a file with the given parameters and replication enabled will not attempt to enable replication when passed an hfilesystem 
 if our filesystem version includes the streamcapabilities class check if the given stream has a particular capability 
 get a coprocessorclassloader for a coprocessor jar path from cache if not in cache create one 
 wrapper around dns getdefaulthost string string calling dns getdefaulthost string string boolean when available 
 blocks until the lock corresponding to the given id is acquired 
 blocks until the lock corresponding to the given id is acquired 
 compute the hash of the specified file
 return return non zero if failed to find bean 
 dump out all registered mbeans as json on system out 
 returns a subset of mbeans defined by qry modeled after dumpregionservermetrics dumpmetrics example string qry  java lang type memory 
 load the implementation of unixoperatingsystemmxbean for oracle jvm and runs the desired method 
 get the number of opened filed descriptor for the runtime jvm if oracle java it will use the com sun management interfaces otherwise this methods implements it linux only 
 workaround to get the current number of process running approach is the one described here http stackoverflow com questions  how to get a list of current open windows process with java
 get the number of the maximum file descriptors the system can use if oracle java it will use the com sun management interfaces otherwise this methods implements it linux only 
 given a byte array returns its md hash as a hex string only length number of bytes starting at offset within the byte array are used 
 convert a human readable string to its value 
 convert a human readable time interval to seconds examples of the human readable time intervals are  days  hour  minutes  seconds etc the units of time specified can be in uppercase as well as lowercase also if a single number is specified without any time unit it is assumed to be in seconds 
 log the current thread stacks at info level 
 get and invoke the target method from the given object with given parameters
 sleep for a back off time as supplied by the backoff policy and increases the attempts
 sleep for period adjusted by passed starttime
 shutdown passed thread using isalive and join 
 param t waits on the passed thread to die dumping a threaddump everyminute while its up 
 if interrupted just prints out the interrupt on stdout resets interrupt and returns
 sleeps for the given amount of time even if interrupted preserves the interrupt status 
 returns the version components as integer and string objects examples    returns       snapshot returns    snapshot 
 verifies that the given key matches the expected format for a zookeeper cluster key the quorum for the zk cluster can have one the following formats see examples below s s s no client port in the list the client port could be obtained from clientport s p s p s p with client port which could be same or different for each server in this case the clientport would be ignored s p s s p mix of  and  if port is not specified in a server it would use the clientport otherwise it would use the specified port 
 separate the given key into the three configurations it should contain hbase zookeeper quorum hbase zookeeper client port and zookeeper znode parent
 finds the classes in current package of classfinder and nested packages 
 finds the classes in a package and nested packages 
 return true if we removed the test dirs
 param subdir test subdir name 
 was overflowing if k or so lists of cellscanners to return 
 choreservices should never have a core pool size that exceeds the number of chores that have been scheduled with the service for example if  scheduledchores are scheduled with a choreservice the number of threads in the choreservice s core pool should never exceed 
 compiles the test class with bogus code into a class file unfortunately it s very tedious 
 makes a jar out of some class files unfortunately it s very tedious 
 determine if we can load the necessary credentialprovider classes only loaded the first time so subsequent invocations of this method should return fast 
 wrapper to fetch the configured code list s 
 create a credentialentry using the configured providers if multiple credentialproviders are configured the first will be used 
 create a credentialentry with the give name and credential in the credentialprovider the credentialprovider argument must be an instance of hadoop credentialprovider 
 verify keyvalue format related functions write and getserializedsize should have the same behaviors as link keyvalue 
 jar a list of files into a jar archive 
 create a test jar for testing purpose for a given class name with specified code string save the class to a file compile it and jar it up if the code string passed in is null a bare empty class will be created and used 
 create a test jar for testing purpose for a given class name with specified code string 
 add a list of jar files to another jar file under a specific folder it is used to generated coprocessor jar files which can be loaded by the coprocessor class loader it is for testing usage only so we don t be so careful about stream closing in case any exception 
 test  bit encoding of integers 
 makes the current thread sleep for the duration equal to the specified time in milliseconds multiplied by the link getwaitforratio configuration 
 waits up to the duration equal to the specified timeout multiplied by the link getwaitforratio configuration for the given link predicate to becometrue failing the test if the timeout is reached and the predicate is still false 
 waits up to the duration equal to the specified timeout multiplied by the link getwaitforratio configuration for the given link predicate to becometrue failing the test if the timeout is reached and the predicate is still false 
 waits up to the duration equal to the specified timeout multiplied by the link getwaitforratio configuration for the given link predicate to becometrue failing the test if the timeout is reached the predicate is still false and failiftimeout is set as true 
 get an instance of the argument type declared in a class s signature the argument type is assumed to be a pb message subclass and the instance is created using parsefrom method on the passed bytestring 
 gives the maximum for a given combination of column qualifier and column family in the given row range as defined in the scan object in its current implementation it takes one column family and one column qualifier if provided in case of null column qualifier maximum value for the entire column family will be returned 
 gives the minimum for a given combination of column qualifier and column family in the given row range as defined in the scan object in its current implementation it takes one column family and one column qualifier if provided in case of null column qualifier minimum value for the entire column family will be returned 
 gives the sum for a given combination of column qualifier and column family in the given row range as defined in the scan object in its current implementation it takes one column family and one column qualifier if provided in case of null column qualifier sum for the entire column family will be returned 
 gives a pair with first object as sum and second object as row count computed for a given combination of column qualifier and column family in the given row range as defined in the scan object in its current implementation it takes one column family and one column qualifier if provided in case of null column qualifier an aggregate sum over all the entire column family will be returned the average is computed in aggregationclient avg byte columninterpreter scan by processing results from all regions so its ok to pass sum and a long type 
 gives a pair with first object a list containing sum and sum of squares and the second object as row count it is computed for a given combination of column qualifier and column family in the given row range as defined in the scan object in its current implementation it takes one column family and one column qualifier if provided the idea is get the value of variance first the average of the squares less the square of the average a standard deviation is square root of variance 
 gives a list containing sum of values and sum of weights it is computed for the combination of column family and column qualifier s in the given row range as defined in the scan object in its current implementation it takes one column family and two column qualifiers the first qualifier is for values column and the second qualifier optional is for weight column 
 check some of the methods and make sure we are incrementing each time its a bit tediuous to cover all methods here and really is a bit brittle since we can always add new methods but won t be sure to add them here so we just can cover the major ones 
 sets the security firstly for getting the correct default realm 
 test the exportendpoint s access levels the link export test is ignoredsince the access exceptions cannot be collected from the mappers 
 main entry point 
 unfortunately hadoop tries to be too clever and permanently keeps track of all names registered so far as a source thus preventing further re registration of the source with the same name in case of dynamic metrics tied to region lifecycles this becomes a problem because we would like to be able to re register and remove with the same name otherwise it is resource leak this ugly code manually removes the name from the uniquenames map todo may not be needed for hadoop versions after yarn  
 get a snapshot of the current values of all the tracked quantiles 
 quote all of the active html characters in the given string as they are added to the buffer 
 return an output stream that quotes all of the output 
 add default servlets 
 start the server does not wait for the server to start 
 open the main listener for the server
 checks the user has privileges to access to instrumentation servlets if hadoop security instrumentation requires admin is set to false default value it always returns true if hadoop security instrumentation requires admin is set to true it will check that if the current user is in the admin acls if the user is in the admin acls it returns true otherwise it returns false 
 process a get request for the specified resource 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create but do not start the test webapp server the test webapp dir is prepared checked in advance 
 create and start a server with the test webapp
 pass in a server return a url bound to localhost and its port
 picks a free port on the host by binding a socket to  
 creates a keystore with a single key and saves it to a file 
 performs complete setup of ssl configuration in preparation for testing an sslfactory this includes keys certs keystores truststores the server ssl configuration file the client ssl configuration file and the master configuration file read by the sslfactory 
 test that verifies headers can be up to k long the test adds a k header leaving k for other headers this is because the header buffer setting is for all headers names and values included 
 verify the access for logs stacks conf loglevel and metrics servlets when authentication filters are set but authorization is not enabled 
 verify the administrator access for logs stacks conf loglevel and metrics servlets 
 apply a transform to all columns in a given table if there are no columns in a table or if the context is stopping does nothing 
 small test to ensure the class basically works 
 wait for chaosmonkey to stop 
 execute the given command on the host using ssh
 this allows tests that subclass children of this base class such as link org apache hadoop hbase test integrationtestreplication toinclude the base options without having to also include the options from the test 
 initializes the state of the cluster it starts a new in process mini cluster or if we are given an already deployed distributed cluster it initializes the state 
 this tears down any tables that existed from before and rebuilds the tables and schemas on the source cluster it then sets up replication from the source to the sink cluster by using the link org apache hadoop hbase client replication replicationadmin connection 
 the main test runner this test has  steps  setuptablesandreplication  generate the data into the source cluster  wait for replication to propagate  verify that the data is available in the sink cluster
 builds a tablerecordreader if no tablerecordreader was provided uses the default 
 calculates the splits that will serve as input for the map tasks splits are created in number equal to the smallest between numsplits and the number of link org apache hadoop hbase regionserver hregion s in the table if the number of splits is smaller than the number of link org apache hadoop hbase regionserver hregion s then splits are spanned across multiple link org apache hadoop hbase regionserver hregion s and are grouped the most evenly possible in the case splits are uneven the bigger splits are placed first in the link inputsplit array 
 close the table and related objects that were initialized via link initializetable connection tablename 
 use this before submitting a tablemap job it will appropriately set up the jobconf 
 sets up the job for reading from one or more multiple table snapshots with one or more scans per snapshot it bypasses hbase servers and read directly from snapshot files 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 creates a new record writer be aware that the baseline javadoc gives the impression that there is a single link recordwriter per job but in hbase it is more natural if we give you a newrecordwriter per call of this method you must close the returned recordwriter when done failure to do so will drop writes 
 build the scanner not done in constructor to allow for extension 
 sets up the actual job 
 param row row key
 open a tablehash reader starting at the first hash at or after the given key 
 read the next key hash pair returns true if such a pair exists and false when at the end of the data 
 configure a mapreduce job to perform an incremental load into the given table this inspects the table to configure a total order partitioner uploads the partitions file to the cluster and adds it to the distributedcache sets the number of reduce tasks to match the current number of regions sets the output key value class to match hfileoutputformat s requirements sets the reducer up to perform the appropriate sorting either keyvaluesortreducer or putsortreducer the user should be sure to set the map output value class to either keyvalue or put before running this function 
 configure a mapreduce job to perform an incremental load into the given table this inspects the table to configure a total order partitioner uploads the partitions file to the cluster and adds it to the distributedcache sets the number of reduce tasks to match the current number of regions sets the output key value class to match hfileoutputformat s requirements sets the reducer up to perform the appropriate sorting either keyvaluesortreducer or putsortreducer the user should be sure to set the map output value class to either keyvalue or put before running this function 
 run inside the task to deserialize column family to given conf value map 
 configure job with a totalorderpartitioner partitioning against splitpoints cleans up the partitions file after job exists 
 param row the current table row key 
 param row the current table row key 
 param row the current table row key 
 param row the current table row key 
 param row the current table row key 
 create a link filter to apply to all incoming keys link keyvalue keyvalues to optionally not include in the job output
 sets up the actual job 
 returns the full path to the jar containing the class it always return a jar 
 analogous to link hfileoutputformat configureincrementalload job tabledescriptor regionlocator this function will configure the requisite number of reducers to write hfiles for multple tables simultaneously
 writes an action put or delete to the specified table 
 configure conf to read from snapshotscans with snapshots restored to a subdirectory of restoredir sets link restore dirs key link snapshot to scans key 
 return the list of splits extracted from the scans snapshots pushed to conf by link setinput org apache hadoop conf configuration java util map org apache hadoop fs path 
 restore each snapshot name restore directory pair in snapshottodir
 run the application s maps using a thread pool 
 computes size of each region for table and given column families 
 sets up the actual job 
 sets up the actual job 
 if there is an open hash batch complete it and sync if there are diffs start a new batch and seek to read the
 finish the currently open hash batch compare the target hash to the given source hash if they do not match then sync the covered key range 
 rescan the given range directly from the source and target tables count and log differences and if this is not a dry run output puts and deletes to make the target table match the source table for this range
 sets the configuration this is used to set the details for the table to be scanned 
 sets up a link scan instance applying settings from the configuration propertyconstants defined in code tableinputformat this allows specifying things such as start and stop rows column qualifiers or families timestamps or timerange scanner caching and batch size 
 builds a link tablerecordreader if no link tablerecordreader was provided usesthe default 
 calculates the splits that will serve as input for the map tasks 
 create one inputsplit per region
 calculates the number of mapreduce input splits for the map tasks the number of mapreduce input splits depends on the average region size make it public for testing
 close the table and related objects that were initialized via link initializetable connection tablename 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 use this before submitting a tablemap job it will appropriately set up the job 
 sets up the job for reading from one or more table snapshots with one or more scans per snapshot it bypasses hbase servers and read directly from snapshot files 
 sets up the job for reading from a table snapshot it bypasses hbase servers and read directly from snapshot files 
 sets up the job for reading from a table snapshot it bypasses hbase servers and read directly from snapshot files 
 use this before submitting a multi tablemap job it will appropriately set up the job 
 use this before submitting a multi tablemap job it will appropriately set up the job 
 use this before submitting a multi tablemap job it will appropriately set up the job 
 obtain an authentication token for the specified cluster on behalf of the current user and add it to the credentials for the given map reduce job the quorumaddress is the key to the zk ensemble which contains hbase zookeeper quorum hbase zookeeper client port and zookeeper znode parent
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 use this before submitting a tablereduce job it will appropriately set up the jobconf 
 add hbase and its dependencies only to the job configuration this is intended as a low level api facilitating code reuse between this class and its mapred counterpart it also of use to external tools that need to build a mapreduce job that interacts with hbase but want fine grained control over the jars shipped to the cluster 
 add the hbase dependency jars as well as jars for any of the configured job classes to the job configuration so that jobclient will ship them to the cluster and add them to the distributedcache 
 add the jars containing the given classes to the job s configuration such that jobclient will ship them to the cluster and add them to the distributedcache 
 add the jars containing the given classes to the job s configuration such that jobclient will ship them to the cluster and add them to the distributedcache n b that this method at most adds one jar per class given if there is more than one jar available containing a class with the same name as a given class we don t define which of those jars might be chosen 
 finds the jar for a class or creates it if it doesn t exist if the class is in a directory in the classpath it creates a jar on the fly with the contents of the directory and returns the path to that jar if a jar is created it is created in the system temporary directory otherwise returns an existing jar that contains a class of the same name maintains a mapping from jar contents to the tmp jar created 
 add entries to packagedclasses corresponding to class files contained in jar 
 find a jar that contains a class of the same name if any it will return a jar file even if that is not the first thing on the class path that has a class with the same name looks first on the classpath and then in the packagedclasses map 
 creates a new record writer be aware that the baseline javadoc gives the impression that there is a single link recordwriter per job but in hbase it is more natural if we give you a newrecordwriter per call of this method you must close the returned recordwriter when done failure to do so will drop writes 
 build the scanner not done in constructor to allow for extension 
 configures the job to use tablesnapshotinputformat to read from a snapshot 
 reads the values of each field 
 convert a line of tsv text into an hbase table row 
 implementation shared with deprecated hloginputformat
 sets up the actual job 
 execute the compaction on the specified path 
 execute the actual compaction job if the compact once flag is not specified execute the compaction until no more compactions are needed uses the configuration settings provided 
 returns a split for each store files directory using the block location of each file as locality reference 
 create the input file for the given directories to compact the file is a textfile with each line corrisponding to a store files directory to compact 
 execute compaction using a map reduce job 
 execute compaction from this client one path at the time 
 create the output folder and optionally set ownership 
 set path ownership 
 set path permission 
 execute the export snapshot by copying the snapshot metadata hfiles and wals 
 setup a table with two rows and values 
 create a table that ioe s on first scanner next call
 create a table that throws a donoretryioexception on first scanner next call
 run test assuming no errors using mapred api 
 run test assuming scanner ioexception failure using mapred api 
 run test assuming scanner ioexception failure using mapred api 
 run test assuming notservingregionexception using mapred api 
 run test assuming notservingregionexception using mapred api 
 verify the example we present in javadocs on tableinputformatbase
 pass the key and value to reduce 
 tests a mr scan using specific start and stop rows 
 test that link hfileoutputformat recordwriter amends timestamps ifpassed a keyvalue whose timestamp is link hconstants latest timestamp 
 test that link hfileoutputformat recordwriter writes tags such as ttl intohfile 
 test for hfileoutputformat locality sensitive conf key   true this test could only check the correctness of original logic if locality sensitive conf key is set to true because minihbasecluster always run with single hostname and different ports it s not possible to check the region locality by comparing region locations and dn hostnames when minihbasecluster supports explicit hostnames parameter just like minidfscluster does we could test region locality features more easily 
 test for link hfileoutputformat configurecompression configuration htabledescriptor and link hfileoutputformat createfamilycompressionmap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configurebloomtype htabledescriptor configuration and link hfileoutputformat createfamilybloomtypemap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configureblocksize htabledescriptor configuration and link hfileoutputformat createfamilyblocksizemap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configuredatablockencoding htabledescriptor configuration and link hfileoutputformat createfamilydatablockencodingmap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test that link hfileoutputformat recordwriter uses compression andbloom filter settings from the column family descriptor
 this test is to test the scenario happened in hbase  all files are bulk loaded and excluded from minor compaction without the fix of hbase  an arrayindexoutofboundsexception will be thrown 
 test simple replication case with column mapping
 test main method export should print help and call system exit
 test walkeyvaluemapper setup and map
 test cellcounter all data should print to output
 test cellcounter all data should print to output
 test cellcounter with time range all data should print to output
 test cellcounter with time range all data should print to output
 test cellcounter with time range all data should print to output
 test cellcounter for complete table all data should print to output
 simple end to end test
 simple end to end test with bulkload 
 test copy of table from sourcetable to targettable all rows from family a
 test that link hfileoutputformat recordwriter amends timestamps ifpassed a keyvalue whose timestamp is link hconstants latest timestamp 
 test that link hfileoutputformat recordwriter writes tags such as ttl intohfile 
 test for hfileoutputformat locality sensitive conf key   true this test could only check the correctness of original logic if locality sensitive conf key is set to true because minihbasecluster always run with single hostname and different ports it s not possible to check the region locality by comparing region locations and dn hostnames when minihbasecluster supports explicit hostnames parameter just like minidfscluster does we could test region locality features more easily 
 test for link hfileoutputformat configurecompression configuration htabledescriptor and link hfileoutputformat createfamilycompressionmap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configurebloomtype htabledescriptor configuration and link hfileoutputformat createfamilybloomtypemap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configureblocksize htabledescriptor configuration and link hfileoutputformat createfamilyblocksizemap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test for link hfileoutputformat configuredatablockencoding htabledescriptor configuration and link hfileoutputformat createfamilydatablockencodingmap configuration tests that the compression map is correctly serialized into and deserialized from configuration
 test that link hfileoutputformat recordwriter uses compression andbloom filter settings from the column family descriptor
 this test is to test the scenario happened in hbase  all files are bulk loaded and excluded from minor compaction without the fix of hbase  an arrayindexoutofboundsexception will be thrown 
 test simple replication case with column mapping
 test main method export should print help and call system exit
 if table is not present in non bulk mode dry run should fail just like normal mode 
 if table is not present in bulk mode and create table is not set to yes import should fail with tablenotfoundexception 
 if there are invalid data rows as inputs then only those rows should be ignored 
 run an importtsv job and perform basic validation on the results returns the importtsv tool instance so that other tests can inspect it for further validation as necessary this method is static to insure non reliance on instance s util conf facilities 
 confirm importtsv via hfiles on fs 
 run an importtsv job and perform basic validation on the results returns the importtsv tool instance so that other tests can inspect it for further validation as necessary this method is static to insure non reliance on instance s util conf facilities 
 run an importtsv job and perform basic validation on the results returns the importtsv tool instance so that other tests can inspect it for further validation as necessary this method is static to insure non reliance on instance s util conf facilities 
 confirm importtsv via hfiles on fs 
 test multithreadedtablemappper map reduce against a multi region table
 when size of region in megabytes is larger than largest possible integer there could be error caused by lost of precision 
 when calculator is disabled it should return  for each request 
 throws java lang exception
 test a case when no column was specified in command line arguments 
 test a case when the column specified in command line arguments is exclusive for few rows 
 test a case when the column specified in command line arguments is one for which the qualifier contains colons 
 test a case when the column specified in command line arguments is not part of first kv for a row 
 test a case when the column specified in command line arguments is exclusive for few rows and also a row range filter is specified
 test a case when a range is specified with single range of start end keys
 test a case when a range is specified with single range with end key only
 test a case when a range is specified with two ranges where one range is with end key only
 test a case when a range is specified with multiple ranges of start end keys
 test a case when a range is specified with multiple ranges of start end keys one range is filled another two are not
 test a case when the timerange is specified with starttime and endtime options
 setup a table with two rows and values 
 create a table that ioe s on first scanner next call
 create a table that throws a notservingregionexception on first scanner next call
 run test assuming no errors using newer mapreduce api
 run test assuming scanner ioexception failure using newer mapreduce api
 run test assuming scanner ioexception failure using newer mapreduce api
 run test assuming notservingregionexception using newer mapreduce api
 run test assuming notservingregionexception using newer mapreduce api
 verify the example we present in javadocs on tableinputformatbase
 test a map reduce against a multi region table
 test walkeyvaluemapper setup and map
 test partial reads from the log based on passed time range
 test basic functionality
 convert a line of tsv text into an hbase table row after transforming the values by multiplying them by  
 provides an extension point for tests that don t want a per row invocation 
 parse options passed in via an arguments array assumes that array has been split on white space and placed into a code queue any unknown arguments will remain in the queue at the conclusion of this method call it s up to the caller to deal with these unrecognized arguments 
 do a small loading into a table make sure the data is really the same then run the verifyreplication job to check the results do a second comparison where all the cells are different 
 load a row into a table make sure the data is really the same delete the row make sure the delete marker is replicated run verify replication with and without raw to check the results 
 create a table and take a snapshot of the table used by the export test 
 verify if exported snapshot and copied files matches the original one 
 creates destination directory runs exportsnapshot tool and runs some verifications 
 check that exportsnapshot will succeed if something fails but the retry succeed 
 check that exportsnapshot will fail if we inject failure more times than mr will retry 
 mock a snapshot with files in the archive dir two regions and one reference file 
 exercise the mr spec writing simple assertions to make sure it is basically working 
 when num tables is specified the function starts multiple worker threads which individually start a loadtesttool instance to load a table each table name is in format for example tn test num tables  table names will be test  test  
 creates a link metricregistries instance using the corresponding link metricregistries available to link serviceloader on the classpath if no instance is found then defaultimplementation will be loaded 
 internal method called by the procedureexecutor that starts the user level code execute 
 internal method called by the procedureexecutor that starts the user level code rollback 
 initialize the procedure executor but do not start workers we will start them later it calls procedurestore recoverlease and procedurestore load to recover the lease and ensure a single executor and start the procedure replay to resume and recover the previous pending and in progress procedures 
 initialize the procedure executor but do not start workers we will start them later it calls procedurestore recoverlease and procedurestore load to recover the lease and ensure a single executor and start the procedure replay to resume and recover the previous pending and in progress procedures 
 bypass a procedure if the procedure is set to bypass all the logic in execute rollback will be ignored and it will return success whatever it is used to recover buggy stuck procedures releasing the lock resources and letting other procedures run bypassing one procedure and its ancestors will be bypassed automatically may leave the cluster in a middle state e g region not assigned or some hdfs files left behind after getting rid of those stuck procedures the operators may have to do some clean up on hdfs or schedule some assign procedures to let region online do at your own risk a procedure can be bypassed only if  the procedure is in state of runnable waiting waiting timeout or it is a root procedure without any child  no other worker thread is executing it  no child procedure has been submitted if all the requirements are meet the procedure and its ancestors will be bypassed and persisted to wal if the procedure is in waiting state will set it to runnable add it to run queue todo what about waiting timeout 
 helper to convert the procedure to protobuf used by procedurestore implementations 
 helper to convert the protobuf procedure used by procedurestore implementations todo optimization some of the field never change during the execution e g classname procid parentid we can split in data and state and the store may take advantage of it by storing the data only on insert 
 reads a log file and outputs its contents 
 pass one or more log file names and formatting options and it will dump out a text version of the contents on stdout 
 make sure that the file set are gotten by calling link getlogfiles where we will sort the file set by log id 
 run through all procedure flow states twice while also restarting procedure executor at each step i e force a reread of procedure store it does execute step n kill the executor before store update restart executor store execute step n and then save to store this is a good test for finding state that needs persisting and steps that are not idempotent 
 tests that tracker for all old logs are loaded back after procedure store is restarted 
 add a new remote slave cluster for replication 
 a private method used during initialization this method attempts to add all registered peer clusters this method does not set a watch on the peer cluster znodes 
 attempt to connect to a new remote slave cluster 
 helper method to connect to a peer
 start a state tracker to check whether this peer is enabled or not
 parse the raw data from zk to get a peer s state
 param state
 get a list of all region servers that have outstanding replication queues these servers could be alive dead or from a previous run of the cluster 
 get the queue id s and meta data owner and history for the queues belonging to the named server
 attempts to acquire the replication table this operation will block until it is assigned by the createreplicationworker thread it is up to the caller of this method to close the returned table
 execute a transaction method given only the path will select at random one of the members of the supplied cluster definition and iterate through the list until a transaction can be successfully completed the definition of success here is a complete http transaction irrespective of result code 
 execute a transaction method given a complete uri 
 execute a transaction method will call either executepathonly or executeuri depending on whether a path only is supplied in path or if a complete uri is passed instead respectively 
 initiate client side kerberos negotiation with the server 
 send a head request
 send a head request
 send a get request
 send a get request
 send a get request
 send a get request
 send a get request
 returns the response body of the httpresponse if any as an array of bytes if response body is not available or cannot be read returns null note this will cause the entire response body to be buffered in memory a malicious server may easily exhaust all the vm memory it is strongly recommended to use getresponseasstream if the content length of the response is unknown or reasonably large 
 send a get request
 send a put request
 send a put request
 send a put request
 send a put request
 send a put request
 send a put request
 send a post request
 send a post request
 send a post request
 send a post request
 send a post request
 send a post request
 send a delete request
 send a delete request
 send a delete request
 send a delete request
 param tablename name of table to check
 return string representing the rest api s version
 return string representing the cluster s version
 return string representing the cluster s version
 param tablename name of table to check
 creates a new table 
 deletes a table 
 deletes a table 
 return string representing the cluster s version
 convert the rest server s response to an xml reader 
 handles an link httpinteraction by applying the filtering logic 
 param scan the scan specification
 constructor
 build a response for delete delete namespace 
 dispatch to namespaceinstanceresource for getting list of tables 
 the main method for the hbase rest server 
 param conf existing configuration to use in rest servlet
 constructor
 validates the input request parameters parses columns from cellsetmodel and invokes increment on htable 
 provides an extension point for tests that don t want a per row invocation 
 an example to scan using listener in unmarshaller for xml 
 read protobuf stream 
 check passed name fail if nulls or if corresponding rsgroupinfo not found 
 check servers and tables 
 moves every region from servers which are currently located on these servers but should not be located there 
 read rsgroup info from the source of truth the hbase rsgroup table update zk cache called on startup of the manager 
 test the load balancing algorithm invariant is that all servers of the group should be hosting either floor average or ceiling average 
 all regions have an assignment 
 tests the bulk assignment used during cluster startup round robin should yield a balanced cluster so same invariant as the load balancer holds all servers holding either floor avg or ceiling avg 
 test the cluster startup bulk assignment which attempts to retain assignment info 
 asserts a valid retained assignment plan must meet the following conditions every input region has an assignment and to an online server if a region had an existing assignment to a server with the same address a a currently online server it will be assigned to it 
 generate assigned regions to a given server using group information 
 turn on auto backups of hfiles on the specified table when hfiles would be deleted from the hfile archive they are instead preserved 
 stop retaining hfiles for the given table in the archive hfiles will be cleaned up on the next pass of the link org apache hadoop hbase master cleaner hfilecleaner if the hfiles are retained by another cleaner 
 add this table to the tracker and then read a watch on that node handles situation where table is deleted in the time between the update and resetting the watch by deleting the table via link safestoptrackingtable string 
 stop tracking a table ensures that the table doesn t exist but if it does it attempts to add the table back via link addandrewatchtable string its a safe removal 
 sets the watch on the top level archive znode and then updates the monitor with the current tables that should be archived and ensures that those nodes are watched as well 
 turn on backups for all hfiles for the given table all deleted hfiles are moved to the archive directory under the table directory rather than being deleted if backups are already enabled for this table does nothing if the table does not exist the archiving the table s hfiles is still enabled as a future table with that name may be created shortly 
 disable hfile backups for the given table previously backed up files are still retained if present asynchronous operation some extra hfiles may be retained in the archive directory after disable is called dependent on the latency in zookeeper to the servers 
 disable hfile backups for the given table previously backed up files are still retained if present asynchronous operation some extra hfiles may be retained in the archive directory after disable is called dependent on the latency in zookeeper to the servers 
 disable hfile backups for all tables previously backed up files are still retained if present asynchronous operation some extra hfiles may be retained in the archive directory after disable is called dependent on the latency in zookeeper to the servers 
 determine if archiving is enabled but not necessarily fully propagated for a table
 determine if archiving is enabled but not necessarily fully propagated for a table
 cleans up all the files for a hregion by archiving the hfiles to the archive directory
 remove an entire region from the table directory via archiving the region s hfiles 
 remove from the specified region the store files of the specified column family either by archiving them or outright deletion
 removes from the specified region the store files of the specified column family either by archiving them or outright deletion
 remove the store files either by archiving them or outright deletion
 archive the store file
 resolve any conflict with an existing archive file via timestamp append renaming of the existing file and then archive the passed in files 
 attempt to archive the passed in file to the archive directory if the same file already exists in the archive it is moved to a timestamped directory under the archive directory and the new file is put in its place 
 move the file to the given destination
 param timeout in milliseconds if set to  waits indefinitely 
 creates a tablesnapshotscanner 
 pack the full number version in a int by shifting each component by bit except the dot release which has bit examples    is x    is x 
 add configuration less constraints to the table this will overwrite any configuration associated with the previous constraint of the same class each constraint when added to the table will have a specific priority dictating the order in which the link constraint will be run a link constraint earlier in the list will be run before those later inthe list the same logic applies between two constraints over time earlier added is run first on the regionserver 
 add constraints and their associated configurations to the table adding the same constraint class twice will overwrite the first constraint s configuration each constraint when added to the table will have a specific priority dictating the order in which the link constraint will be run a link constraint earlier in the list will be run before those later inthe list the same logic applies between two constraints over time earlier added is run first on the regionserver 
 add a link constraint to the table with the given configuration each constraint when added to the table will have a specific priority dictating the order in which the link constraint will be run a link constraint added will run on the regionserver before those added tothe link htabledescriptor later 
 write the raw constraint and configuration to the descriptor this method takes care of creating a new configuration based on the passed in configuration and then updating that with enabled and priority of the constraint when a constraint is added it is automatically enabled 
 write the given key and associated configuration to the link htabledescriptor 
 write the configuration to a string
 read in the configuration from the string encoded configuration
 update the configuration for the link constraint does not change the order in which the constraint is run 
 enable the given link constraint retains all the information e g configuration for the link constraint but makes sure that it gets loaded on the table 
 disable the given link constraint retains all the information e g configuration for the link constraint but it just doesn t load the link constraint on the table 
 change the whether the constraint if it is already present is enabled or disabled 
 check to see if the given constraint is enabled 
 get the constraints stored in the table descriptor
 try to grab a lock on the task zk node to own and execute the task 
 wait for tasks to become available at hbase splitlog zknode grab a task one at a time this policy puts an upper limit on the number of simultaneous log splitting that could be happening in a cluster synchronization using taskreadyseq ensures that it will try to grab every task that has been put up
 pass a processor to region to process multiple rows atomically the rowprocessor implementations should be the inner classes of your rowprocessorendpoint this way the rowprocessor can be class loaded with the coprocessor endpoint together see code testrowprocessorendpoint for example the request contains information for constructing processor see link constructrowprocessorfromrequest the processor object defines the read modify write procedure 
 load a coprocessor implementation into the host
 load a coprocessor implementation into the host
 param implclass implementation class
 in case of coprocessors which have many kinds of observers for eg link regioncoprocessor has bulkloadobserver regionobserver etc some implementations may not need all observers in which case they will return null for that observer s getter we simply ignore such cases 
 do not call with an observeroperation that is null have the caller check 
 return true if we are to bypass can only be true ifobserveroperation isbypassable 
 coprocessor classes can be configured in any order based on that priority is set and chained in a sorted order should be used prestop hooks i e when master regionserver is going down this function first calls coprocessor methods using observeroperation call and then shutdowns the environment in postenvcall need to execute all coprocessor methods first then postenvcall otherwise some coprocessors may remain shutdown if any exception occurs during next coprocessor execution which prevent master regionserver stop or cluster shutdown refer hbase 
 called prior to modifying a table s properties called as part of modify table rpc call 
 called after the modifytable operation has been requested called as part of modify table rpc call 
 called prior to modifying a table s properties called as part of modify table procedure and it is async to the modify table rpc call 
 called after to modifying a table s properties called as part of modify table procedure and it is async to the modify table rpc call 
 called prior to modifying a namespace s properties 
 called after the modifynamespace operation has been requested 
 dump a textual representation of the executor s status to the given writer 
 update meta table with favored nodes info
 update meta table with favored nodes info
 for regions that share the primary avoid placing the secondary and tertiary on a same rs used for generating new assignments for the primary secondary tertiary regionservers
 place secondary and tertiary nodes in a multi rack case if there are only two racks then we try the place the secondary and tertiary on different rack than primary but if the other rack has only one region server then we place primary and tertiary on one rack and secondary on another the aim is two distribute the three favored nodes on    racks todo see how we can use generatemissingfavorednodemultirack api here
 create a filesystem object for hbase regionservers 
 returns an instance of filesystem wrapped into the class specified in hbase fs wrapper property if one is set in the configuration returns unmodified fs instance passed in as an argument otherwise 
 add an interceptor on the calls to the namenode getblocklocations from the dfsclient linked to this filesystem see hbase  for the background there should be no reason except testing to create a specific reorderblocks 
 create a new hfilesystem object similar to filesystem get this returns a filesystem object that avoids checksum verification in the filesystem for hfileblock reads for these blocks checksum verification is done by hbase 
 end the current block and complete file at namenode you should call link recoverandclose cancelableprogressable if this method throws an exception 
 create a link fanoutoneblockasyncdfsoutput the method maybe blocked so do not call it inside an link eventloop 
 open the filelink for read it uses a wrapper of fsdatainputstream that is agnostic to the location of the file even if the file switches between locations 
 open the filelink for read it uses a wrapper of fsdatainputstream that is agnostic to the location of the file even if the file switches between locations 
 this will free sockets and file descriptors held by the stream only when the stream implements org apache hadoop fs canunbuffer not thread safe must be called only when all the clients using this stream to read the blocks have finished reading if by chance the stream is unbuffered and there are clients still holding this stream for read then on next client read request a new socket will be opened by datanode without client knowing about it and will serve its read request note if this socket is idle for some time then the datanode will close the socket and the socket will move into close wait state and on the next client request on this stream the current socket will be closed and a new socket will be opened to serve the requests 
 rebuild the allocator s data structures from a persisted map 
 flush the entries in ramcache to ioengine and add bucket entry to backingmap process all that are passed in even if failure being sure to remove from ramcache else we ll never undo the references and we ll oome 
 blocks until elements available in code q then tries to grab as many as possiblebefore returning 
 transfers data from file to the given byte buffer
 transfers data from the given byte buffer to file
 close the file
 sync the data to file after writing
 close the file
 de serialization for compound bloom filter metadata must be consistent with what link compoundbloomfilterwriter does 
 this is modeled after link compoundbloomfilterwriter metawriter for simplicity although the two metadata formats do not have to be consistent this does have to be consistent with how link compoundbloomfilter compoundbloomfilter datainput org apache hadoop hbase io hfile hfile reader reads fields 
 write the trailer to a data stream we support writing version  for testing and for determining version  trailer size it is also easy to see what fields changed in version  
 write trailer data as protobuf note we run a translation on the comparator name and will serialize the old hbase  x where it makes sense see link gethbasecompatiblename string 
 deserialize the fixed file trailer from the given stream the version needs to already be specified make sure this is consistent with link serialize dataoutputstream 
 deserialize the file trailer as protobuf
 deserialize the file trailer as writable data
 reads a file trailer from the given file 
 the sockets and the file descriptors held by the method parameter code fsdatainputstreamwrapper passed will be freed after its usage so caller needs to ensurethat no other threads have access to the same passed reference 
 creates reader with cache configuration disabled
 param fs filesystem
 this factory method is used only by unit tests the sockets and the file descriptors held by the method parameter code fsdatainputstreamwrapper passed will be freed after its usage so caller needs to ensurethat no other threads have access to the same passed reference 
 returns true if the specified file has a valid hfile trailer 
 write out this instance on the passed in out stream we write it as a protobuf 
 populate this instance with what we find on the passed in in stream can deserialize protobuf of old writables format 
 now parse the old writable format it was a list of map entries each map entry was a key and a value of a byte the old map format had a byte before each entry that held a code which was short for the key or value type we know it was a byte so in below we just read and dump it 
 checks if the block is internally consistent i e the first link hconstants hfileblock header size bytes of the buffer contain avalid header consistent with the fields assumes a packed block structure this function is primary for testing and debugging and is not thread safe because it alters the internal buffer pointer used by tests only 
 read from an input stream at least necessarylen and if possible extralen also if available analogous to link ioutils readfully inputstream byte int int but specifies a number of extra bytes to also optionally read 
 transitions the block writer from the writing state to the block ready state does nothing if a block is already finished 
 finish up writing of the block flushes the compressing stream if using compression fills out the header does any compression encryption of bytes to flush out to disk and manages the cache on write content if applicable sets block write state to block ready 
 similar to link writeheaderanddata fsdataoutputstream but records the offset of this block so that it can be referenced in the next block of the same type 
 writes the header and the compressed data of this block or uncompressed data when not using compression into the given stream can be called in the writing state or in the block ready state if called in the writing state transitions the writer to the block ready state 
 returns the header or the compressed data or uncompressed data when not using compression as a byte array can be called in the writing state or in the block ready state if called in the writing state transitions the writer to the block ready state this returns the header  data  checksums stored on disk 
 takes the given link blockwritable instance creates a new block ofits appropriate type writes the writable into this block and flushes the block into the output stream the writer is instructed not to buffer uncompressed bytes for cache on write 
 does a positional read or a seek and read into the given buffer returns the on disk size of the next block or  if it could not be read determined e g eof 
 reads a version  block version  blocks not supported and not expected tries to do as little memory allocation as possible using the provided on disk size 
 reads a version  block 
 return the data block which contains this key this function will only be called when the hfile version is larger than  
 read in the root level index from the given input stream must match what was written into the root level by link blockindexwriter writeindexblocks fsdataoutputstream at theoffset that function returned 
 read in the root level index from the given input stream must match what was written into the root level by link blockindexwriter writeindexblocks fsdataoutputstream at theoffset that function returned 
 read the root level metadata of a multi level block index based on link readrootindex datainput int but also reads metadata necessary to compute the mid key in a multi level index 
 writes the root level and intermediate levels of the block index into the output stream generating the tree from bottom up assumes that the leaf level has been inline written to the disk if there is enough data for more than one leaf block we iterate by breaking the current level of the block index starting with the index of all leaf level blocks into chunks small enough to be written to disk and generate its parent level until we end up with a level small enough to become the root level if the leaf level is not large enough there is no inline block index anymore so we only write that level of block index to disk as the root level 
 writes the block index data as a single level only does not do any block framing 
 split the current level of the block index into intermediate index blocks of permitted size and write those blocks to disk return the next level of the block index referencing those intermediate level blocks 
 write out the current inline index block inline blocks are non root blocks so the non root index format is used 
 used when writing the root block index of a multi level block index serializes additional information allowing to efficiently identify the mid key 
 writes the block index chunk in the non root index block format this format contains the number of entries an index of integer offsets for quick binary search on variable length records and tuples of block offset on disk block size and the first key for each entry 
 writes this chunk into the given output stream in the root block index format this format is similar to the link hfile version  blockindex format except that we store on disk size of the block instead of its uncompressed size 
 runs the command line pretty printer and returns the desired command exit code zero for success non zero for failure 
 opens a hfile you must load the index before you can use it by calling link loadfileinfo 
 an internal api function seek to the given key optionally rewinding to the first key of the block before doing the seek 
 set our selves up for the next next invocation set up next block 
 go to the next key value in the block section loads the next block if necessary if successful link getkey and link getvalue canbe called 
 positions this scanner at the start of the file 
 retrieve block from cache validates the retrieved block s type vs code expectedblocktype and its encoding vs code expecteddatablockencoding unpacks the block as necessary 
 param metablockname
 returns a buffer with the bloom filter metadata the caller takes ownership of the buffer 
 sets the file info offset in the trailer finishes up populating fields in the file info and writes the file info into the given data output the reason the data output is not always link outputstream is that we storefile info as a block in version  
 at a block boundary write all the inline blocks and opens new block 
 add key value to file keys must be added in an order that agrees with the comparator passed on construction 
 create an hfilelink instance from table region family hfile location
 create a new hfilelink it also adds a back reference to the hfile back reference directory to simplify the reference count and the cleaning process 
 create a new hfilelink it also adds a back reference to the hfile back reference directory to simplify the reference count and the cleaning process 
 create a new hfilelink it also adds a back reference to the hfile back reference directory to simplify the reference count and the cleaning process 
 create a new hfilelink starting from a hfilelink name it also adds a back reference to the hfile back reference directory to simplify the reference count and the cleaning process 
 create a new hfilelink starting from a hfilelink name it also adds a back reference to the hfile back reference directory to simplify the reference count and the cleaning process 
 deprecated writables are going away use the pb serialization methods instead remove in a release after   goes out this is here only to migrate old reference files written with writables before   
 read a reference from filesystem 
 write out our chain of buffers in chunks
 this is a server side method which is invoked over rpc on success the return response has protobuf response payload on failure the exception name and the stack trace are returned in the protobuf response 
 this is a wrapper around link java nio channels readablebytechannel read java nio bytebuffer if the amount of data is large it writes to channel in smaller chunks this is to avoid jdk from creating many direct buffers as the size of bytebuffer increases there should not be any performance degredation 
 helper for link channelread java nio channels readablebytechannel java nio bytebuffer only one of readch or writech should be non null 
 set up cell block codecs
 no protobuf encoding of raw sasl messages
 send the response for connection header
 constructs a server listening on the named port and address 
 wait for the server to be stopped does not wait for all subthreads to finish see link stop 
 a convenience method to bind to a given address and report better exceptions if the address is not a valid host 
 process all the responses for this connection
 read off the wire if there is not enough data to read update the connection state with what we have and returns 
 constructor 
 test things basically work 
 block until becoming the active master method blocks until there is not another active master and our attempt to become the new active master is successful this also makes sure that we are watching the master znode so will be notified if another master dies 
 called when the regionserver wants to report a procedure transition ends up calling link reporttransition regionstatenode servername transitioncode long 
 to rollback link mergetableregionsprocedure two assignprocedures are asynchronously submitted for each region to be merged rollback doesn t wait on the completion of the assignprocedures this can be improved by changing rollback to support sub procedures see hbase  for details 
 prepare merge and do some check
 create a merged region
 rollback close regions
 rollback close parent region
 create daughter regions
 create split directory
 set it up so when procedure is unsuspended we ll move to the procedure finish 
 raw call to remote regionserver to get info on a particular region 
 returns an ordered list of hosts that are hosting the blocks for this region the weight of each host is the sum of the block lengths of all files on that host so the first host in the list is the server which holds the most bytes of the given region s hfiles 
 scans hbase meta and returns a number of scanned rows and a map of merged regions and an ordered map of split parents 
 if merged region no longer holds reference to the merge regions archive merge region on hdfs and perform deleting references in hbase meta
 run janitorial scan of catalog hbase meta table looking for garbage to collect 
 if daughters no longer hold reference to the parents delete the parent 
 checks if a daughter region either splita or splitb still holds references to parent 
 checks if the specified region has merge qualifiers if so try to clean them
 a utility method to create new instances of logcleanerdelegate based on the class name of the logcleanerdelegate 
 perform a delete on a specified type 
 get cleaner results of subdirs 
 remove the undeleted replication queue s zk node for removed peers 
 remove the undeleted hfile refs queue s zk node for removed peers 
 starts the tracking of draining regionservers all draining rss will be tracked after this method is called 
 finish initialization of hmaster after becoming the primary master the startup order is a bit complicated but very important do not change it unless you know what you are doing initialize file system based components file system manager wal manager table descriptors etc publish cluster id here comes the most complicated part initialize server manager assignment manager and region server tracker create server manager create procedure executor load the procedures but do not start workers we will start it later after we finish scheduling scps to avoid scheduling duplicated scps for the same server create assignment manager and start it load the meta region state but do not load data from meta region start region server tracker construct the online servers set and find out dead servers and schedule scp for them the online servers will be constructed by scanning zk and we will also scan the wal directory to find out possible live region servers and the differences between these two sets are the dead servers if this is a new deploy schedule a initmetaprocedure to initialize meta start necessary service threads balancer catalog janior executor services and also the procedure executor etc notice that the balancer must be created first as assignment manager may use it when assigning regions wait for meta to be initialized if necesssary start table state manager wait for enough region servers to check in let assignment manager load data from meta and construct region states start all other things such as chore services etc notice that now we will not schedule a special procedure to make meta online unless the first time where meta has not been created yet we will rely on scp to bring meta online 
 check hbase meta is up and ready for reading for use during master startup only 
 check hbase namespace table is assigned if not startup will hang looking for the ns table todo fix this ns should not hold up startup 
 it first sleep to the next balance plan start time meanwhile throttling by the max number regions in transition to protect availability 
 checks whether the table conforms to some sane limits and configured values compression etc work throws an exception if something is wrong 
 return cluster status
 utility for constructing an instance of the passed hmaster class 
 create a new namespace 
 modify an existing namespace 
 delete an existing namespace only empty namespaces no tables can be removed 
 get a namespace
 get all namespaces
 returns the list of table descriptors that match the specified request
 returns the list of table names that match the specified request
 acquire the lock waiting indefinitely until the lock is released or the thread is interrupted 
 constructor for namespace lock 
 constructor for table lock 
 constructor for region lock s 
 create initial layout in filesystem check if the meta region exists and is readable if not create it create hbase version and the hbase meta directory if not one idempotent 
 get the rootdir make sure its wholesome and exists before returning 
 wait for thread finish 
 triggers a synchronous attempt to run a distributed procedure and sets return data in response inheritdoc 
 compact a region on the master 
 submit the procedure that gets created by f
 a raw version of assign that does bulk and skirts master state checks assigns can be made during master startup for use by hbck 
 a raw version of unassign that does bulk and skirts master state checks unassigns can be made during master startup for use by hbck 
 get servernames which are currently splitting paths have a splitting suffix 
 get servernames that could be alive excludes those that have a splitting suffix as these are already being split they cannot be alive 
 return listing of servernames found by parsing wal directory paths in fs 
 inspect the log directory to find dead servers which need recovery work
 specialized method to handle the splitting for meta wal
 specialized method to handle the splitting for meta wal
 return true if a wal directory exists will return true also if wals found inservername splitting too 
 this method is the base split method that splits wal files matching a filter callers should pass the appropriate filter for meta and non meta wals 
 check that cluster is up and master is running check table is modifiable if enabled check table is enabled else check it is disabled call in procedure constructor so can pass any exception to caller 
 create regions in file system 
 create region layout in file system 
 action before disabling table 
 action after disabling table 
 action before enabling table 
 action after enabling table 
 helper used to deal with submitting procs with nonce internally the nonceprocedurerunnable run will be called only if no one else registered the nonce any exception thrown by the run method will be collected handled and rethrown long procid   masterprocedureutil submitprocedure new nonceprocedurerunnable procexec noncegroup nonce literal override public void run cphost preoperation submitprocedure new myproc cphost postoperation 
 action before modifying table 
 removes from hdfs the families that are not longer present in the new table descriptor 
 update replica column families if necessary 
 action after modifying table 
 verify the region placement is consistent with the assignment plan
 update the assignment plan into hbase meta
 update the assignment plan to all the region servers
 return how many regions will move per table since their primary rs will change
 compares two plans and check whether the locality dropped or increased prints the information as a string also prints the baseline locality
 starts the tracking of online regionservers starts the tracking of online regionservers all rses will be tracked after this method is called in this method we will also construct the region server sets in link servermanager if a region server is dead between the crash of the previous master instance and the start of the current master instance we will schedule a scp for it this is done in link servermanager findoutdeadserversandprocess set set we call it here under the lock protection to prevent concurrency issues with server expiration operation 
 if replicate all flag is true it means all user tables will be replicated to peer cluster then allow config exclude namespaces or exclude table cfs which can t be replicated to peer cluster if replicate all flag is false it means all user tables can t be replicated to peer cluster then allow to config namespaces or table cfs which will be replicated to peer cluster 
 let the server manager know a new regionserver has come online
 sends a warmup rpc to the specified server to warmup the specified region a region server could reject the close request because it either does not have the specified region or the region is being split 
 contacts a region server and waits up to timeout ms to close the region this bypasses the active hmaster 
 wait for the region servers to report in we will wait until one of this condition is met the master is stopped the hbase master wait on regionservers maxtostart number of region servers is reached the hbase master wait on regionservers mintostart is reached and there have been no new region server in for hbase master wait on regionservers interval time and the hbase master wait on regionservers timeout is reached
 this method kicks off a snapshot procedure other than that it hangs around for various phases to complete 
 verify that the snapshot in the directory is a valid snapshot
 check that all the regions in the snapshot are valid and accounted for 
 trigger a cache refresh even if its before the next cache refresh does not affect pending cache refreshes blocks until the cache is refreshed exposed for testing 
 check to see if any of the passed file names is contained in any of the snapshots first checks an in memory cache of the files to keep if its not in the cache then the cache is refreshed and the cache checked again for that file this ensures that we never return files that exist note this may lead to periodic false positives for the file being referenced periodically the cache is refreshed even if there are no requests to ensure that the false negatives get removed eventually for instance suppose you have a file in the snapshot and it gets loaded into the cache then at some point later that snapshot is deleted if the cache has not been refreshed at that point cache will still think the file system contains that file and return true even if it is no longer present false positive however if the file never was on the filesystem we will never find it and always return false 
 fully specify all necessary components of a snapshot manager exposed for testing 
 gets the list of all completed snapshots 
 delete the specified snapshot
 check if the specified snapshot is done
 take a snapshot of a disabled table 
 take a snapshot of an enabled table 
 take a snapshot based on the enabled disabled state of the table 
 clone the specified snapshot the clone will fail if the destination table has a snapshot or restore in progress 
 restore or clone the specified snapshot
 restore the specified snapshot the restore will fail if the destination table has a snapshot or restore in progress 
 called at startup to verify if snapshot operation is supported and to avoid starting the master if there re snapshots present but the cleaners needed are missing otherwise we can end up with snapshot data loss 
 execute the core common portions of taking a snapshot the link snapshotregions list call should get implemented for each snapshot flavor 
 param logdir one region sever wal dir path in logs
 the caller will block until all the log files of the given region server have been processed successfully split or an error is encountered by an available worker region server this method must only be called after the region servers have been brought online 
 the caller will block until all the hbase meta log files of the given region server have been processed successfully split or an error is encountered by an available worker region server this method must only be called after the region servers have been brought online 
 param path
 return true if the switch is on false otherwise
 create namespace in a blocking manner keeps trying until link clusterschema hbase master cluster schema operation timeout key expires note by passes notifying coprocessors and name checks use for system namespaces only 
 this method checks if the namespace table is assigned and then tries to create its table reference if it was already created before it also makes sure that the connection isn t closed 
 set table state to provided caller should lock table on write 
 set table state to provided but only if table in specified states caller should lock table on write 
 set table state to provided but only if table not in specified states caller should lock table on write 
 this code is for case where a hbase master is starting for the first time zookeeper is where we used to keep table state on first startup read zookeeper and update hbase meta with the table states found in zookeeper this is tricky as we ll do this check every time we startup until mirroring is disabled see the link migrate table state from zk key flag originalform of this migration came in with hbase  it deleted all znodes when done we can t do that if we want to support hbase  x clients who need to be able to read table state out of zk see link mirroringtablestatemanager 
 compacts the mob files for the current column family 
 compacts the mob files by compaction type for the current column family 
 selects the compacted mob del files iterates the candidates to find out all the del files and small mob files 
 performs the compaction on the selected files compacts the del files compacts the selected small mob files and all the del files if all the candidates are selected delete the del files 
 compacts the selected small mob files and all the del files 
 compacts a partition of selected small mob files and all the del files 
 compacts a partition of selected small mob files and all the del files in a batch 
 compacts the del files in batches which avoids opening too many files 
 compacts the del file in a batch 
 flushes the snapshot of the memstore if this store is not a mob store flush the cells in the snapshot to store files of hbase if the store is a mob one the flusher flushes the memstore into two places one is the store files of hbase the other is the mob files cells that are not put type or have the delete mark will be directly flushed to hbase if the size of a cell value is larger than a threshold it ll be flushed to a mob file another cell with the path of this file will be flushed to hbase if the size of a cell value is smaller than or equal with a threshold it ll be flushed to hbase directly 
 reads a cell from the mob file 
 parses the string to a date 
 cleans the expired mob files cleans the files whose creation date is older than current columnfamily ttl and the minversions of that column family is  
 creates a writer for the mob file in temp directory 
 creates a writer for the ref file in temp directory 
 creates a writer for the mob file in temp directory 
 creates a writer for the del file in temp directory 
 creates a writer for the mob file in temp directory 
 commits the mob file 
 creates a thread pool 
 checks if the mob file is expired 
 fill out partition id based on compaction policy and date threshold 
 check quota to create table we add the table information to namespace state cache assuming the operation will pass if the operation fails then the next time namespace state chore runs namespace state cache will be corrected 
 check and update region count quota for an existing table 
 get region count for table
 starts the namespacestatemanager the boot strap of cache is done in the post master start hook of the namespaceauditor class 
 initialize namespace state cache by scanning meta table 
 flush the online regions on this rs for the target table 
 if in a running state creates the specified subprocedure to flush table regions because this gets the local list of regions to flush and not the set the master had there is a possibility of a race where regions may be missed 
 wait for all of the currently outstanding tasks submitted via link submittask callable this must be called after all tasks are submitted via submittask 
 this call is the main execution thread of the barriered procedure it sends messages and essentially blocks until all procedure members acquire or later complete but periodically checks for foreign exceptions 
 waits until the entire procedure has globally completed or has been aborted if an exception is thrown the procedure may or not have run cleanup to trigger the completion latch yet 
 waits until the entire procedure has globally completed or has been aborted if an exception is thrown the procedure may or not have run cleanup to trigger the completion latch yet 
 wait for latch to count to zero ignoring any spurious wake ups but waking periodically to check for errors
 load system procedures read the class names from configuration called by constructor 
 shutdown the threadpool and wait for upto timeoutms millis before bailing
 execute the subprocedure link acquirebarrier and link insidebarrier methodswhile keeping some state for other threads to access this would normally be executed by the procedurememeber when a acquire message comes from the coordinator rpcs are used to spend message back to the coordinator after different phases are executed any exceptions caught during the execution except for interruptedexception get converted and propagated to coordinator via link procedurememberrpcs sendmemberaborted subprocedure foreignexception 
 recursively print the current state of zk non transactional 
 helper method to print the current state of the zk tree 
 processes each code tablename which has a quota defined and moves it in or out ofviolation based on the space use 
 processes each namespace which has a quota defined and moves all of the tables contained in that namespace into or out of violation of the quota tables which are already in violation of a quota at the table level which also have a reside in a namespace with a violated quota will not have the namespace quota enacted the table quota takes priority over the namespace quota 
 filters out all tables for which the master currently doesn t have enough region space reports received from regionservers yet 
 check the quota for the current rpc context user returns the operationquota used to get the available quota and to report the data usage of the operation 
 check the quota for the current rpc context user returns the operationquota used to get the available quota and to report the data usage of the operation 
 fetches each table with a quota table or namespace quota and then fetch the name of each snapshot which was created from that table 
 writes the snapshot sizes to the code hbase quota table 
 instantiates the link spacequotasnapshotnotifier implementation as defined in theconfiguration provided 
 commit all writers notice that here we use the same maxseqid for all output files since we haven t find an easy to find enough sequence ids for different output files in some corner cases see comments in hbase  for more details 
 constructs the priority function given the rpc server implementation and the annotations on the methods in the provided code clz 
 get the priority for a given request from the header and the param this doesn t consider which user is sending the request at all this doesn t consider annotations
 heuristics for guessing whether we need minor compaction 
 we receive store files sorted in ascending order by seqid then scan the list of files if the current file has a maxtimestamp older than last known maximum treat this file as it carries the last known maximum this way both seqid and timestamp are in the same order if files carry the same maxtimestamps they are ordered by seqid we then reverse the list so they are ordered by seqid and maxtimestamp in descending order and build the time windows all the out of order data into the same compaction windows guaranteeing contiguous compaction based on sequence id 
 do a minor major compaction on an explicit set of storefiles from a store 
 compact a list of files for testing creates a fake link compactionrequestimpl to pass to link compact compactionrequestimpl throughputcontroller user 
 param candidatefiles candidate files ordered from oldest to newest by seqid we rely ondefaultstorefilemanager to sort the files by seqid to guarantee contiguous compaction based on seqid for data consistency 
 dump out a subset of regionserver mbeans only not all of them as json on system out 
 create the flushpolicy configured for the given table 
 get flushpolicy class for the given table 
 update zk or meta this can take a while if for example the hbase meta is not available if server hosting hbase meta crashed and we are waiting on it to come back so run in a thread and keep updating znode state meantime so master doesn t timeout our region in transition caller must cleanup region if this fails 
 creates the writer for the mob file in temp directory 
 creates the writer for the del file in temp directory the del file keeps tracking the delete markers its name has a suffix del the format is  a f  del 
 creates the writer for the mob file in temp directory 
 commits the mob file 
 reads the cell from the mob file and the read point does not count this is used for defaultmobstorecompactor where we can read empty value for the missing cell 
 reads the cell from the mob file 
 initialize this region used only by tests and splittransaction to reopen the region you should use createhregion or openhregion 
 initialize this region 
 open all stores 
 this is a helper function to compute hdfs block distribution on demand
 close down this hregion flush the cache shut down each hstore don t service any more calls this method could take some time to execute so don t call it from a time sensitive thread 
 close down this hregion flush the cache unless abort parameter is true shut down each hstore don t service any more calls this method could take some time to execute so don t call it from a time sensitive thread 
 wait for all current flushes and compactions of the region to complete 
 return the wal link hregionfilesystem used by this region 
 synchronously compact all stores in the region this operation could block for a long time so don t call it from a time sensitive thread note that no locks are taken to prevent possible conflicts between compaction and splitting activities the regionserver does not normally compact and split in parallel however by calling this method you may introduce unexpected and unhandled concurrency don t do this unless you know what you are doing 
 this is a helper function that compact all the stores synchronously it is used by utilities and testing
 this is a helper function that compact the given store it is used by utilities and testing
 called by compaction thread and after region is opened to compact the hstores if necessary this operation could block for a long time so don t call it from a time sensitive thread note that no locking is necessary at this level because compaction only conflicts with a region split and that cannot happen because the region server does them sequentially and not in parallel 
 flush the cache when this method is called the cache will be flushed unless the cache is empty the region is closed a flush is already in progress writes are disabled this method may block for some time so it should not be called from a time sensitive thread 
 flush the cache when this method is called the cache will be flushed unless the cache is empty the region is closed a flush is already in progress writes are disabled this method may block for some time so it should not be called from a time sensitive thread 
 flushing all stores 
 flushing given stores 
 flush the memstore flushing the memstore is a little tricky we have a lot of updates in the memstore all of which have also been written to the wal we need to write those updates in the memstore out to disk while being able to process reads writes as much as possible during the flush operation this method may block for some time every time you call it we up the regions sequence id even if we don t flush i e the returned region id will be at least one larger than the last edit applied to this region the returned id does not refer to an actual edit the returned id can be used for say installing a bulk loaded file just ahead of the last hfile that was the result of this flush etc 
 prepare a delete for a row mutation processor
 this is used only by unit tests not required to be a public api 
 set up correct timestamps in the kvs in delete object caller should have the row and region locks 
 helper method for visiting pending all batch operations
 helper method that checks and prepares only one mutation this can be used to implement link checkandprepare for entire batch note as cp preput predelete hooks may modify mutations this method should be called after preput predelete cp hooks are run for the mutation
 builds separate waledit per nonce by applying input mutations if waledits from cp are present they are merged to result waledit 
 atomically apply the given map of family edits to the memstore this handles the consistency control on its own but the caller should already have locked updateslock readlock this also does not check the families for validity 
 runs preput predelete coprocessor hook for input mutation in a batch
 perform a batch of mutations it supports only put and delete mutations and will ignore other types passed operations in a batch are stored with highest durability specified of for all operations in a batch except for link durability skip wal this function is called from link batchreplay mutationreplay long with link replaybatchoperation instance and link batchmutate mutation long long with link mutationbatchoperation instance as an argument as the processing of replay batchand mutation batch is very similar lot of code is shared by providing generic methods in base class link batchoperation the logic for this method and link dominibatchmutate batchoperation is implemented using methods in base class whichare overridden by derived classes to implement special behavior 
 called to do a piece of the batch that came in to link batchmutate mutation long long in here we also handle replay of edits on region recover 
 checkandmutate and checkandrowmutate are  the same rather than copy paste below has switches in the few places where there is deviation 
 add updates first to the wal and then add values to memstore warning assumption is caller has lock on passed in row 
 see applytomemstore hstore list boolean memstoresizing 
 check the collection of families for validity 
 read the edits put under this region by wal splitting process put the recovered edits back up into this region we can ignore any wal message that has a sequence id that s equal to or lower than minseqid because we know such messages are already reflected in the hfiles while this is running we are putting pressure on memory yet we are outside of our usual accounting because we are not yet an onlined region this stuff is being run as part of region initialization this means that if we re up against global memory limits we ll not be flagged to flush because we are not online we can t be flushed by usual mechanisms anyways we re not yet online so our relative sequenceids are not yet aligned with wal sequenceids not till we come up online post processing of split edits but to help relieve memory pressure at least manage our own heap size flushing if are in excess of per region limits flushing though we have to be careful and avoid using the regionserver wal sequenceid its running on a different line to whats going on in here in this region context so if we crashed replaying these edits but in the midst had a flush that used the regionserver wal with a sequenceid in excess of whats going on in here in this region and with its split editlogs then we could miss edits the next time we go to recover so we have to flush inline using seqids that make sense in a this single region context only until we online 
 call to complete a compaction its for the case where we find in the wal a compaction that was not finished we could find one recovering a wal after a regionserver crash see hbase  
 replay the flush marker from primary region by creating a corresponding snapshot of the store memstores only if the memstores do not have a higher seqid from an earlier wal edit because the events may be coming out of order 
 be careful this method will drop all data in the memstore of this region currently this method is used to drop memstore to prevent memory leak when replaying recovered edits while opening region 
 drops the memstore contents after replaying a flush descriptor or region open event replay if the memstore edits have seqnums smaller than the given seq id
 get an exclusive write lock lock on a given row 
 attempts to atomically load a group of hfiles this is critical for loading rows with multiple column families atomically 
 attempts to atomically load a group of hfiles this is critical for loading rows with multiple column families atomically 
 return true if more cells exist after this batch false if scanner is done
 a utility method to create new instances of hregion based on the link hconstants region impl configuration property 
 convenience method creating new hregions used by createtable 
 open a region 
 open a region 
 open a region 
 open a region 
 open a region 
 open a region 
 open a region 
 useful when reopening a closed region normally for unit tests 
 open hregion calls initialize and sets sequenceid 
 open a region on a read only file system like hdfs snapshots 
 perform atomic all or none mutations within the region 
 add deltas to cells deltas are increments or appends switch on op if increment add deltas to current values or if an append then append the deltas to the current cell values append and increment code paths are mostly the same they differ in just a few places this method does the code path for increment and append and then in key spots switches on the passed in op to do increment or append specific paths 
 return writeentry associated with this append
 reckon the cells to apply to wal memstore and to return to the client these sets are not always the same dependent on whether to write wal 
 reckon the cells to apply to wal memstore and to return to the client in passed column family store does get of current value and then adds passed in deltas for this store returning the result 
 do a specific get on passed columnfamily and column qualifiers 
 this method needs to be called before any public call that reads or modifies stores in bulk it has to be called just before a try closebulkregionoperation needs to be called in the try s finally block acquires a writelock and checks if the region is closing or closed 
 clean up any temp detritus that may have been left around from previous operation attempts 
 create the store directory for the specified family name
 returns the store files available for the family this methods performs the filtering based on the valid store files 
 check whether region has reference file
 remove the region family from disk archiving the store files 
 move the file from a build temp location to the main family store directory 
 generate the filename in the main family store directory for moving the file from a build temp location 
 bulk load add a specified store file to the specified family if the source file is on the same different file system is moved from the source location to the destination location otherwise is copied over 
 clean up any split detritus that may have been left around from previous split attempts 
 clean up any split detritus that may have been left around from previous split attempts call this method on initial region deploy 
 remove daughter region
 commit a daughter region moving it from the split temporary directory to the proper location in the filesystem 
 create the region splits directory 
 clean up any merge detritus that may have been left around from previous merge attempts 
 create the region merges directory 
 write out an info file under the stored region directory useful recovering mangled regions if the regioninfo already exists on disk then we fast exit 
 write out an info file under the region directory useful recovering mangled regions 
 write out an info file under the region directory useful recovering mangled regions 
 create a new region on file system 
 open region from file system 
 creates a directory assumes the user has already checked for this directory existence 
 renames a directory assumes the user has already checked for this directory existence 
 deletes a directory assumes the user has already checked for this directory existence 
 sleeping logic handles the interrupt exception 
 creates a directory for a filesystem and configuration object assumes the user has already checked for this directory existence 
 sleeping logic for static methods handles the interrupt exception keeping a static version for this to avoid re looking for the integer values 
 starts a hregionserver at the default location
 setup our cluster connection if not already initialized 
 bring up connection to zk ensemble and then wait until a master for this cluster and then after that wait until cluster up flag has been set this is the order in which master does things finally open long living server short circuit connection 
 the hregionserver sticks in this loop until closed 
 param encodedregionname
 setup wal log and replication if enabled replication setup is done in here because it wants to be hooked up to wal 
 puts up the webui 
 load the replication executorservice objects if any
 utility for constructing an instance of the passed hregionserver class 
 try to close the region logs a warning on failure but continues 
 close asynchronously a region can be called from the master or internally by the regionserver when stopping if called from the master the region will update the znode status if an opening was in progress this method will cancel it but will not start a new close the coprocessors are not called in this case a notservingregionexception exception is thrown if a close was in progress this new request will be ignored and an exception thrown 
 protected utility method for safely obtaining an hregion handle 
 constructor
 creates an unsorted list of storefile loaded in parallel from the given directory 
 replaces the store files that the store has with the given files mainly used by secondary region replicas to keep up to date with the primary region files 
 checks the underlying store files and opens the files that have not been opened and removes the store file readers for store files no longer available mainly used by secondary region replicas to keep up to date with the primary region files 
 close all the readers we don t need to worry about subsequent requests because the region holds a write lock that will prevent any more reads or writes 
 write out current snapshot presumes link snapshot has been called previously 
 param path the pathname of the tmp file into which the store was flushed
 change storefiles adding into place the reader produced by this new flush 
 get all scanners with no filtering based on ttl that happens further down the line 
 create scanners on the given files and if needed on the memstore with no filtering based on ttl that happens further down the line 
 compact the storefiles this method may take some time so the calling thread must be able to block for long periods during this time the store can work as usual getting values from storefiles and writing new storefiles from the memstore existing storefiles are not destroyed until the new compacted storefile is completely written out to disk the compactlock prevents multiple simultaneous compactions the structurelock prevents us from interfering with other write operations we don t want to hold the structurelock for the whole time as a compact can be lengthy and we want to allow cache flushes during this period compaction event should be idempotent since there is no io fencing for the region directory in hdfs a region server might still try to complete the compaction after it lost the region that is why the following events are carefully ordered for a compaction  compaction writes new files under region tmp directory compaction output  compaction atomically moves the temporary file under region directory  compaction appends a wal edit containing the compaction input and output files forces sync on wal  compaction deletes the input files from the region directory failure conditions are handled like this if rs fails before  compaction wont complete even if rs lives on and finishes the compaction later it will only write the new data file to the region directory since we already have this data this will be idempotent but we will have a redundant copy of the data if rs fails between  and  the region will have a redundant copy of the data the rs that failed won t be able to finish snyc for wal because of lease recovery in wal if rs fails after  the region region server who opens the region will pick up the the compaction marker from the wal and replay it by removing the compaction input files failed rs can also attempt to delete those files but the operation will be idempotent see hbase  for details 
 call to complete a compaction its for the case where we find in the wal a compaction that was not finished we could find one recovering a wal after a regionserver crash see hbase  
 this method tries to compact n recent files for testing note that because compacting recent files only makes sense for some policies e g the default one it assumes default policy is used it doesn t use policy but instead makes a compaction candidate list by itself 
 return a scanner for both the memstore and the hstore files assumes we are not in a compaction 
 recreates the scanners on the current list of active store file scanners
 similar to commit but called in secondary region replicas for replaying the flush cache from primary region adds the new files to the store and drops the snapshot depending on dropmemstoresnapshot argument 
 abort the snapshot preparation drops the snapshot if any 
 closes and archives the compacted files under this store
 archives and removes the compacted files
 opens reader on this store file called by constructor 
 initialize the reader used for pread 
 get a scanner which uses streaming read must be called after initreader 
 param evictonclose whether to evict blocks belonging to this file
 delete this file
 grab the next row s worth of values 
 constructor 
 gets the next row of keys from the top most scanner this method takes care of updating the heap this can only be called when you are using scanners that implement internalscanner as well as keyvaluescanner a link storescanner 
 seeks all scanners at or below the specified seek key if we earlied out of a row we may end up skipping values that were never reached yet rather than iterating down we want to give the opportunity to re seek as individual scanners may run past their ends those scanners are automatically closed and removed from the heap this function and link reseek cell does not do multi column bloom filter and lazy seek optimizations to enable those call link requestseek cell boolean boolean 
 this function is identical to the link seek cell function exceptthat scanner seek seekkey is changed to scanner reseek seekkey 
 inheritdoc 
 param islazy whether we are trying to seek to exactly the given row col enables bloom filter and most recent file first optimizations for multi column get scan queries 
 see thread run 
 create a lease and insert it to the map of leases 
 client explicitly cancels a lease 
 the worker thread performs the compaction asynchronously the solo per compactor thread only reads the compaction pipeline there is at most one thread per memstore instance 
 check if the regionserver s memstore memory usage is greater than the limit if so flush regions with the biggest memstores until we re down to the lower limit this method blocks callers until we re down to a safe amount of memstore consumption 
 wait for the global readpoint to advance up to the passed in write entry number 
 atomically checks if a row family qualifier value matches the expected value and if it does it performs the mutation if the passed value is null the lack of column value ie non existence is used see checkandrowmutate to do many checkandputs at a time on a single row 
 atomically checks if a row family qualifier value matches the expected values and if it does it performs the row mutations if the passed value is null the lack of column value ie non existence is used use to do many mutations on a single row use checkandmutate to do one checkandmutate at a time 
 constructor
 sanity check the table coprocessor attributes of the supplied schema will throw an exception if there is a problem 
 create the regionsplitpolicy configured for the given table 
 do a backwardseek in a reversed storescanner scan backward 
 checks for the following pre checks in order regionserver is running if authorization is enabled then rpc caller has admin permissions 
 mutate a list of rows atomically 
 execute an append mutation 
 execute an increment mutation 
 run through the regionmutation rm and per mutation do the work and then when done add an instance of a link resultorexception that corresponds to each mutation 
 execute a list of put delete mutations 
 close a region on the region server 
 compact a region on the region server 
 flush a region on the region server 
 get some information of the region server 
 open asynchronously a region or a set of regions on the region server the opening is coordinated by zookeeper and this method requires the znode to be created before being called as a consequence this method should be called only from the master different manages states for the region are region not opened the region opening will start asynchronously a close is already in progress this is considered as an error an open is already in progress this new open request will be ignored this is important because the master can do multiple requests if it crashes the region is already opened this new open request will be ignored bulk assign if there are more than  region to open it will be considered as a bulk assign for a single region opening errors are sent through a serviceexception for bulk assign errors are put in the response as failed opening 
 wamrmup a region on this server this method should only be called by master it synchrnously opens the region and closes the region bringing the most important pages in cache 
 replay the given changes when distributedlogreplay wal edits from a failed rs the guarantee is that the given mutations will be durable on the receiving rs if this method returns without any exception 
 replicate wal entries on the region server 
 roll the wal writer of the region server 
 atomically bulk load several hfiles into an open region
 get data from a table 
 execute multiple actions on a table get mutate and or execcoprocessor
 mutate data in a table 
 scan data in a table 
 seek the scanner at or before the row of specified cell it firstly tries to seek the scanner at or after the specified cell return if peek keyvalue of scanner has the same row with specified cell otherwise seek the scanner at the first cell of the row which is the previous row of specified keyvalue
 seek the scanner at the first cell of the row which is the previous row of specified key
 seek the scanner at the first keyvalue of last row
 starts the operation if operation with such nonce has not already succeeded if the operation is in progress waits for it to end and checks whether it has succeeded 
 do a flush snapshot of every region on this rs from the target table 
 if in a running state creates the specified subprocedure for handling an online snapshot because this gets the local list of regions to snapshot and not the set the master had there is a possibility of a race where regions may be missed this detected by the master in the snapshot verification step 
 wait for all of the currently outstanding tasks submitted via link submittask callable this must be called after all tasks are submitted via submittask 
 create the storeengine configured for the given store 
 open a reader for the storefile
 compute the hdfs block distribution for this storefile
 return the link filestatus of the file 
 return get the modification time of the file 
 return an array of scanners corresponding to the given set of store files 
 return an array of scanners corresponding to the given set of store files 
 pretend we have done a seek but don t do it yet if possible the hope is that we find requested columns in more recent files and won t have to seek in older files creates a fake key value with the given row column and the highest most recent possible timestamp we might get from this file when users of such lazy scanner need to know the next kv precisely e g when this scanner is at the top of the heap they run link enforceseek note that this function does guarantee that the current kv of this scanner will be advanced to at least the given kv because of this it does have to do a real seek in cases when the seek timestamp is older than the highest timestamp of the file e g when we are trying to seek to the next row column and use oldest timestamp in the seek key 
 writes meta data call before link close since its written as meta data to this file 
 writes meta data call before link close since its written as meta data to this file 
 add timestamprange and earliest put timestamp to metadata
 create a store file writer client is responsible for closing file when done if metadata add before closing using link storefilewriter appendmetadata 
 opens a scanner across memstore snapshot and all storefiles assumes we are not in a compaction 
 seek the specified scanners with the given key
 get the next row of values from this store 
 return if top of heap has changed and keyvalueheap has to try the next kv 
 do a reseek in a normal storescanner scan forward 
 seek storefiles in parallel to optimize io latency as much as possible
 gets the mid point of the largest file passed in as split point 
 override of getsplitpoint that determines the split point as the boundary between two stripes unless it causes significant imbalance between split sides sizes in that case the split boundary will be chosen from the middle of one of the stripes to minimize imbalance 
 param data the serialization data it can t be null 
 this method used to serialize timerangetracker trt by protobuf while this breaks the forward compatibility on hfile see hbase  in previous hbase version     we use dataoutput to serialize trt these old versions don t have capability to deserialize trt which is serialized by protobuf so we need to revert the change of serializing timerangetracker back to dataoutput for more information please check hbase  
 a log file has a creation timestamp in ms in its file name link filenum this helper method returns the creation timestamp from a given log file it extracts the timestamp assuming the filename is created with the link computefilename long filenum method 
 archive old logs a wal is eligible for archiving if all its waledits have been flushed 
 cleans up current writer closing it and then puts in place the passed in nextwriter in the case of creating a new wal oldpath will be null in the case of rolling over from one file to the next none of the parameters will be null in the case of closing out this fshlog with no further use newpath and nextwriter will be null 
 pass one or more log file names and it will either dump out a text version on stdout or split the specified log files 
 command line tool to compress and uncompress wals 
 reads the next compressed entry and returns it as a byte array
 reads a compressed entry into an array the output into the array ends up length prefixed 
 compresses and writes an array to a dataoutput
 for thread a to call when it is ready to wait on the safe point to be attained thread a will be held in here until thread b calls link safepointattained 
 called by thread b when it attains the safe point in this method thread b signals thread a it can proceed thread b will be held in here until link releasesafepoint is calledby thread a 
 check if we should attain safe point if so go there and then wait till signalled before we proceeding 
 append to the wal does all cp and wal listener calls 
 to check whether a trailer is present in a wal it seeks to position filelength pb wal complete magic size bytes sizeof int it reads the int value to know the size of the trailer and checks whether the trailer is present at the end or not by comparing the last pb wal complete magic size bytes in case trailer is not present it returns false otherwise sets the trailer and sets this waleditsstopoffset variable up to the point just before the trailer the trailer is ignored in case filelength is  or not correct when file is under recovery etc the trailer size is negative in case the trailer size  this trailermaxsize it is read after a warn message 
 write the marker that a compaction has succeeded and is about to be committed this provides info to the hmaster to allow it to recover the compaction if this regionserver dies in the middle it also prevents the compaction from finishing if this regionserver has already lost its lease on the log this write is for internal use only not for external client consumption 
 write a flush marker indicating a start abort or a complete of a region flush this write is for internal use only not for external client consumption 
 write a region open marker indicating that the region is opened this write is for internal use only not for external client consumption 
 write a log marker that a bulk load has succeeded and is about to be committed this write is for internal use only not for external client consumption 
 returns a default set of filters 
 a private method used to re establish a zookeeper session with a peer cluster 
 get a list of all the addresses of all the region servers for this peer cluster
 do the sleeping logic
 do the shipping logic
 create a sink for replication
 do the changes and handle the pool
 do the sleeping logic
 adds a normal source per registered peer cluster and tries to process all old region server wal queues
 add sources for the given peer cluster on this region server for the newly added peer we only need to enqueue the latest log of each wal group and do replication
 factory method to create a replication source
 do the shipping logic
 do the sleeping logic
 retrieves the next batch of wal entries from the queue waiting up to the specified time for a batch to become available
 return true if there is another wal link entry 
 return the next wal entry in this stream
 inheritdoc 
 should be called if the stream is to be reused i e used again after hasnext has returned false 
 returns whether the file is opened for writing 
 authorizes that the current user has global privileges for the given action 
 observer implementations 
 verifies user has create privileges on the column families involved in the bulkloadhfile request specific column write privileges are presently ignored 
 authorization check for securebulkloadprotocol preparebulkload 
 authorization security check for securebulkloadprotocol cleanupbulkload 
 stores a new user permission grant in the access control lists table 
 removes a previously granted permission from the stored access control lists the link tablepermission being removed must exactly match whatis stored no wildcard matching is attempted ie if user bob has been granted read access to the data table but only to column family plus qualifier info cola then trying to call this method with only user bob and the table name data but without specifying the column qualifier info cola will have no effect 
 remove specified table column from the acl table 
 returns the currently granted permissions for a given table as a list of user plus associated permissions 
 reads a set of permissions as link org apache hadoop io writable instances from the inputstream 
 updates the internal global permissions cache
 returns a tableauthmanager from the cache if not cached constructs a new one returned instance should be released back by calling link release tableauthmanager 
 inheritdoc 
 obtain and return an authentication token for the current user 
 obtain and return an authentication token for the current user 
 obtain an authentication token for the given user and add it to the user s credentials 
 obtain an authentication token on behalf of the given user and add it to the credentials for the given map reduce job 
 obtain an authentication token on behalf of the given user and add it to the credentials for the given map reduce job 
 checks for an authentication token for the given user obtaining a new token if necessary and adds it to the credentials for the given map reduce job 
 checks for an authentication token for the given user obtaining a new token if necessary and adds it to the credentials for the given map reduce job 
 checks if an authentication tokens exists for the connected cluster obtaining one if needed and adding it to the user s credentials 
 param putvistags visibility tags in put mutation
 checks whether cell contains any tag with type as visibility tag type this tag type is reserved and should not be explicitly set by user 
 checks whether cell contains any tag with type as visibility tag type this tag type is reserved and should not be explicitly set by user there are two versions of this method one that accepts pair and other without pair in case of preappend and preincrement the additional operations are not needed like checking for string vis tag type and hence the api without pair could be used 
 visibilityendpoint service related methods 
 creates the singleton instance if not yet present and returns the same 
 param conf
 restore the on disk table to a specified snapshot state 
 restore specified regions by restoring content to the snapshot state 
 restore specified mob regions by restoring content to the snapshot state 
 restore region by removing files not in the snapshot and adding the missing ones from the snapshot 
 restore mob region by removing files not in the snapshot and adding the missing ones from the snapshot 
 restore region by removing files not in the snapshot and adding the missing ones from the snapshot 
 clone specified regions for each region create a new region and create a hfilelink for each hfile 
 clone the mob region for the region create a new region and create a hfilelink for each hfile 
 clone region directory content from the snapshot info each region is encoded with the table name so the cloned region will have a different region name instead of copying the hfiles a hfilelink is created 
 clone region directory content from the snapshot info each region is encoded with the table name so the cloned region will have a different region name instead of copying the hfiles a hfilelink is created 
 create a new link hfilelink to reference the store file the store file in the snapshot can be a simple hfile an hfilelink or a reference hfile abc  table region abc reference abc   table region abc  hfilelink table region hfile  table region hfile 
 copy the snapshot files for a snapshot scanner discards meta changes 
 convert the passed snapshot description into a full snapshot description based on default parameters if none have been supplied this resolves any optional parameters that aren t supplied to their default values 
 collect the hfiles and logs statistics of the snapshot and dump the file list if requested and the collected information 
 returns the snapshot stats
 returns the snapshot stats
 returns the map of store files based on path for all snapshots
 return a snapshotmanifest instance with the information already loaded in memory snapshotmanifest manifest   snapshotmanifest open tabledescriptor htd   manifest gettabledescriptor for snapshotregionmanifest regionmanifest manifest getregionmanifests hri   regionmanifest getregioninfo for regionmanifest getfamilyfiles 
 creates a manifest for the specified region by reading directly from the hregion object this is used by the online snapshot when the table is enabled 
 creates a manifest for the specified region by reading directly from the disk this is used by the offline snapshot when the table is disabled 
 load the information in the snapshotmanifest called by snapshotmanifest open if the format is v and there is no data manifest means that we are loading an in progress snapshot since we support rolling upgrades we loook for v and v regions format 
 iterate over the snapshot store files
 iterate over the snapshot store files restored edits and logs
 ?iterate over the snapshot store files
 iterate over the snapshot store files in the specified region
 verify the validity of the snapshot
 verify the validity of the snapshot
 returns the store file names in the snapshot 
 returns the store file names in the snapshot 
 canary entry point for specified table 
 check hfile contents are readable by hbase  
 prepare a collection of link loadqueueitem from list of source hfiles contained in thepassed directory and validates whether the prepared queue has all the valid table column families in it 
 prepare a collection of link loadqueueitem from list of source hfiles contained in thepassed directory and validates whether the prepared queue has all the valid table column families in it 
 prepare a collection of link loadqueueitem from list of source hfiles contained in thepassed directory and validates whether the prepared queue has all the valid table column families in it 
 perform a bulk load of the given directory into the given pre existing table this method is not threadsafe 
 perform a bulk load of the given directory into the given pre existing table this method is not threadsafe 
 perform a bulk load of the given directory into the given pre existing table this method is not threadsafe 
 used by the replication sink to load the hfiles from the source cluster it does the following loadincrementalhfiles grouporsplitphase table executorservice deque pair loadincrementalhfiles bulkloadphase table connection executorservice deque multimap 
 used by the replication sink to load the hfiles from the source cluster it does the following loadincrementalhfiles grouporsplitphase table executorservice deque pair loadincrementalhfiles bulkloadphase table connection executorservice deque multimap 
 this takes the lqi s grouped by likely regions and attempts to bulk load them any failures are re queued for another pass with the grouporsplitphase protected for testing 
 param table the table to load into
 attempt to assign the given load queue item into its target region group if the hfile boundary no longer fits into a region physically splits the hfile such that the new bottom half will fit and returns the list of lqi s corresponding to the resultant hfiles protected for testing
 if the table is created for the first time then completebulkload reads the files twice more modifications necessary if we want to avoid doing it 
 walk the given directory for all hfiles and return a queue containing all such files 
 iterate over the bulkdir hfiles skip reference hfilelink files starting with and non valid hfiles 
 iterate over the bulkdir hfiles skip reference hfilelink files starting with check and skip non valid hfiles by default or skip this validation by setting hbase loadincremental validate hfile to false 
 split a storefile into a top and bottom half maintaining the metadata recreating bloom filters etc 
 perform bulk load on the given table 
 perform bulk load on the given table 
 bloom information from the cell is retrieved
 loads bloom filter meta data from file input 
 determines amp initializes bloom filter meta data from user config call link allocbloom to allocate bloom filter data 
 writes just the bloom filter to the output array
 instantiates the correct bloom filter class based on the version provided in the meta block data 
 caller doesn t close the admin afterwards we need to manage it and close it properly 
 caller closes the table afterwards 
 retrieve a regionlocator for the table the user should close the regionlocator 
 return the current amount of direct memory used 
 directbytebuffers are garbage collected by using a phantom reference and a reference queue every once a while the jvm checks the reference queue and cleans the directbytebuffers however as this doesn t happen immediately after discarding all references to a directbytebuffer it s easy to outofmemoryerror yourself using directbytebuffers this function explicitly calls the cleaner method of a directbytebuffer 
 check that the specified cipher can be loaded and initialized or throw an exception verifies key and cipher provider configuration as a prerequisite for cipher verification 
 param fs
 recover the lease from hdfs retrying multiple times 
 call hdfs  isfileclosed if it is available 
 param fsreadonly true if we are read only when it comes to filesystemoperations i e on remove we do not do delete in fs 
 get the current table descriptor for the given table or null if none exists uses a local cache of the descriptor but still checks the filesystem on each call to see if a newer file has been created since the cached one was read 
 returns a map from table name to table descriptor for all tables 
 find descriptors by namespace 
 adds or updates the table descriptor to the filesystem and updates the local cache with it 
 checks if a current table info file exists for the given table
 find the most current table info file for the given table in the hbase root directory 
 find the most current table info file for the table located in the given table directory looks within the link tableinfo dir subdirectory of the given directory for any table infofiles and takes the current one meaning the one with the highest sequence number if present or no sequence number at all if none exist for backward compatibility from before there were sequence numbers 
 find the most current table info file for the table in the given table directory looks within the link tableinfo dir subdirectory of the given directory for any table infofiles and takes the current one meaning the one with the highest sequence number if present or no sequence number at all if none exist for backward compatibility from before there were sequence numbers if there are multiple table info files found and removeoldfiles is true it also deletes the older files 
 param p path to a tableinfo file 
 returns the latest table descriptor for the given table directly from the file system if it exists bypassing the local cache returns null if it s not found 
 returns the latest table descriptor for the table located at the given directory directly from the file system if it exists 
 update table descriptor on the file system
 deletes all the table descriptor files from the file system used in unit tests only 
 attempts to write a new table descriptor to the given table s directory it first writes it to the tmp dir then uses an atomic rename to move it into place it begins at the currentsequenceid   and tries  times to find a new sequence number not already in use removes the current descriptor file if passed in 
 create new tabledescriptor in hdfs happens when we are creating table used by tests 
 create new tabledescriptor in hdfs happens when we are creating table if forcecreation is true then even if previous table descriptor is present it will be overwritten
 create a new tabledescriptor in hdfs in the specified table directory happens when we create a new table or snapshot a table 
 create the specified file on the filesystem by default this will overwrite the file if it exists apply the umask in the configuration if it is enabled use the fs configured buffer size or  if not set use the configured column family replication or default replication if link hcolumndescriptor default dfs replication use the default block size not track progress 
 we use reflection because link distributedfilesystem setsafemode hdfsconstants safemodeaction action boolean ischecked is not in hadoop  
 check whether dfs is in safemode 
 verifies current version of file system
 verifies current version of file system
 verifies current version of file system
 sets version of file system
 sets version of file system
 sets version of file system
 checks that a cluster id file exists in the hbase root directory
 returns the value of the unique cluster id stored for this hbase instance 
 param cid
 writes a new unique identifier for this cluster to the hbase id file in the hbase root directory
 if dfs check safe mode and if so wait until we clear it 
 returns the total overall fragmentation percentage includes hbase meta and root as well 
 runs through the hbase rootdir and checks how many stores for each table have more than one file in them checks root and hbase meta too the total percentage across all tables is stored under the special key total 
 runs through the hbase rootdir and checks how many stores for each table have more than one file in them checks root and hbase meta too the total percentage across all tables is stored under the special key total 
 given a particular table dir return all the regiondirs inside it excluding files such as tableinfo
 runs through the hbase rootdir tablename and creates a reverse lookup map for table storefile names to the full path example key    value   hdfs localhost  user userid root  info 
 runs through the hbase rootdir tablename and creates a reverse lookup map for table storefile names to the full path note that because this method can be called on a live hbase system that we will skip files that no longer exist by the time we traverse them and similarly the user of the result needs to consider that some entries in this map may not exist by the time this call completes example key    value   hdfs localhost  user userid root  info 
 runs through the hbase rootdir and creates a reverse lookup map for table storefile names to the full path example key    value   hdfs localhost  user userid root  info 
 runs through the hbase rootdir and creates a reverse lookup map for table storefile names to the full path example key    value   hdfs localhost  user userid root  info 
 this function is to scan the root path of the file system to get the degree of locality for each region on each of the servers having at least one block of that region this is used by the tool link org apache hadoop hbase master regionplacementmaintainer 
 this function is to scan the root path of the file system to get the degree of locality for each region on each of the servers having at least one block of that region 
 this function is to scan the root path of the file system to get either the mapping between the region name and its best locality region server or the degree of locality of each region on each of the servers having at least one block of that region the output map parameters are both optional 
 param c
 iterate over the table store files
 iterate over the region store files
 constructor
 to repair region consistency one must call connect in order to repair online state 
 get deployed regions according to the region servers 
 this repair method analyzes hbase data in hdfs and repairs it to satisfy the table integrity rules hbase doesn t need to be online for this operation to work 
 this repair method requires the cluster to be online since it contacts region servers and the masters it makes each region s state in hdfs in hbase meta and deployments consistent 
 contacts the master and prints out cluster wide information
 iterates through the list of all orphan invalid regiondirs 
 orphaned regions are regions without a regioninfo file in them we adopt these orphans by creating a new region and moving the column families recovered edits wals into the new region dir we determine the region startkey and endkeys by looking at all of the hfiles inside the column families to identify the min and max keys the resulting region will likely violate table integrity but will be dealt with by merging overlapping regions 
 this method determines if there are table integrity errors in hdfs if there are errors and the appropriate fix options are enabled the method will first correct orphan regions making them into legit regiondirs and then reload to merge potentially overlapping regions 
 scan all the store file names to find any lingering reference files which refer to some none exiting files if fix option is enabled any lingering reference file will be sidelined if found lingering reference file prevents a region from opening it has to be fixed before a cluster can start properly 
 scan all the store file names to find any lingering hfilelink files which refer to some none exiting files if fix option is enabled any lingering hfilelink file will be sidelined if found 
 populate hbi s from regioninfos loaded from file system 
 to fix the empty regioninfo qualifier rows from hbase meta 
 to fix orphan table by creating a tableinfo file under tabledir  if tableinfo is cached to recover the tableinfo accordingly  else create a default tableinfo file with following items nbsp   the correct tablename nbsp   the correct colfamily list nbsp   the default properties for both link tabledescriptor and link columnfamilydescriptor 
 suggest fixes for each table
 rebuilds meta from information in hdfs fs depends on configuration settings passed into hbck constructor to point to a particular fs dir assumes hbase is offline 
 sideline a region dir instead of deleting it 
 sideline a region dir instead of deleting it 
 return path to backup of original directory
 scan hdfs for all regions recording their information into regioninfomap
 contacts each regionserver and fetches metadata about regions 
 check consistency of all regions that have been found in previous phases 
 check consistency of all regions using mulitple threads concurrently 
 deletes region from meta table
 attempts to undeploy a region from a region server based in information in meta any operations that modify the file system should make sure that its corresponding region is not deployed to prevent data races a separate call is required to update the master in memory region state kept in the assignementmanager because disable uses this state instead of that found in meta we can t seem to cleanly disable delete tables that have been hbck fixed when used on a version of hbase that does not have the offline ipc call exposed on the master lt    lt    a master restart or failover may be required 
 check a single region for consistency and correct deployment 
 checks tables integrity goes over all regions and scans the tables collects all the pieces for each table and checks if there are missing repeated or overlapping ones 
 merge hdfs data by moving from contained hbckinfo into targetregiondir 
 this takes set of overlapping regions and merges them into a single region this covers cases like degenerate regions shared start key general overlaps duplicate ranges and partial overlapping regions cases clean regions that overlap only oldlogs regions can t find start stop range or figure out this is basically threadsafe except for the fixer increment in mergeoverlaps 
 sideline some regions in a big overlap group so that it will have fewer regions and it is easier to merge them later on 
 check the region chain from meta of this table we are looking for holes overlaps and cycles 
 check values in regioninfo for hbase meta check if zero or more than one regions with hbase meta are found if there are inconsistencies i e zero or more than one regions pretend to be holding the hbase meta try to fix that and report an error 
 ls r for debugging purposes
 ls r for debugging purposes
 ls r for debugging purposes
 fix multiple assignment by doing silent closes on each rs hosting the region and then force zk unassigned node to offline to trigger assignment by master 
 fix unassigned by creating transition the unassigned zk node for this region to offline state with a special flag to tell the master that this is a forced operation by hbck this assumes that info is in meta 
 checks a path to see if it is a valid hfile 
 check all files in a column family dir 
 check all files in a mob column family dir 
 checks a path to see if it is a valid mob file 
 checks all the mob files of a table 
 check all column families in a region dir 
 check all the regiondirs in the specified tabledir
 check the specified table dirs for bad hfiles 
 get the directory to archive a store directory
 get the path to the table archive directory based on the configured archive directory assumed that the table should already be archived 
 get the full path to the archive directory on the configured link org apache hadoop hbase master masterfilesystem 
 creates a link regionserverthread call start on the returned thread to make it run 
 creates a link masterthread call start on the returned thread to make it run 
 start the cluster waits until there is a primary master initialized and returns its address 
 utility method to wait some time for an event to occur and then return control to the caller 
 param masters
 create new set of regions on the specified file system note that you should add the regions to hbase meta after this operation 
 create new set of regions on the specified file system note that you should add the regions to hbase meta after this operation 
 create new set of regions on the specified file system 
 execute the task on the specified set of regions 
 close the open connections and shutdown the batchpool
 this method builds the appropriate regionmover object which can then be used to load unload using load and unload methods
 loads the specified link hostname with regions listed in the link filename regionmoverobject has to be created using link regionmover regionmoverbuilder 
 unload regions from given link hostname using ack noack mode and link maxthreads in noack mode we do not make sure that region is successfully online on the target region server hence it is best effort we do not unload regions to hostnames given in link excludefile 
 return list of servers from the exclude file in format hostname port 
 excludes the servername whose hostname and port portion matches the list given in exclude file
 returns true if passed region is still on servername when we look at hbase meta 
 the main function for the regionsplitter application common uses create a table named mytable with  pre split regions containing  column families test amp rs assuming the keys are hex encoded ascii bin hbase org apache hadoop hbase util regionsplitter c  f test rs mytable hexstringsplit create a table named mytable with  pre split regions assuming the keys are decimal encoded ascii bin hbase org apache hadoop hbase util regionsplitter c  mytable decimalstringsplit perform a rolling split of mytable i e    gt  regions  outstanding splits at a time assuming keys are uniformly distributed bytes bin hbase org apache hadoop hbase util regionsplitter r o  mytable uniformsplit there are three splitalgorithms built into regionsplitter hexstringsplit decimalstringsplit and uniformsplit these are different strategies for choosing region boundaries see their source code for details 
 throws ioexception if the specified splitalgorithm class couldn t beinstantiated
 create a new link com codahale metrics histogram instance these constructors arenot public in    so we use reflection to find them 
 method for table states migration used when upgrading from pre   to   reading state from zk applying them to internal state and delete used by master to clean migration from zk based states to table descriptor based states 
 param factory factory that made us identity used for fs layout may not be null
 it returns the file create timestamp from the file name for name format see link validatewalfilename string public until remaining tests move to o a h h wal
 opens wal reader with retries and additional exception handling
 public because of asyncfswal should be package private
 public because of asyncfswal should be package private
 public because of fshlog should be package private
 public because of fshlog should be package private
 instantiate a strategy from a config property requires conf to have already been set as well as anything the provider might need to read 
 instantiate a provider from a config property requires conf to have already been set as well as anything the provider might need to read 
 param conf must not be null will keep a reference to read params in later reader writerinstances 
 param region the region which we want to get a wal for it could be null 
 create a reader for the wal if you are reading from a file that s being written to and need to reopen it multiple times use link wal reader reset instead of this methodthen just seek back to the last known good position 
 pass one or more log file names and formatting options and it will dump out a text version of the contents on stdout 
 splits a wal file into region s recovered edits directory this is the main entry point for distributed log splitting from splitlogworker if the log file has n regions then n recovered edits files will be produced 
 log splitting implementation splits one log file 
 completes the work done by splitlogfile by archiving logs it is invoked by splitlogmanager once it knows that one of the splitlogworkers have completed the splitlogfile part if the master crashes then this function might get called multiple times 
 check whether there is recovered edits in the region dir
 get the max sequence id which is stored in the region directory  if none 
 create a file with name as region s max sequence id
 create a new link reader for reading logs to split 
 append a log entry into the corresponding region buffer blocks if the total heap usage has crossed the specified threshold 
 wait for writer threads to dump all info to the sink
 return null if failed to report progress
 close all of the output streams 
 param completionservice threadpool to execute the closing tasks
 get a writer and path for a log starting at the given entry this function is threadsafe so long as multiple threads are always acting on different regions 
 return a path with a write for that path caller should close 
 append the buffer to a new recovered edits file then close it after all done
 logs the errors without failing on exception 
 read the content of znode file expects a single line 
 delete the master znode if its content servername string is the same as the one in the znode file env hbase znode file i case of master rs colloaction we extract servername string from rsznode path hbase  
 test archiving cleaning across multiple tables where some are retained and others aren t
 get all the files non directory entries in the file system under the passed directory
 param cleaner
 test that the store files are archived when a column family is removed 
 get the names of all the files below the given directory
 recursively lookup all the file names under the file array 
 test that an operation can fail if we read the global operation timeout even if the individual timeout is fine we do that with client side an operation timeout of  seconds server side we sleep  second at each attempt the first work fails the second one succeeds but the client won t wait that much because      so the client timed out when the server answers 
 verify that tables created from the snapshot are still alive after source table deletion 
 initialize the tests with a table filled with some data and two snapshots snapshotname snapshotname of different states the tablename snapshotnames and the number of rows in the snapshot are initialized 
 test that abort is called when lockheartbeat fails with ioexception 
 initialize the tests with a table filled with some data and two snapshots snapshotname snapshotname of different states the tablename snapshotnames and the number of rows in the snapshot are initialized 
 tests forcing split from client and having scanners successfully ride over split 
 multi family scenario tests forcing split from client and having scanners successfully ride over split 
 test that we can handle connection close it will trigger a retry but the calls will finish 
 test that connection can become idle without breaking everything 
 test that the connection to the dead server is cut immediately when we receive the notification 
 test that when we delete a location using the first row of a region that we really delete it 
 we were only clearing rows that had a hregioninfo column in hbase meta mangled rows that were missing the hregioninfo because of error were being left behind messing up any subsequent table made with the same name hbase 
 the workers are going to perform a couple of reads the second read will follow the killing of a regionserver so that we make sure that some of threads go into preemptivefastfailexcception
 test append result when there are duplicate rpc request 
 test from client side of an involved filter against a multi family that involves deletes 
 test filters when multiple regions it does counts needs eye balling of logs to ensure that we re not scanning more regions that we re supposed to related to the testfilteracrossregions over in the o a h h filter package 
 test simple table and non existent row cases 
 test basic puts gets scans and deletes for a single row in a multiple family table 
 hbase  if millions of columns in a column family hbase scanner won t come up test will create numrows rows each with numcolsperrow columns  version each and attempt to scan them all to test at scale up numcolsperrow to the millions have not gotten that to work running as junit though 
 hbase  get with timestamp will return a value if there is a version with an earlier timestamp
 hbase  add a htable get obtainscanner method that retrieves all versions of a particular column and row between two timestamps
 hbase  commit batchupdate method should return timestamp
 hbase  scan for columns  some timestamp
 hbase  add a means of scanning over all versions
 verify a single column using gets expects family and qualifier arrays to be valid for at least the range idx   idx  idx
 verify a single column using scanners expects family and qualifier arrays to be valid for at least the range idx  to idx expects row array to be valid for at least idx to idx
 verify we do not read any values by accident around a single column same requirements as getverifysinglecolumn
 test scanmetrics
 tests that cache on write works all the way up from the client side performs inserts flushes and compactions verifying changes in the block cache along the way 
 test simple table and non existent row cases 
 tests reversed scan under multi regions
 test for hbase 
 test increment result when there are duplicate rpc request 
 the invalid cells is in rm the rm should fail but the subsequent mutations should succeed currently we have no client api to submit the request consisting of condition rm and mutation hence this test build the request manually 
 throw some exceptions mostly throw exceptions which do not clear meta cache periodically throw notsevingregionexception which clears the meta cache 
 only run one multi test with a forced regionserver abort otherwise the unit tests will take an unnecessarily long time to run 
 set table auto flush to false and test flushing commits
 method to check whether the cached region location is non empty for the given table it repeats the same check several times as clearing of cache by some async operations may not reflect immediately 
 method to check whether the passed row exists in the given table
 run some tests to ensure result acts like a proper cellscanner 
 calls non functional test methods 
 scan on not existing table should throw the exception with correct message
 run through a variety of test configurations with a small scan
 make sure that no rows are lost if the scanner timeout is longer on the client than the server and the scan times out on the server but not the client 
 all rowkeys are fit in the last region 
 verify that adding data to the cloned table will not affect the original and vice versa when it is taken as an online snapshot 
 verify that adding data to the cloned table will not affect the original and vice versa when it is taken as an offline snapshot 
 verify that adding metadata to the cloned table will not affect the original and vice versa when it is taken as an online snapshot 
 verify that adding netadata to the cloned table will not affect the original and vice versa when is taken as an online snapshot 
 verify that region operations in this case splitting a region are independent between the cloned table and the original 
 verify that region operations in this case splitting a region are independent between the cloned table and the original 
 verify that adding data to original table or clone table doesn t affect other table 
 do a split and verify that this only affects one table
 verify that deleting the snapshot does not affect either table 
 verify that the describe for a restored table matches the describe for one the original 
 verify that if metadata changed after a snapshot was taken that the old metadata replaces the new metadata during a restore
 verify that when the table is empty making metadata changes after the restore does not affect the restored table s original metadata
 test from client side for timestampsfilter the timestampsfilter provides the ability to request cells keyvalues whose timestamp version is in the specified list of timestamps version 
 test timestampsfilter in the presence of version deletes 
 read the attributes from hadoop hbase regionserver metatablemetrics in jmx
 do a single put that is bypassed by a regionobserver
 test various multiput operations if the column family is test then bypass is invoked 
 test that when bypass is called we skip out calling any other coprocessors stacked up method in this case a preput if the column family is test then bypass is invoked 
 test various multiput operations 
 tests that added mutations from coprocessors end up in the wal 
 tests overriding compaction handling via coprocessor hooks
 unfortunately the easiest way to test this is to spin up a mini cluster since we want to do the usual compaction mechanism on the region rather than going through the backdoor to the region
 test wal write behavior with walobserver the coprocessor monitors a waledit written to wal and ignore modify and add keyvalue s for the waledit 
 coprocessors shouldn t get notice of empty waledits 
 test wal replay behavior with walobserver 
 test to see cp loaded successfully or not there is a duplication at testhlog but the purpose of that one is to see whether the loaded cp will impact existing wal tests or not 
 demonstrate timeoutexceptioninjector semantics completion means no more exceptions passed to error listener 
 demonstrate timeoutexceptioninjector semantics triggering fires exception and completes the timer 
 tests serialization
 test scans using a dependentcolumnfilter
 test must pass one 
 test must pass all 
 test list ordering
 test serialization
 tests identification of the stop row
 tests serialization
 test page size filter
 test filter serialization
 tests serialization
 tests identification of the stop row
 tests serialization
 test that we re can add a hook and that this hook works when we try to read the file in hdfs 
 allow to get the hostname using gethostname hadoop  or getdisplayname hadoop  
 test that the reorder algo works as we expect 
 test that the hook works within hbase including when there are multiple blocks 
 wait for the specified region server to join the cluster
 blocks until there is an active master and that master has completed initialization 
 restores the cluster to it s initial state if this is a real cluster otherwise does nothing this is a best effort restore if the servers are not reachable or insufficient permissions etc restoration might be partial 
 get the servername of region server serving the first hbase meta region
 you must call close on the returned region and then close on the log file it created do link hbasetestingutility closeregionandwal hregion to close both the region and the wal 
 add content to region r on the passed column column adds data of the from aaa aab etc where key and value are the same 
 add content to region r on the passed column column adds data of the from aaa aab etc where key and value are the same 
 add content to region r on the passed column column adds data of the from aaa aab etc where key and value are the same 
 close both the region code r and it s underlying wal for use in tests 
 return where to write test data on the test filesystem returns working directoryfor the test filesystem by default
 returns a path in the test filesystem obtained from link gettestfilesystem to write temporary test data call this method after setting up the mini dfs cluster if the test relies on it 
 returns a path in the test filesystem obtained from link gettestfilesystem to write temporary test data call this method after setting up the mini dfs cluster if the test relies on it 
 sets up a path in test filesystem to be used by tests creates a new directory if not already setup 
 sets up a new path in test filesystem to be used by tests 
 cleans the test data directory on the test filesystem 
 cleans a subdirectory under the test data directory on the test filesystem 
 start a minidfscluster 
 start a minidfscluster this is useful if you want to run datanode on distinct hosts for things like hdfs block location verification if you start minidfscluster without host names all instances of the datanodes will have the same host name 
 start a minidfscluster can only create one 
 this is used before starting hdfs and map reduce mini clusters 
 start up a minicluster of hbase dfs and zookeeper 
 start up a minicluster of hbase dfs and zookeeper where wal s waldir is created separately 
 start up a minicluster of hbase dfs and zookeeper set the create flag to create root or data directory path or not will overwrite if dir already exists 
 start up a minicluster of hbase optionally dfs and zookeeper modifies configuration homes the cluster data directory under a random subdirectory in a directory under system property test build data directory is cleaned up on exit 
 start minicluster whether to create a new root or data dir path even if such a path has been created earlier is decided based on flag create
 start minicluster
 start up a minicluster of hbase optionally dfs and zookeeper modifies configuration homes the cluster data directory under a random subdirectory in a directory under system property test build data directory is cleaned up on exit 
 same as link startminicluster int int but with custom number of datanodes 
 start up a minicluster of hbase optionally dfs and zookeeper modifies configuration homes the cluster data directory under a random subdirectory in a directory under system property test build data directory is cleaned up on exit 
 same as link startminicluster int int string class class but with custom number of datanodes 
 starts up mini hbase cluster usually used after call to link startminicluster int int when doing stepped startup of clusters usually you won t want this you ll usually want link startminicluster 
 stops mini hbase zk and hdfs clusters 
 shutdown hbase mini cluster does not shutdown zk or dfs if running 
 abruptly shutdown hbase mini cluster does not shutdown zk or dfs if running 
 returns the path to the default root dir the minicluster uses if create is true a new root directory path is fetched irrespective of whether it has been fetched before or not if false previous path is used note this does not cause the root dir to be created 
 same as link hbasetestingutility getdefaultrootdirpath boolean create except that create flag is false note this does not cause the root dir to be created 
 creates an hbase rootdir in user home directory also creates hbase version file normally you won t make use of this method root hbasedir is created for you as part of mini cluster startup you d only use this method if you were doing manual operation 
 same as link hbasetestingutility createrootdir boolean create except that create flag is false 
 creates a hbase waldir in the user s home directory normally you won t make use of this method root hbasewaldir is created for you as part of mini cluster startup you d only use this method if you were doing manual operation 
 create a table 
 create a table 
 create a table 
 create a table with multiple regions 
 create a table 
 create a table with multiple regions 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table 
 create a table with multiple regions 
 create a table 
 create a table 
 create a table 
 create a table with multiple regions 
 modify a table synchronous waiting logic similar to that of code admin rb alter status 
 drop an existing table
 drop an existing table
 create an hregion that writes to the local tmp dirs
 create an hregion that writes to the local tmp dirs creates the wal for you be sure to call link hbasetestingutility closeregionandwal hregion when you re finished with it 
 param tablename
 param tablename
 provide an existing table name to truncate scans the table and issues a delete for each row read 
 truncate a table using the admin command effectively disables deletes and recreates the table 
 truncate a table using the admin command effectively disables deletes and recreates the table for previous behavior of issuing row deletes see deletetabledata expressly does not preserve regions of existing table 
 load table with rows from aaa to zzz 
 load table with rows from aaa to zzz 
 load table of multiple column families with rows from aaa to zzz 
 load table of multiple column families with rows from aaa to zzz 
 return the number of rows in the given table 
 return the number of rows in the given table 
 return an md digest of the entire contents of a table 
 create rows in hbase meta for regions of the specified table with the specified start keys the first startkey should be a  length byte array if you want to form a proper range of regions 
 create rows in hbase meta for regions of the specified table with the specified start keys the first startkey should be a  length byte array if you want to form a proper range of regions 
 create a region with it s own wal be sure to call link hbasetestingutility closeregionandwal hregion to clean up all resources 
 create a region with it s own wal be sure to call link hbasetestingutility closeregionandwal hregion to clean up all resources 
 returns all rows from the hbase meta table 
 returns all rows from the hbase meta table for a given user table
 tool to get the reference to the region server object that holds the region of the specified user table it first searches for the meta rows that contain the region of the specified table then gets the index of that rs and finally retrieves the rs s reference 
 starts a minimrcluster with a default number of tasktracker s 
 tasktracker has a bug where changing the hadoop log dir system property will not change its internal static log dir variable 
 starts a minimrcluster call link setfilesystemuri string to use a differentfilesystem 
 create a stubbed out regionserverservice mainly for getting fs 
 create a stubbed out regionserverservice mainly for getting fs this version is used by testtokenauthentication
 create a stubbed out regionserverservice mainly for getting fs this version is used by testopenregionhandler
 expire the master s session
 expire a region server s session
 expire a zookeeper session as recommended in zookeeper documentation http hbase apache org book html trouble zookeeper there are issues when doing this  http www mail archive com dev zookeeper apache org msg html  https issues apache org jira browse zookeeper 
 returns a admin instance this instance is shared between hbasetestingutility instance users closing it has no effect it will be closed automatically when the cluster shutdowns
 returns an admin instance which is shared between hbasetestingutility instance users closing it has no effect it will be closed automatically when the cluster shutdowns
 returns an link hbck instance needs be closed when done 
 unassign the named region 
 unassign the named region 
 closes the region containing the given row 
 closes the region containing the given row 
 set the minidfscluster
 wait until all regions in a table have been assigned waits default timeout before giving up  seconds 
 waits for a table to be enabled enabled means that table is set as enabled and the regions have been all assigned will timeout after default period  seconds tolerates nonexistent table 
 waits for a table to be enabled enabled means that table is set as enabled and the regions have been all assigned 
 waits for a table to be disabled disabled means that table is set as disabled will timeout after default period  seconds 
 waits for a table to be disabled disabled means that table is set as disabled 
 make sure that at least the specified number of region servers are running we don t count the ones that are currently stopping or are stopped 
 set maxrecoveryerrorcount in dfsclient in   pre append its hard coded to  and makes tests linger here is the exception you ll see        warn datastreamer for file hbase logs wal  block blk   hdfs dfsclientdfsoutputstream  error recovery for block blk   failed because recovery from primary datanode      failed  times pipeline was           will retry 
 move region to destination server and wait till region is completely moved and online
 wait until all regions for a table in hbase meta have a non empty info server up to a configuable timeout value default is  seconds this means all regions have been deployed master has been informed and updated hbase meta with the regions deployed server 
 waith until all system table s regions get assigned
 wait until all regions for a table in hbase meta have a non empty info server or until timeout this means all regions have been deployed master has been informed and updated hbase meta with the regions deployed server 
 do a small get scan against one store this is required because store has no actual methods of querying itself and relies on storescanner 
 creates a random table with the given parameters 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 creates a pre split table for load testing if the table already exists logs a warning and continues 
 due to async racing issue a region may not be in the online region list of a region server yet after the assignment znode is deleted and the new assignment is recorded in master 
 check to make sure the region is open on the specified region server but not on any other one 
 wait until no regions in transition time limit min 
 add any supported codec or cipher to test the hfile read write performance specify none to disable codec or cipher or both 
 write a test hfile with the given codec cipher
 run all the read benchmarks for the test hfile 
 run benchmark
 param args
 test data block encoding of empty keyvalue 
 test keyvalues with negative timestamp 
 test whether compression  decompression gives the consistent results on pseudorandom sample 
 test seeking while file is encoded 
 test whether the decompression of first key is implemented correctly 
 test seeking while file is encoded 
 test seeking while file is encoded 
 test seeking while file is encoded 
 test seeking while file is encoded 
 set up variables and get bucketcache and writerthread into state where tests can manually control the running of writerthread and bucketcache is empty 
 test non error case just works 
 pass through a too big entry and ensure it is cleared from queues and ramcache manually run the writerthread 
 do ioe take the ramqueueentry that was on the queue doctor it to throw exception then put it back and process it 
 do cache full exception
 test all checksum types by writing and reading back blocks 
 introduce checksum failures and check that we can still read the data
 test different values of bytesperchecksum
 create a truncated hfile and verify that exception thrown 
 test none codecs
 test encoding decoding data blocks 
 testing block index through the hfile writer reader apis allows to test setting index block size through configuration intermediate level index blocks and caching index blocks on write 
 test putting and taking out blocks into cache with different encoding options 
 test for hbase  
 test encoding 
 test encoding with offheap keyvalue this test just verifies if the encoders work with dbb and does not use the getxxxarray api
 scanner seekbefore could fail because when seeking to a previous hfile data block it needs to know the size of that data block which it calculates using current data block offset and the previous data block offset this fails to work when there are leaf level index blocks in the scannable section of the hfile i e starting in hfilev this test will try seekbefore on a flat single level and multi level   hfile and confirm this bug is now fixed this bug also happens for inline bloom blocks for the same reasons 
 test on hdfs that the filelink is still readable even when the current file gets renamed 
 test on a local filesystem that the filelink is still readable even when the current file gets renamed 
 test that link is still readable even when the current file gets renamed 
 test that link is still readable even when the current file gets deleted note this test is valid only on hdfs when a file is deleted from a local file system it is simply unlinked the inode which contains the file s data is not deleted until all processes have finished with it in hdfs when the request exceed the cached block locations a query to the namenode is performed using the filename and the deleted file doesn t exists anymore filenotfoundexception 
 test the scanner and reseek of a half hfile scanner the scanner api demands that seekto and reseekto only return  if we got an imperfect match the latter case being the most common we should generally be returning  and if we do there may or may not be a next in the scanner file a bug in the half file scanner was returning  at the end of the bottom half and that was causing the infrastructure above to go null causing npes and other problems this test reproduces that failure and also tests both the bottom and top of the file while we are at it 
 ensure we do not have to have a codec 
 it is hard to verify the compression is actually happening under the wraps hope that if unsupported we ll get an exception out of some time meantime have to trace it manually to confirm that compression is happening down in the client and server 
 tests that the rpc scheduler is called when requests arrive 
 tests that the rpc scheduler is called when requests arrive 
 tests that the rpcserver creates dispatches callrunner object to scheduler with non null remoteaddress set to its call object
 tests that the connection closing is handled by the client with outstanding rpc calls 
 the original intention of this test was to force an abort of a region server and to make sure that the failure path in the region servers is properly evaluated but it is difficult to ensure that the region server doesn t finish the log splitting before it aborts also now there is this code path where the master will preempt the region server when master detects that the region server has aborted 
 find a rs that has regions of a table 
 it is possible that when am send assign meta request to a rs successfully but rs can not send back any response which cause master startup hangs forever
 this tests two region merges
 this tests two concurrent region merges
 test that we make it through to the end if parent region is offlined between start of this move and when we go to run the move unassignprocedure 
 ignore this test see hbase 
 manufacture a state that will throw unexpectedstateexception change an assigned region s state to be opening that ll mess up a subsequent unassign causing it to throw unexpectedstateexception we can easily manufacture this infinite retry state in unassignprocedure because it has no starttransition assignprocedure does where it squashes whatever the current region state is making it offline that makes it harder to mess it up make do with unassignprocedure for now 
 this tests verifies logcleaner works correctly with wals and procedure wals located in the same oldwals directory created files  invalid files  old procedure wals  old wals from which  are in replication  recent procedure wals  recent wal  very new wal timestamp in future masterprocedurewals subdirectory files which should stay  replication wals  new wals  latest procedure wals masterprocedurewals subdirectory
 replicationhfilecleaner should be able to ride over zookeeper errors without aborting 
 test that the contract from the master for checking on a snapshot are valid if a snapshot fails with an error we expect to get the source error if there is no snapshot name supplied we should get an error if asking about a snapshot has hasn t occurred you should get an error 
 test that the snapshot hfile archive cleaner works correctly hfiles that are in snapshots should be retained while those that are not in a snapshot should be deleted 
 returns immediately if the lock is acquired 
 starts the threads and waits for them to finish 
 run through all procedure flow states twice while also restarting procedure executor at each step i e force a reread of procedure store it does execute step n kill the executor before store update restart executor store execute step n and then save to store this is a good test for finding state that needs persisting and steps that are not idempotent use this version of the test when a procedure executes all flow steps from start to finish 
 run through all procedure flow states twice while also restarting procedure executor at each step i e force a reread of procedure store it does execute step n kill the executor before store update restart executor store executes hook for each step twice execute step n and then save to store this is a good test for finding state that needs persisting and steps that are not idempotent use this version of the test when the order in which flow steps are executed is not start to finish where the procedure may vary the flow steps dependent on circumstance found 
 execute the procedure up to laststep and then the procedureexecutor is restarted and an abort is injected if the procedure implement abort this should result in rollback being triggered each rollback step is called twice by restarting the executor after every step at the end of this call the procedure should be finished and rolledback this method assert on the procedure being terminated with an abortexception 
 execute the procedure up to laststep and then the procedureexecutor is restarted and an abort is injected if the procedure implement abort this should result in rollback being triggered at the end of this call the procedure should be finished and rolledback this method assert on the procedure being terminated with an abortexception 
 restart the procedureexecutor and inject an abort to the specified procedure if the procedure implement abort this should result in rollback being triggered at the end of this call the procedure should be finished and rolledback if abort is implemnted
 verify that write operations for a single table are serialized but different tables can be executed in parallel 
 tests proper fencing in case the current wal store is fenced
 tests proper fencing in case the current wal store does not receive writes until after the new wal does a couple of wal rolls 
 test that we deal with servercrashprocedure zero ing out the targetserver in the regionstatenode in the midst of our doing an assign the trickery is done above in targetserverbeingnulledonusassignprocedure we skip a bunch of logic to get at the guts where the problem happens we also skip out the failure handling because it d take a bunch of mocking to get it to run fix is inside in remoteproceduredispatch addoperationtonode it now notices empty targetserver and just returns false so we fall into failure processing and we ll reassign elsewhere instead of npe ing the fake of servercrashprocedure nulling out the targetserver happens inside in updatetransition just after it was called but before it gets to the near the end when addtoremotedispatcher is called see the targetserverbeingnulledonusassignprocedure class above see hbase  before fix this test would fail w a nullpointerexception 
 verify the snapshot support based on the configuration 
 unit tests that uses zookeeper but does not use the master side methods but rather acts directly on zk 
 test clearing a split parent 
 make sure parent gets cleaned up even if daughter is cleaned up before it 
 make sure last parent with empty end key gets cleaned up even if daughter is cleaned up before it 
 make sure parent with specified end key gets cleaned up even if daughter is cleaned up before it 
 catalogjanitor scan should not clean parent regions if their own parents are still referencing them this ensures that grandparent regions do not point to deleted parent regions 
 test that if a store file with the same name is present as those already backed up cause the already archived files to be timestamped backup
 test clearing a split parent from memory 
 test that if the master fails the load balancer maintains its state running or not when the next master takes over
 kill the master and wait for a new active master to show up
 test master failover start up three fake regionservers and a master 
 start up a mini cluster and put a small table of many empty regions into it 
 to verify the region assignment status it will check the assignment plan consistency between hbase meta and region servers also it will verify weather the number of region movement and the number regions on the primary region server are expected
 verify the number of region movement is expected
 verify the number of user regions is assigned to the primary region server based on the plan is expected
 this tests retaining assignments on a cluster restart
 test whether the splitlog correctly creates a task in zookeeper
 basic client side validation of hbase 
 returns a result object constructed from the given region information simulating a catalog table result 
 returns a result object constructed from the given region information simulating a catalog table result 
 param rsports ports that regionserver should use pass ports if you want to test clusterrestart where for sure the regionservers come up on same addressport but just with different startcode by default mini hbase clusters choose new arbitrary ports on each cluster start 
 starts a region server thread and waits until its processed by master throws an exception when it can t start a region server or when the region server is not processed by master within the timeout 
 call flushcache on all regions on all participating regionservers 
 call flushcache on all regions of the specified table 
 this case tests the following mob compaction and normal compaction scenario after mob compaction the mob reference in new bulkloaded hfile will win even after it is compacted with some other normal hfiles this is to make sure the mvcc is included after compaction for mob enabled store files 
 tests the compactedelfile
 creates store files 
 creates a  day old hfile and an  day old hfile then sets expiry to  days verifies that the  day old hfile is removed but the  day one is still present after the expiry based cleaner is run 
 create the mob store file 
 create the mob store file
 verify that no assertions have failed inside a future used for unit tests that spawn threads e g list results   lists newarraylist future f   executor submit new callable public void call asserttrue somemethod results add f assertonfutures results 
 wait for all of the currently outstanding tasks submitted via link submittask callable 
 do a log roll 
 with a single member verify ordered execution the coordinator side is run in a separate thread so we can only trigger from members and wait for particular state latches 
 test that if nodes join the barrier early we still correctly handle the progress
 just run a procedure with the standard name and data with not special task for the mock coordinator it works just like a regular coordinator for custom behavior see link runcoordinatedoperation procedure acquirebarrieranswer barrieranswer string 
 setup a procedure member that returns the spied upon link subprocedure 
 test the normal sub procedure execution case 
 make sure we call cleanup etc when we have an exception during link subprocedure acquirebarrier 
 make sure we call cleanup etc when we have an exception during prepare 
 fail correctly if coordinator aborts the procedure the subprocedure will not interrupt a running link subprocedure acquirebarrier prepare needs to finish first and the the abortis checked thus the link subprocedure acquirebarrier should succeed but later get rolled backvia link subprocedure cleanup 
 handle failures if a member s commit phase fails note this is the core difference that makes this different from traditional pc in true pc the transaction is committed just before the coordinator sends commit messages to the member members are then responsible for reading its tx log this implementation actually rolls back and thus breaks the normal tx guarantees 
 handle failures if a member s commit phase succeeds but notification to coordinator fails note this is the core difference that makes this different from traditional pc in true pc the transaction is committed just before the coordinator sends commit messages to the member members are then responsible for reading its tx log this implementation actually rolls back and thus breaks the normal tx guarantees 
 test a distributed commit with multiple cohort members where one of the cohort members has a timeout exception during the prepare stage 
 smaller test to just test the actuation on the cohort member
 little test to check we can basically convert list of a list of kvs into a cellscanner
 removes all quotas defined in the hbase quota table 
 waits seconds for the hbase quota table to exist 
 test the perf of a compactionpolicy with settings 
 unit test for hbase 
 helper methods 
 verify the compaction that includes several entire stripes 
 param lcount number of l files 
 param lcount number of l files 
 param lcount number of l files 
 runs the tools 
 benchmark codec s speed 
 check decompress performance of a given algorithm and print it 
 test a data block encoder on the given hfile output results to console 
 a command line interface to benchmarks parses command line arguments and runs the appropriate benchmarks 
 param path path to the hfile which will be used 
 command line interface 
 regression test for hbase 
 test basic append operation more tests in
 test multi threaded increments 
 test multi threaded row mutations 
 test multi threaded region mutations 
 test written as a verifier for hbase  checkandput should properly read mvcc moved into testatomicoperation from its original location testhbase
 test of blocks read for some simple seek cases 
 test of blocks read targeted at some of the cases lazy seek optimizes 
 test of blocks read to ensure disabling cache fill on scan works 
 test memstore snapshots
 test getnextrow from memstore
 tests that the timeofoldestedit is updated correctly for the various edit operations in memstore 
 verify that you can stop a long running compaction used during rs shutdown 
 create a custom compaction request and be sure that we can track it through the queue knowing when the compaction is completed 
 hbase  regression test to ensure adding to the correct list in the link compactsplit 
 test compaction priority management and multiple compactions per store hbase  
 load data to a table flush it to disk trigger compaction confirm the compaction state is right and wait till it is done 
 a simple test which verifies the  possible states when scanning across snapshot 
 regression test for hbase  hbase  when we insert a higher memstorets version of a cell but with the same timestamp we still need to provide consistent reads for the same scanner 
 when we insert a higher memstorets deletion of a cell but with the same timestamp we still need to provide consistent reads for the same scanner 
 test memstore snapshots
 test getnextrow from memstore
 tests that the timeofoldestedit is updated correctly for the various edit operations in memstore 
 tests the hregion shouldflush method adds an edit in the memstore and checks that shouldflush returns true and another where it disables the periodic flush functionality and tests whether shouldflush returns false 
 blocks until the region split is complete in hbase meta and region server opens the daughters
 reproduce locking up that happens when we get an exceptions appending and syncing see hbase  first i need to set up some mocks for server and regionserverservices i also need to set up a dodgy wal that will throw an exception when we go to append to it 
 getting data from memstore
 getting mob data from files
 getting the reference data from files
 getting data from memstore and files
 getting data from memstore and files
 flush the memstore
 test that i can use the max flushed sequence id after the close 
 test for bug  of hbase  bug  conditions for the first flush of region close so called pre flush if memstoresize is smaller than a certain value or when region close starts a flush is ongoing the first flush is skipped and only the second flush takes place however two flushes are required in case previous flush fails and leaves some data in snapshot the bug could cause loss of data in current memstore the fix is removing all conditions except abort check so we ensure  flushes for region close 
 a test case of hbase 
 test we do not lose data if we fail a flush and then close part of hbase  tests the following from the issue description bug  wrong calculation of hregion memstoresize when a flush fails data to be flushed is kept in each memstore s snapshot and wait for next flush attempt to continue on it but when the next flush succeeds the counter of total memstore size in hregion is always deduced by the sum of current memstore sizes instead of snapshots left from previous failed flush this calculation is problematic that almost every time there is failed flush hregion memstoresize gets reduced by a wrong value if region flush could not proceed for a couple cycles the size in current memstore could be much larger than the snapshot it s likely to drift memstoresize much smaller than expected in extreme case if the error accumulates to even bigger than hregion s memstore size limit any further flush is skipped because flush does not do anything if memstoresize is not larger than  
 return syncs initial synctimenumops
 tests that the special latest timestamp option for puts gets replaced by the actual timestamp
 tests that there is server side filtering for invalid timestamp upper bound note that the timestamp lower bound is automatically handled for us by the ttl field 
 this method tests https issues apache org jira browse hbase  
 added for hbase  here we test scan optimization when only subset of cfs are used in filter conditions 
 hbase  test case when scan limits amount of kvs returned on each next call 
 write an hfile block full with cells whose qualifier that are identical between  and short max value see hbase  
 flushes the cache in a thread while scanning the tests verify that the scan is coherent e g the returned results are always of the same or later update as the previous results 
 writes very wide records and scans for the latest every time flushes and compacts the region every now and then to keep things realistic 
 block calling thread until this instance of putthread has put at least one row 
 writes very wide records and gets the latest row every time flushes and compacts the region aggressivly to catch issues 
 test case to check increment function with memstore flushing
 test case to check append function with memstore flushing
 test case to check put function with memstore flushing for same row same ts
 test that we get the expected flush results back
 return a region on which you must call link hbasetestingutility closeregionandwal hregion when done 
 return a region on which you must call link hbasetestingutility closeregionandwal hregion when done 
 test for hbase  reverse scan threw stackoverflow caused by readpt checking
 test regiontoobusyexception thrown when region is busy
 hbase  make sure no stuck if roll writer when ring buffer is filled with appends
 create hstore instance 
 tests a case where we replay only a flush start marker then the region is closed this region should not block indefinitely
 tests cases where we prepare a flush with some seqid and we receive other flush start markers equal to greater or less than the previous flush start marker 
 tests the case where we prepare a flush with some seqid and we receive a flush commit marker less than the previous flush start marker 
 tests the case where we prepare a flush with some seqid and we receive a flush commit marker larger than the previous flush start marker 
 tests the case where we receive a flush commit before receiving any flush prepare markers the memstore edits should be dropped after the flush commit replay since they should be in flushed files
 tests the case where we receive a flush commit before receiving any flush prepare markers the memstore edits should be not dropped after the flush commit replay since not every edit will be in flushed files based on seqid 
 tests the case where we receive a flush commit before receiving any flush prepare markers
 tests replaying region open markers from primary region checks whether the files are picked up
 tests the case where we replay a region open event after a flush start but before receiving flush commit
 tests whether edits coming in for replay are skipped which have smaller seq id than the seqid of the last replayed region open event 
 tests that a region opened in secondary mode would not write region open close events to its wal 
 tests the reads enabled flag for the region when unset all reads should be rejected
 tests the case where a request for flush cache is sent to the region but region cannot flush it should write the flush request marker instead 
 test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region replaying cannot flush flush marker entry should restore the reads enabled status in the region and allow the reads to continue 
 test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region replaying flush start and commit entries should restore the reads enabled status in the region and allow the reads to continue 
 test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region replaying flush start and commit entries should restore the reads enabled status in the region and allow the reads to continue 
 test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region replaying region open event entry from primary should restore the reads enabled status in the region and allow the reads to continue 
 tests replaying region open markers from primary region checks whether the files are picked up
 atomic bulk load 
 run test on an hbase instance for  minutes this assumes that the table under test only has a single region 
 a test case of hbase 
 test we do not lose data if we fail a flush and then close part of hbase 
 verify that compression and data block encoding are respected by the store createwriterintmp method used on store flush 
 test for hbase  
 getting data from memstore only
 getting data from files only
 getting data from memstore and files
 test to ensure correctness when using stores with multiple timestamps
 test for hbase  test split on empty colfam no store files 
 if there are two running inmemoryflushrunnable the later inmemoryflushrunnable may change the versionedlist and the first inmemoryflushrunnable will use the chagned versionedlist to remove the corresponding segments in short there will be some segements which isn t in merge are removed 
 write a file and then assert that we can read from top and bottom halves using two halfmapfiles 
 test that our mechanism of writing store files in one region to reference store files in other regions works 
 this test creates an hfile and then the dir structures and files to verify that references to hfilelinks created by snapshot clones can be properly interpreted 
 test for hbase 
 command line interface 
 make sure that deleted rows are retained family delete markers are deleted column delete markers are versioned time range scan of deleted rows are possible
 basic verification of existing behavior
 verify that delete markers are removed from an otherwise empty store 
 test delete marker removal from store files 
 test delete marker removal from store files 
 verify correct range demarcation
 verify that column version delete makers are sorted with their respective puts and removed correctly by versioning i e not relying on the store earliestputts 
 test keeping deleted rows together with min versions set
 test keeping deleted rows together with min versions set
 test that on a major compaction if all cells are expired or deleted then we ll end up with no product make sure scanner over region returns right answer in this case and that it just basically works 
 test that on a major compaction deleted cells are retained if keep deleted cells is set to true
 run compaction and flushing memstore assert deletes get cleaned up 
 test for hbase  test user requested major compactions always occurring
 test for hbase 
 test that on a major compaction if all cells are expired or deleted then we ll end up with no product make sure scanner over region returns right answer in this case and that it just basically works 
 create an address tracker instance
 unit tests that uses zookeeper but does not use the master side methods but rather acts directly on zk 
 test allocation from lots of threads making sure the results don t overlap in any way
 test frequent chunk retirement with chunk pool triggered by lots of threads making sure there s no memory leak hbase  
 test frequent chunk retirement with chunk pool triggered by lots of threads making sure there s no memory leak hbase  
 during compaction cells smaller than the threshold won t be affected 
 during compaction the mob threshold size is changed 
 this test will first generate store files then bulk load them and trigger the compaction when compaction the cell value will be larger than the threshold 
 test one put command 
 test multi threaded puts 
 when a log roll is about to happen we do a flush of the regions who will be affected by the log roll these flushes cannot be a selective flushes otherwise we cannot roll the logs this test ensures that we do a full flush in that scenario 
 hbase  itbll fails for me if generator does anything but m per maptask create a region close it then copy into place a file to replay one that is bigger than configured flush size so we bring on lots of flushes then reopen and confirm all edits made it in 
 have each thread update its own cell avoid contention with another thread 
 have each thread update its own cell avoid contention with another thread this is
 not really restarting the master simulate it by clear of new region state since it is not persisted will be lost after master restarts 
 this test tests  merging region not online  merging same two regions  merging unknown regions they are in one test case so that we don t have to create many tables and these tests are simple 
 tests the case where killing a primary region with unflushed data recovers
 tests the case where killing a secondary region with unflushed data recovers and the replica becomes available to read again shortly 
 tests the case where there are  region replicas and the primary is continuously accepting new writes while one of the secondaries is killed verification is done for both of the secondary replicas 
 close the region without using zk
 test that if we do a close while opening it stops the opening 
 tests an on the fly rpc that was scheduled for the earlier rs on the same port for openregion the region server should reject this rpc hbase  
 tests region sever reportforduty with backup master becomes primary master after the first master goes away 
 test verifies whether a region server is removed from online servers list in master if it went down after registering with master test will timeout if an error 
 the test 
 use a scanner to get the region info and then validate the results 
 use get to retrieve the hregioninfo and validate it 
 tests to do a sync flush during the middle of a scan this is testing the storescanner update readers code essentially this is not highly concurrent since its all  thread hbase  
 tests to do a concurrent flush using a nd thread while scanning this tests both the storescanner update readers and the transition from memstore  snapshot  store file 
 run the test callable when heartbeats are enabled disabled we expect all tests to only pass when heartbeat messages are enabled otherwise the test is pointless when heartbeats are disabled the test should throw an exception 
 test the case that the time limit for the scan is reached after each full row of cells is fetched 
 test the case that the time limit for scans is reached in between column families
 test the case that there is a filter which filters most of cells
 test the case that there is a filter which filters most of rows
 test the filter by adding all columns of family a in the scan ok 
 test the filter by adding all columns of family a and b in the scan ko row  without a foo qualifier is returned 
 test the filter by adding  columns of family a and  column of family b in the scan ko row  without a foo qualifier is returned 
 test the filter by adding  column of family a the one used in the filter and  column of family b in the scan ok 
 test the filter by adding  columns of family a in the scan ok 
 after a secure bulkload finished there is a clean up for filesystems used in the bulkload sometimes filesystems used in the finished bulkload might also be used in other bulkload calls or there are other filesystems created by the same user they could be closed by a filesystem closeallforugi call so during the clean up those filesystems need to be used later can not get closed or else a race condition occurs testforracecondition tests the case that two secure bulkload calls from the same ugi go into two different regions and one bulkload finishes earlier when the other bulkload still needs its filesystems checks that both bulkloads succeed 
 the test checks splitlogworker should not spawn more splitters than expected num of tasks per rs
 test that if daughter split on us we won t do the shutdown handler fixup just because we can t find the immediate daughter of an offlined parent 
 verifies hbase  here the case is that splitting is completed but before the cj could remove the parent region the master is killed and restarted 
 if a table has regions that have no store files in a region they should split successfully into two regions with no store files 
 ensure single table region is not on same server as the single hbase meta table region 
 test optimize in storescanner test that we skip to the next block when we it makes sense reading the block index 
 ensure the optimize scan method in storescanner does not get in the way of a get doing minimum work seeking to start of block and then skipping until we find the wanted cell this simple scenario mimics case of all cells fitting inside a single hfileblock see hbase  this test is a little cryptic takes a bit of staring to figure what it up to 
 ensure that optimize does not cause the get to do more seeking than required optimize see hbase  was causing us to seek all cells in a block when a get scan if the next block index start key was a different row to the current one a bug we d call next too often because we had to exhaust all cells in the current row making us load the next block just to discard what we read there this test is a little cryptic takes a bit of staring to figure what it up to 
 verifies scenario for finding a split point 
 run a bunch of threads against a single timerangetracker and ensure we arrive at right range here we do ten threads each incrementing over k at an offset of the thread index max is  k and min is  
 run a bunch of threads against a single timerangetracker and ensure we arrive at right range the data chosen is going to ensure that there are lots collisions i e some other threads may already update the value while one tries to update min max value 
 reproduce locking up that happens when we get an inopportune sync during setup for zigzaglatch wait see hbase  if below is broken we will see this test timeout because it is locked up first i need to set up some mocks for server and regionserverservices i also need to set up a dodgy wal that will throw an exception when we go to append to it 
 if below is broken we will see this test timeout because ringbuffereventhandler was stuck in attainsafepoint everyone will wait for sync to finish forever see hbase  
 test the tuning task of link pressureawarecompactionthroughputcontroller 
 test the tuning task of link pressureawareflushthroughputcontroller 
 test the logic for striped store 
 a loaded wal coprocessor won t break existing wal test cases 
 tests the log comparator ensure that we are not mixing meta logs with non meta logs throws exception if we do comparison is based on the timestamp present in the wal name 
 on rolling a wal after reaching the threshold link wal rollwriter returns the list ofregions which should be flushed in order to archive the oldest wal file this method tests this behavior by inserting edits and rolling the wal enough times to reach the max number of logs threshold it checks whether we get the right regions for flush on rolling the wal 
 test flush for sure has a sequence id that is beyond the last edit appended we do this by slowing appends in the background ring buffer thread while in foreground we call flush the addition of the sync over hregion in flush should fix an issue where flush was returning before all of its appends had made it out to the wal hbase  
 tests that logs are deleted
 tests that logs are deleted when some region has a compaction record in wal and no other records see hbase  
 tests that the logroller perform the roll even if there are no edits
 tests that the logroller perform the roll with some data in the log
 reads the wal with and without waltrailer 
 appends entries in the wal and reads it 
 tests for hbase  
 test case of hregion that is only made out of bulk loaded files assert that we don t crash 
 hregion test case that is made of a major compacted hfile created with three bulk loaded files and an edit in the memstore this is for hbase  dataloss bulk loading with seqids can prevent some log entries from being replayed 
 test writing edits into an hregion closing it splitting logs opening region again verify seqids 
 test that we recover correctly when there is a failure in between the flushes i e some stores got flushed but others did not unfortunately there is no easy hook to flush at a store level the way we get around this is by flushing at the region level and then deleting the recently flushed store file for one of the stores this would put us back in the situation where all but that store got flushed and the region died we restart region again and verify that the edits were replayed 
 test that we could recover the data correctly after aborting flush in the test first we abort flush after writing some data then writing more data and flush again at last verify the data 
 create an hregion with the result of a wal split and test we only see the good edits
 testcase for https issues apache org jira browse hbase 
 testcase for https issues apache org jira browse hbase  
 test when returnresults set to false in increment it should not return the result instead it resturn null 
 test case for https issues apache org jira browse hbase 
 tests that logs are rolled upon detecting datanode death requires an hdfs jar with hdfs  syncfs support hdfs  
 test that wal is rolled when all data nodes in the pipeline have been restarted 
 spin up a bunch of threads and have them all append to a wal roll the wal frequently to try and trigger npe 
 test whether calling removepeer on a replicationsourcemanager that failed on initializing the corresponding replicationsourceinterface correctly cleans up the corresponding replication queue and replicationpeer see hbase  
 tests basic reading of log appends
 tests that if after a stream is opened more entries come in and then the log is rolled we don t mistakenly dequeue the current log thinking we re done with it
 tests that if writes come in while we have a stream open we shouldn t miss them
 tests that if we stop before hitting the end of a stream we can continue where we left off using the last position
 it tests the replication scenario involving      it does it by adding and deleting a row to a table in each cluster checking if it s replicated it also tests that the puts and deletes are not replicated back to the originating cluster 
 tests the replication scenario    by default link basereplicationendpoint canreplicatetosamecluster returns false so thereplicationsource should terminate and no further logs should get enqueued
 it tests the replication scenario involving      it does it by bulk loading a set of hfiles to a table in each cluster checking if it s replicated 
 tests the cyclic replication scenario of        by adding and deleting rows to a table in each clusters and ensuring that the each of these clusters get the appropriate mutations it also tests the grouping scenario where a cluster needs to replicate the edits originating from itself and also the edits that it received using replication from a different cluster the scenario is explained in hbase 
 it tests the multi slave hfile replication scenario involving     it does it by bulk loading a set of hfiles to a table in master cluster checking if it s replicated in its peers 
 it tests the bulk loaded hfile replication scenario to only explicitly specified table column families it does it by bulk loading a set of hfiles belonging to both the cfs of table and set only one cf data to replicate 
 tests cyclic replication scenario of        
 throws java lang exception
 test disabling an inactive peer add a peer which is inactive trying to insert disable the peer then activate the peer and make sure nothing is replicated in addition enable the peer and check the updates are replicated 
 throws java lang exception
 load up  tables over  region servers and kill a source during the upload the failover happens internally warning this test sometimes fails because of hbase 
 verify that version and column delete marker types are replicated correctly 
 test disable enable replication trying to insert make sure nothing s replicated enable it the insert should be replicated
 integration test for testreplicationadmin removes and re add a peer cluster
 do a more intense version testsmallbatch one that will trigger wal rolling and other non trivial code paths
 test for hbase  create two new tables with colfamilies enabled for replication then run replicationadmin listreplicated finally verify the table colfamilies note testreplicationadmin is a better place for this testing but it would need mocks 
 test for hbase  waledits under replay will also be replicated
 test for hbase  put a few rows into htable which should be replicated to htable create a clusterstatus instance status from hbaseadmin test status getload server getreplicationloadsourcelist test status getload server getreplicationloadsink throws exception
 add a row to a table in each cluster check it s replicated delete it check s gone also check the puts and deletes are not replicated back to the originating cluster 
 this fails only in case of ade or empty list for any of the users 
 this passes only in case of ade for all users 
 grant permissions globally to the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke permissions globally from the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 grant permissions on a namespace to the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 grant permissions on a namespace to the given user using accesscontrol client will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke permissions on a namespace from the given user using accesscontrol client will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke permissions on a namespace from the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 grant permissions on a table to the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 grant permissions on a table to the given user using accesscontrolclient will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 grant global permissions to the given user using accesscontrolclient will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke permissions on a table from the given user will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke permissions on a table from the given user using accesscontrolclient will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 revoke global permissions from the given user using accesscontrolclient will wait until all active accesscontroller instances have updated their permissions caches or will throw an exception upon timeout  seconds 
 insure we are not granting access in the absence of any cells found when scanning for covered cells 
 test a table modification adding a coprocessor path which is not whitelisted
 test a table creation including a coprocessor path which is not whitelisted
 test a table modification adding a coprocessor path which is whitelisted
 test a table modification adding a coprocessor path which is whitelisted
 test a table modification adding a coprocessor path which is whitelisted
 test a table modification adding a coprocessor path which is whitelisted
 test a table modification adding a coprocessor path which is whitelisted
 test various combinations of server and client qops 
 test sasl encryption with crypto aes 
 test various combinations of server and client configuration for crypto aes 
 sets up a rpc server and a client does a rpc checks the result if an exception is thrown from the stub this function will throw root cause of that exception 
 setup and start kerberos hbase
 create the mob table 
 make sure that there is only one snapshot returned from the master
 make sure that there is only one snapshot returned from the master and its name and table match the passed in parameters 
 confirm that the snapshot contains references to all the files that should be in the snapshot 
 confirm that the snapshot has no references files but only metadata 
 helper method for testing async snapshot operations just waits for the given snapshot to complete on the server by repeatedly checking the master 
 take a snapshot of the specified table and verify that the given family is not empty note that this will leave the table disabled in the case of an offline snapshot 
 take a snapshot of the specified table and verify the given families note that this will leave the table disabled in the case of an offline snapshot 
 corrupt one region manifest file
 corrupt data manifest file
 demonstrate that we reject snapshot requests if there is a snapshot already running on the same table currently running and that concurrent snapshots on different tables can both succeed concurretly 
 tests adding a region to the snapshot manifest while compactions are running on the region the idea is to slow down the process of adding a store file to the manifest while triggering compactions on the region allowing the store files to be marked for archival while snapshot operation is running this test checks for the correct behavior in such a case that the compacted files should not be moved around if a snapshot operation is in progress see hbase 
 initialize the tests with a table filled with some data and two snapshots snapshotname snapshotname of different states the tablename snapshotnames and the number of rows in the snapshot are initialized 
 execute the restore operation
 tests launching the cluster by first starting regionserver and then the master to ensure that it does not matter which is started first 
 tests launching the cluster by first starting master and then the regionserver to ensure that it does not matter which is started first 
 test the global mem store size in the region server is equal to sum of each region s mem store size
 ensure when we go to top level index pages that we get redirected to an info server specific status page 
 test that the status pages in the minicluster load properly this is somewhat a duplicate of testrsstatusservlet and testmasterstatusservlet but those are true unit tests whereas this uses a cluster 
 test that puts up a regionserver starts a compaction on a loaded region but holds the compaction until after we have killed the server and the region has come up on a new regionserver altogether this fakes the double assignment case where region in one location changes the files out from underneath a region being served elsewhere 
 test that puts up a regionserver starts a compaction on a loaded region but holds the compaction completion until after we have killed the server and the region has come up on a new regionserver altogether this fakes the double assignment case where region in one location changes the files out from underneath a region being served elsewhere 
 creates and binds a regular serversocket 
 creates a nio serversocketchannel and gets the serversocket from there then binds the obtained socket this fails on windows with oracle jdk  u if the passed inetaddress is a ipv address works on oracle jdk   
 checks whether we are effected by the jdk issue on windows and if so ensures that we are running with preferipvstack true 
 checks whether we are running with java net preferipvstack true
 tests whether every inetaddress we obtain by resolving can open a serversocketchannel 
 this tests to validate the hmaster s connectorserver after unauthorised stopmaster call 
 this tests to validate the regionserver s connectorserver after unauthorised stopregionserver call 
 this tests to validate the hmaster s connectorserver after unauthorised shutdown call 
 does link metatableaccessor getregion connection byte and a writeagainst hbase meta while its hosted server is restarted to prove our retrying works 
 test interruptable while blocking wait on meta 
 test we survive a connection refused link connectexception 
 test that verifymetaregionlocation properly handles getting a servernotrunningexception see hbase  note this doesn t check the exact exception thrown in the hbase  as there it is thrown from gethconnection and here it is thrown from get but those are both called from the same function anyway and this way is less invasive than throwing from gethconnection would be 
 test waiting on meat w no timeout specified 
 ensure that the expected key values appear in a result returned from a scanner that is combining partial results into complete results
 ensure that the results returned from a scanner that retrieves all results in a single rpc call matches the results that are returned from a scanner that must incrementally combine partial results into complete results a variety of scan configurations can be tested
 order of cells in partial results matches the ordering of cells from complete results
 setting the max result size allows us to control how many cells we expect to see on each call to next on the scanner test a variety of different sizes for correctness
 param numberofcells
 test various combinations of batching and partial results for correctness
 test the method link result createcompleteresult iterable 
 examine the interaction between the maxresultsize and caching if the caching limit is reached before the maxresultsize limit we should not see partial results on the other hand if the maxresultsize limit is reached before the caching limit it is likely that partial results will be seen 
 param resultsizerowlimit the row limit that will be enforced through maxresultsize
 test partial result re assembly in the presence of different filters the results from the partial scanner should match the results returned from a scanner that receives all of the results in one rpc to the server the partial scanner is tested with a variety of different result sizes all of which are less than the size necessary to fetch an entire row 
 for hbase  try a few different configurations of starting and stopping region servers to see if the assignment or regions is pretty balanced 
 determine if regions are balanced figure out the total divide by the number of online servers then test if each server is   of average rounded up 
 wait until all the regions are assigned 
 in ensemble mode there should be only one ensemble table
 in per table mode each table has a set of metrics
 read the attributes from hadoop hbase master balancer in jmx
 master recovery when the znode already exists internally this test differs from link testmastersessionexpired because herethe master znode will exist in zk 
 test case that creates some regions and loads hfiles that fit snugly inside those regions
 test case that creates some regions and loads hfiles that cross the boundaries of those regions
 test loading into a column family that has a row bloom filter 
 test loading into a column family that has a rowcol bloom filter 
 test case that creates some regions and loads hfiles that have different region boundaries than the table pre split 
 test case that creates some regions and loads hfiles that cross the boundaries and have different region boundaries than the table pre split 
 test case that creates some regions and loads hfiles that cross the boundaries have a row bloom filter and a different region boundaries than the table pre split 
 test case that creates some regions and loads hfiles that cross the boundaries have a rowcol bloom filter and a different region boundaries than the table pre split 
 test loading into a column family that does not exist 
 write a random data file and a non file in a dir with a valid family name but not part of the table families we should we able to bulkload without getting the unmatched family exception hbase  hbase 
 populate table with known values 
 split the known table in half this is hard coded for this test suite 
 test that shows that exception thrown from the rs side will result in an exception on the lihfile client 
 test that shows that exception thrown from the rs side will result in the expected number of retries set by  link hconstants hbase client retries number when link loadincrementalhfiles retry on io exception is set
 this test exercises the path where there is a split after initial validation but before the atomic bulk load call we cannot use presplitting to test this path so we actually inject a split just before the atomic region load 
 this test splits a table and attempts to bulk load the bulk import files should be split before atomically importing 
 this test creates a table with many small regions the bulk load files would be splitted multiple times before all of them can be loaded successfully 
 this simulates an remote exception which should cause lihf to exit with an exception 
 delete a region from assignments meta or completely from hdfs 
 delete a region from assignments meta or completely from hdfs 
 setup a clean table before we start mucking with it it will set tbl which needs to be closed after test
 setup a clean table with a certain region replica count it will set tbl which needs to be closed after test
 setup a clean table with a mob enabled column 
 delete table in preparation for next test
 test that use this should have a timeout because this method could potentially wait forever 
 compare the archived files to the files in the original directory
 create an hfile with the given number of rows between a given start key and end key family qualifier the value will be the key value this file will not have tags 
 create an hfile with the given number of rows between a given start key and end key family qualifier the value will be the key value this file will use certain data block encoding algorithm 
 create an hfile with the given number of rows between a given start key and end key family qualifier the value will be the key value this cells will also have a tag whose value is the key 
 param readerid only the keys with this remainder from division by link numthreads will be read by this thread
 verify compressiontest testcompression on a native codec 
 test for connectioncache cleaning expired connection
 test that isfileclosed makes us recover lease faster 
 ugly test that ensures we can get at the hedged read counters in dfsclient does a bit of preading with hedged reads enabled using code taken from hdfs testpread 
 to test that we successfully exclude a server from the unloading process we test for the number of regions on excluded server and also test that regions are unloaded successfully
 test creating a pre split table using the hexstringsplit algorithm 
 test creating a pre split table using the uniformsplit algorithm 
 creates a pre split table with expectedbounds size  regions then verifies that the region boundaries are the same as the expected region boundaries in expectedbounds 
 tests wal archiving by adding data doing flushing rolling and checking we archive old logs and also don t archive live logs that is a log with un flushed entries this is what it does it creates two regions and does a series of inserts along with log rolling whenever a wal is rolled hlogbase checks previous wals for archiving a wal is eligible for archiving if for all the regions which have entries in that wal file have flushed past their maximum sequence id in that wal file 
 just write multiple logs then split before fix for hadoop  this would fail 
 test new hdfs  sync 
 assert that getspliteditfilessorted returns files in expected order and that it skips moved aside files 
 see hbase  this is a test case of failing open a wal for replication for example after all datanode restarted rolling upgrade for example before this patch low replication detection is only used when syncing wal but if the wal haven t had any entry whiten it will never know all the replica of the wal is broken because of dn restarting and this wal can never be open
 simulates splitting a wal out from under a regionserver that is still trying to write it ensures we do not lose edits 
 param expectedentries  to not assert
 return set of wal names present prior to split attempt 
 param spiedfs should be instrumented for failure 
 test log split process with fake data and lots of edits to trigger threading issues 
 test blocking behavior of the log split process if writers are writing slower than the reader is reading 
 sets up a log splitter with a mock reader and writer the mock reader generates a specified number of edits spread across  regions the mock writer optionally sleeps for each edit it is fed after the split is complete verifies that the statistics show the correct number of edits output into each region 
 see https issues apache org jira browse hbase  
 param leaveopen index to leave un closed  to close all 
 check if zookeeper jaasconfiguration is valid 
 loop until link executorservice awaittermination finally does returnwithout an interrupted exception if we don t do this then we ll shut down prematurely we want to let the executor service clear its task queue closing client sockets appropriately 
 start up or shuts down the thrift server depending on the arguments 
 parse the command line options to set parameters the conf 
 param args
 setting up the thrift tserver
 note this internal interface is slightly different from public apis in regard to handling of the qualifier here we differ from the public java api in that null   byte  rather we respect qual    null as a request for the entire column family the caller link get bytebuffer bytebuffer bytebuffer map interface is consistent in that the column is parse like normal 
 note this public interface is slightly different from public java apis in regard to handling of the qualifier here we differ from the public java api in that null   byte  rather we respect qual    null as a request for the entire column family if you want to access the entire column family use link getver bytebuffer bytebuffer bytebuffer int map with a code column valuethat lacks a code 
 note this internal interface is slightly different from public apis in regard to handling of the qualifier here we differ from the public java api in that null   byte  rather we respect qual    null as a request for the entire column family the caller link getverts bytebuffer bytebuffer bytebuffer long int map interface is consistent in that the column is parse like normal 
 converts multiple link tget s thrift into a list of link get s hbase 
 runs all of the tests under a single junit test method we consolidate all testing to one method because hbaseclustertestcase is prone to outofmemoryexceptions when there are three or more junit test methods 
 tests for creating enabling disabling and deleting tables also tests that creating a table with an invalid column name yields an illegalargument exception 
 tests if the metrics for thrift handler work correctly
 tests adding a series of mutations and batchmutations including a delete mutation also tests data retrieval and getting back multiple versions 
 similar to testtablemutations except mutations are applied with specific timestamps and data retrieval uses these timestamps to extract specific versions of data 
 tests the four different scanner opening methods with and without a stoprow with and without a timestamp 
 for hbase  tests for gettableregions
 check that checkandput fails if the cell does not exist then put in the cell then check that the checkandput succeeds 
 check that checkanddelete fails if the cell does not exist then put in the cell then check that the checkanddelete succeeds 
 tests keeping a hbase scanner alive for long periods of time each call to getscannerrow should reset the connectioncache timeout for the scanner s connection
 see hbase  latency metrics were capped at   seconds due to the use of an int variable to capture the duration 
 put valuea to a row make sure put has happened then create a mutation object to put valueb and delete valuea then check that the row value is only valueb 
 parse zookeeper configuration from hbase xml config and run a quorumpeer 
 returns the instance given by the method link prepare this is an uninterruptible blocking method and the interruption flag will be set just before returning if any 
 return true if the balance switch is on false otherwise
 get the info port of the current master of one is available return  if no current master or zookeeper is unavailable
 get the info port of the backup master if it is available return  if no backup master or zookeeper is unavailable
 get master info port use this instead of link getmasterinfoport if you do not have aninstance of this tracker in your context 
 gets the meta region location if available and waits for up to the specified timeout if not immediately available given the zookeeper notification could be delayed we will try to get the latest data 
 gets the meta region location if available and waits for up to the specified timeout if not immediately available given the zookeeper notification could be delayed we will try to get the latest data 
 waits indefinitely for availability of hbase meta used during cluster startup does not verify meta just that something has been set up in zk 
 verify hbase meta is deployed and accessible 
 verify hbase meta is deployed and accessible 
 gets a connection to the server hosting meta as reported by zookeeper waiting up to the specified timeout for availability warning does not retry use an link org apache hadoop hbase client htable instead 
 sets the location of hbase meta in zookeeper to the specified server address 
 load the meta region state from the meta server znode 
 deletes the location of hbase meta in zookeeper 
 wait until the primary meta region is available get the secondary locations as well but don t block for those 
 wait until the meta region is available and is not in transition 
 wait until the meta region is available and is not in transition 
 param basedir the base directory to use
 throws ioexception if waiting for the shutdown of a server fails
 return clientport return clientport if there is another zk backup can runwhen killing the current active return  if there is no backups 
 kill one back up zk servers 
 exists is an idempotent operation retry before throwing exception
 exists is an idempotent operation retry before throwing exception
 getchildren is an idempotent operation retry before throwing exception
 getchildren is an idempotent operation retry before throwing exception
 getdata is an idempotent operation retry before throwing exception
 getdata is an idempotent operation retry before throwing exception
 setdata is not an idempotent operation retry may cause badversion exception adding an identifier field into the data to check whether badversion is caused by the result of previous correctly setdata
 getacl is an idempotent operation retry before throwing exception
 setacl is an idempotent operation retry before throwing exception
 nonsequential create is idempotent operation retry before throwing exceptions but this function will not throw the nodeexist exception back to the application but sequential is not idempotent operation it is necessary to add identifier to the path to verify whether the previous one is successful or not 
 run multiple operations in a transactional manner retry before throwing exception
 return true if region normalizer is on false otherwise
 get the uuid for the provided zk watcher doesn t handle any zk exceptions
 blocks until this instance has claimed the leader znode in zookeeper
 run the tool 
 gets the data of the node blocking until the node is available 
 gets the data of the node blocking until the node is available or the specified timeout has elapsed 
 creates a new connection to zookeeper pulling settings and ensemble config from the specified configuration object using methods from link zkconfig sets the connection status monitoring watcher to the specified watcher 
 log in the current zookeeper server process using the given configuration keys for the credential file and login principal this is only applicable when running on secure hbase on regular hbase without security features this will safely be ignored 
 log in the current zookeeper client using the given configuration keys for the credential file and login principal this is only applicable when running on secure hbase on regular hbase without security features this will safely be ignored 
 list all the children of the specified znode setting a watch for children changes and also setting a watch on every individual child in order to get the nodecreated and nodedeleted events 
 get the data at the specified znode and set a watch returns the data and sets a watch if the node exists returns null and no watch is set if the node does not exist or there is an exception 
 get the data at the specified znode and set a watch returns the data and sets a watch if the node exists returns null and no watch is set if the node does not exist or there is an exception 
 returns the date of child znodes of the specified znode also sets a watch on the specified znode which will capture a nodedeleted event on the specified znode as well as nodechildrenchanged if any children of the specified znode are created or deleted returns null if the specified node does not exist otherwise returns a list of children of the specified node if the node exists but it has no children an empty list will be returned 
 set data into node creating node if it doesn t yet exist does not set watch 
 sets the data of the existing znode to be the specified data the node must exist but no checks are done on the existing data or version if the node does not exist a link nonodeexception will be thrown no watches are set but setting data will trigger other watchers of this node if there is another problem a keeperexception will be thrown 
 set the specified znode to be an ephemeral node carrying the specified data if the node is created successfully a watcher is also set on the node if the node is not created successfully because it already exists this method will also set a watcher on the node if there is another problem a keeperexception will be thrown 
 creates the specified node iff the node does not exist does not set a watch and fails silently if the node already exists the node created is persistent and open access 
 creates the specified node containing specified data iff the node does not exist does not set a watch and fails silently if the node already exists the node created is persistent and open access 
 creates the specified node and all parent nodes required for it to exist no watches are set and no errors are thrown if the node already exists the nodes created are persistent and open access 
 creates the specified node and all parent nodes required for it to exist the creation of parent znodes is not atomic with the leafe znode creation but the data is written atomically when the leaf node is created no watches are set and no errors are thrown if the node already exists the nodes created are persistent and open access 
 delete the specified node sets no watches throws all exceptions 
 deletes the specified node fails silent if the node does not exist 
 delete the specified node and all of it s children if the node does not exist just returns sets no watches throws all exceptions besides dealing with deletion of children 
 delete all the children of the specified node but not the node itself sets no watches throws all exceptions besides dealing with deletion of children 
 delete all the children of the specified node but not the node itself this will first traverse the znode tree for listing the children and then delete these znodes using multi update api or sequential based on the specified configurations sets no watches throws all exceptions besides dealing with deletion of children if the following is true runsequentialonmultifailure is true on calling multi we get a zookeeper exception that can be handled by a sequential call we retry the operations one by one sequentially 
 delete the specified node and its children this traverse the znode tree for listing the children and then delete these znodes including the parent using multi update api or sequential based on the specified configurations sets no watches throws all exceptions besides dealing with deletion of children if the following is true runsequentialonmultifailure is true on calling multi we get a zookeeper exception that can be handled by a sequential call we retry the operations one by one sequentially 
 bfs traversal of all the children under path with the entries in the list in the same order as that of the traversal lists all the children without setting any watches 
 bfs traversal of all the children under path with the entries in the list in the same order as that of the traversal lists all the children and set watches on to them 
 use zookeeper s multi update functionality if all of the following are true runsequentialonmultifailure is true on calling multi we get a zookeeper exception that can be handled by a sequential call then we retry the operations one by one sequentially note an example is receiving a nodeexistsexception from a create call without multi a user could call createandfailsilent to ensure that a node exists if they don t care who actually created the node i e the nodeexistsexception from zookeeper is caught this will cause all operations in the multi to fail however because the nodeexistsexception that zk create throws will fail the multi transaction in this case if the previous conditions hold the commands are run sequentially which should result in the correct final state but means that the operations will not run atomically 
 return string dump of everything in zookeeper 
 appends replication znodes to the passed stringbuilder 
 returns a string with replication znodes and position of the replication log
 gets the statistics from the given server 
 recursively print the current state of zk non transactional 
 helper method to print the current state of the zk tree 
 on master start we check the znode acls under the root directory and set the acls properly if needed if the cluster goes from an unsecure setup to a secure setup this step is needed so that the existing znodes created with open permissions are now changed with restrictive perms 
 set the znode perms recursively this will do post order recursion so that baseznode acls will be set last in case the master fails in between 
 call this if you only want a zk cluster 
 call this if you only want a zk cluster 
 verifies that for the given root node it should delete all the child nodes recursively using multi update api 
 verifies that for the given root node it should delete all the nodes recursively using multi update api 
 test that we can interrupt a node that is blocked on a wait 
 get a list of rowid ranges by scanning a column index 
 generate the proper instance of a columnmapping
 extract the appropriate token for accumulo from the provided code user and add it to the link jobconf s credentials 
 sets all jars requried by accumulo input output tasks in the configuration to be dynamically loaded when the task is executed 
 obtains an accumulo delegationtoken and sets it in the configuration for input and output jobs the accumulo token is converted into a hadoop style token and returned to the caller 
 setup accumulo input format from conf properties delegates to final recordreader from mapred package 
 loop through search conditions and build iterator settings for predicates involving columns other than rowid if any 
 create an iteratorsetting for the right qualifier constant compareopt and primitivecompare type 
 attempts to construct the binary value from the given inspector falls back to utf encoding when the value cannot be coerced into binary 
 serialize the hive map into an accumulo row
 serialize an accumulo rowid
 compute the serialized value from the given element and object inspectors based on the hive types represented through the objectinspectors for the whole object and column within the object serialize the object appropriately 
 recursively serialize an object using its link objectinspector respecting the separators defined by the link lazyserdeparameters 
 serialize the given primitive to the given output buffer using the provided encoding mechanism 
 starts the program 
 starts the program with redirected input for redirected output setoutputstream and seterrorstream can be used exits with  on success  on invalid arguments and  on any other error
 obtains a password from the passed file path 
 start accepting input from stdin and dispatch it to the appropriate link commandhandler until theglobal variable exit is true 
 try to obtain the current size of the specified link resultset by jumping to the last rowand getting the row number 
 use call statement to retrieve the configurations for substitution and sql for the substitution 
 connect to the database defined in the specified properties file 
 connection to the specified data source 
 merge the connection properties read from beeline hs connection xml with the jdbcconnectionparams extracted from the jdbc url specified in beeline xml
 return whether the specified column  based index is a primary key since this method depends on whether the jdbc driver property implements link resultsetmetadata gettablename many do not itis not reliable for all databases 
 run beeline with the given metastore script flatten the nested scripts into single file 
 test of print method of class tableoutputformat there was an empty extra column after the last one 
 execute the cli work
 print record to string builder
 test parse file
 execute the buffered reader which stores the commands
 archive all the files in the inputfiles into outputfile
 untar an input file into an output file the output file is created in the output folder having the same name as the input file minus the tar extension 
 untar an input file into an output file the output file is created in the output folder having the same name as the input file minus the tar extension 
 private implementation 
 recursively lists status for all files starting from a particular directory or individual file as base case 
 find the parent of path that exists if path does not exist
 perform a check to determine if the user is able to access the file passed in if the user name passed in is different from the current user this method will attempt to do impersonate the user to do the check the current user should be able to create proxy users in this case 
 check if user username has permissions to perform the given fsaction action on all files under the file whose filestatus filestatus is provided
 a best effort attempt to determine if if the file is a local file
 copies files between filesystems 
 checks if delete can be performed on given path by given user if file does not exist it just returns without throwing an exception
 create temporary file and register it to delete on exit hook file deleteonexit is not used for possible memory leakage make sure to use link deletetmpfile file after the file is no longer required and has been deleted to avoid a memory leak 
 get the uri of the path assume to be local file system if no scheme 
 given a path string get all the jars from the folder or the files themselves 
 reads length bytes of data from the stream into the byte buffer 
 simple main to facilitate manual testing of the pause monitor this main function just leaks memory running this class will quickly result in a gc hell and subsequent alerts from the gctimemonitor 
 checks if the current thread has been interrupted and throws runtimeexception is it has 
 get all file status from a root path and recursively go deep into certain levels 
 inheritdoc 
 param printer
 param object
 throws jsonexception
 param object
 initialize logj 
 initialize logj for execution mode 
 stop the subordinate appender for the operation log so it will not leak a file descriptor 
 initializes static metrics instance 
 resets the static context state to initial used primarily for testing purposes note that threadlocalscopes threadlocal is not cleared in this call 
 initializes reporting using hive codahale metrics reporter classes 
 get the inet address of the machine of the given host name 
 return name of current host
 param str the string array to be parsed into an uri array 
 return hostname without throwing exception 
 convert a string to long the input string is first be trimmed and then it is parsed with traditional binary prefix for example k will be converted to      g will be converted to  ^    
 create an object for the given class and initialize it from conf
 create a status server on the given port 
 checks the user has privileges to access to instrumentation servlets if hadoop security instrumentation requires admin is set to false default value it always returns true if hadoop security instrumentation requires admin is set to true it will check if the current user is in the admin acls if the user is in the admin acls it returns true otherwise it returns false 
 same as link httpserver isinstrumentationaccessallowed servletcontext httpservletrequest httpservletresponse except that it returns true only if hadoop security instrumentation requires admin is set to true 
 check if the remote user has access to an object e g query history that belongs to a user
 process a get request for the specified resource 
 test json reporter increment the counter value wait a bit for the new repor to be written read the value from json file verify that the value matches expectation this check is repeated a few times to verify that the values are updated over time 
 read counter value from json metric report
 workaround an incompatible change from commons codec   to   since hadoop has this jar on its classpath we have no way of knowing which version we are running against 
 return  on success  on failure
 param args
 reads a typed bytes sequence and converts it to a java object the first byte is interpreted as a type code and then the right number of subsequent bytes are read depending on the obtained type 
 reads a typed bytes sequence the first byte is interpreted as a type code and then the right number of subsequent bytes are read depending on the obtained type 
 reads the raw bytes following a type bytes code 
 reads the short following a type short code 
 reads the vector following a type vector code 
 reads the list following a type list code 
 reads the map following a type map code 
 writes a java object as a typed bytes sequence 
 writes a bytes array as a typed bytes sequence using a given typecode 
 writes a bytes array as a typed bytes sequence 
 writes a short as a typed bytes sequence 
 writes a vector as a typed bytes sequence 
 writes a list as a typed bytes sequence 
 writes a map as a typed bytes sequence 
 writes a null type marker to the output 
 this function checks the load status of druid segments by polling druid coordinator 
 method that creates a request for druid query using smile format 
 writes to filesystem serialized form of segment descriptor if an existing file exists it will try to replace it 
 this method always needs to be called within a doas block so that the client s tgt credentials can be read from the subject 
 param writable druid writable to be deserialized 
 function to convert druid primitive values to hive primitives main usage of this is to pipe data to vectorrow this has the exact same logic as link druidserde deserialize writable any modification here should be done there as well reason to have  function is that no vectorized path expects writables 
 autogenerates the column types from the given serialization class
 read the schema from the given hdfs url for the schema
 auto generates the column struct
 auto generate the avro struct from class
 auto generate the avro struct from schema
 create a bare tablesnapshotregionsplit needed because writables require a default constructed instance to hydrate from the datainput todo remove once hbase  is fixed 
 parse code jobconf to create a link scan instance 
 initialize the instance for link avroschemaretriever 
 test the default behavior of the lazy family of objects and object inspectors 
 checks for the given roottasks and calls authorizeddlwork for each ddlwork to be authorized the hooks should override this or authorizeddlwork to perform the actual authorization 
 create an instance of a storage handler defined in storerinfo if one cannot be found then fosterstoragehandler is used to encapsulate the inputformat outputformat and serde this storagehandler assumes the other supplied storage artifacts are for a file based storage system 
 create an instance of a storage handler if storagehandler    null then surrrogate storagehandler is used to encapsulate the inputformat outputformat and serde this storagehandler assumes the other supplied storage artifacts are for a file based storage system 
 param timeout the length of time in seconds after a client is created that it should be automatically removed
 returns a cached client if exists or else creates one caches and returns it it also checks that the client is healthy and can be reused
 note that the proper way to construct a java sql date for use with this object is date valueof    
 the purpose of the serialize method is to turn an object representation with a provided objectinspector into a writable format which the underlying layer can then use to write out in this case it means that hive will call this method to convert an object with appropriate objectinspectors that it knows about to write out a hcatrecord 
 return serialized hcatrecord from an underlying object representation and readable by an objectinspector
 return underlying java object from an object representation that is readable by a provided objectinspector 
 helper method to return underlying java map from an object representation that is readable by a provided mapobjectinspector
 constructor constructing a primitive datatype hcatfieldschema
 constructor for constructing a array type or struct type hcatfieldschema passing type and subschema
 constructor for constructing a map type hcatfieldschema passing type of key and value
 deprecated as of   slated for removal with  use link getmapkeytypeinfo instead
 convert a hcatfieldschema to a fieldschema
 move all of the files from the temp directory to the final location
 handles duplicate publish of partition or data into an unpartitioned table if the table is immutable for partitioned tables fails if partition already exists for non partitioned tables fails if files are present in table directory for dynamic partitioned publish does nothing check would need to be done at recordwriter time
 logically split the set of input files for the job returns the underlying inputformat s splits
 create the recordreader for the given inputsplit returns the underlying recordreader if the required operations are supported and schema matches with hcattable schema returns an hcatrecordreader if operations need to be implemented in hcat 
 gets the hcattable schema for the table specified in the hcatinputformat setinput call on the specified job context this information is available only after hcatinputformat setinput has been called for a jobcontext 
 gets the table schema for the table specified in the hcatoutputformat setoutput call on the specified job context note this is the record schema for the table it does not include the table s partition columns 
 gets the table schema for the table specified in the hcatoutputformat setoutput call on the specified job context note this is the complete table schema including the record schema and the partitioning schema 
 check for validity of the output specification for the job 
 gets the output format instance 
 configure the output storage handler
 configure the output storage handler with allowing specification of partvals from which it picks the dynamic partvals
 initializes the input with a null filter see link setinput org apache hadoop conf configuration string string string 
 initializes the input with a provided filter see link setinput org apache hadoop conf configuration string string string 
 initializes the input with a null filter see link setinput org apache hadoop conf configuration string string string 
 see org apache hive hcatalog mapreduce hcatoutputformat setoutput org apache hadoop conf configuration credentials outputjobinfo 
 see org apache hive hcatalog mapreduce hcatoutputformat setschema org apache hadoop conf configuration org apache hive hcatalog data schema hcatschema 
 see org apache hive hcatalog mapreduce initializeinput setinput org apache hadoop conf configuration inputjobinfo 
 set the input to use for the job this queries the metadata server with the specified partition predicates gets the matching partitions and puts the information in the job configuration object to ensure a known inputjobinfo state only the database name table name filter and properties are preserved all other modification from the given inputjobinfo are discarded after calling setinput inputjobinfo can be retrieved from the job configuration as follows code linkedlist lt inputjobinfo gt inputinfo   linkedlist lt inputjobinfo gt hcatutil deserialize job getconfiguration get hcatconstants hcat key job info code 
 returns the given inputjobinfo after populating with data queried from the metadata service 
 serialize this object compressing the partitions which can exceed the allowed jobconf size partitions will be compressed and put into a byte array which then we have to write into the objectoutputstream this method is given 
 deserialize this object decompressing the partitions which can exceed the allowed jobconf size 
 serialization method used by java serialization suppresses serialization of redundant information that s already available from tableinfo before writing out so as to minimize amount of serialized space but restore it back before returning so that partinfo object is still usable afterwards see hive  and hive  for details 
 trigger commit for taskattempt as specified by the taskattemptcontext argument 
 trigger abort for taskattempt as specified by the taskattemptcontext argument 
 method to register call backs to control commits and aborts of taskattempts 
 method to discard the committer call backs for a specified taskattemptid 
 check that a new client is returned for the same configuration after the expiry time also verify that the expiry time configuration is honoured
 test that a long table name actually breaks the hmsc subsequently check that isopen reflects and tells if the client is broken
 test that we properly serialize deserialize hcatrecords
 removes all databases and tables from the metastore
 run a local map reduce job to load data from in memory records to an hcatalog table
 run the dynamic partitioning test but with single map task
 run the dynamic partitioning test but with multiple map task see hcatalog 
 simple test case submits a mapred job which writes out one fixed line to each of the tables uses hive fetch task to read the data and see if it matches what was written 
 create a input file for map
 create a file for map input
 for debugging and testing 
 constructs hcatschema from pigschema passed tableschema is the existing schema of the table in metastore 
 here we are processing hcat table schema as derived from metastore thus it should have information about all fields sub fields but not for partition columns
 convert from pig value object to hive value object this method assumes that link validateschema org apache pig impl logicallayer schema schema fieldschema org apache hive hcatalog data schema hcatfieldschema org apache pig impl logicallayer schema schema org apache hive hcatalog data schema hcatschema int which checks the types in pig schema are compatible with target hive table has been called 
 this method encodes which pig type can map be stored in to which hcat type 
 param optstring may empty str not null in which case it s no op
 param hfs the field schema of the column
 converts object from hive s value system to pig s value system see hcatbasestorer getjavaobj for pig hive conversion 
 test that we properly translate data types in hive hcat table schema into pig schema
 test that value from hive table are read properly in pig
 simulates pig relying on hcatloader to inform about input size of multiple tables 
 note that the value that comes back from hive will have local tz on it using local is arbitrary but datetime needs tz or will assume default and hive does not have tz so if you start with pig value in tz x and write to hive when you read it back the tz may be different the millis value should match of course 
 this is used to test how pig values of various data types which are out of range for hive target column are handled currently the options are to raise an error or write null  create a data file with  column  row  load into pig  use pig to store into hive table  read from hive table using pig  check that read value is what is expected
 artificially complex nested schema to test nested schema conversion
 tests that unnecessary tuples are drop while converting schema pig requires tuples in bags 
 artificially complex nested schema to test nested schema conversion
 test round trip of smallint tinyint hive pig hive this is a more general use case in hcatalog read some data from hive process it in pig write result back to a hive table 
 ensure pig can read write tinyint smallint columns 
 param tableevent table event 
 param tableevent table event 
 param tableevent alter table event
 param partitionevent partition event
 param partitionevent partition event
 param partitionevent partition event
 param dbevent database event
 param dbevent database event
 param dbevent alter database event
 param fnevent function event
 param fnevent function event
 param addprimarykeyevent add primary key event
 param addforeignkeyevent add foreign key event
 param adduniqueconstraintevent add unique constraint event
 param addnotnullconstraintevent add not null constraint event
 param dropconstraintevent drop constraint event
 param allocwriteidevent alloc write id event
 close statement instance 
 close the resultset 
 param hcateventmessage the hcateventmessage being sent over jms this method is threadsafe
 does a health check on the connection by sending a dummy message create the connection if the connection is found to be bad also recreates the session
 create the jms connection
 send a dummy message to probe if the jms connection is healthy
 param m
 param m
 constructor allows overriding separator of the lazysimpleserde
 deprecated as of release     replaced by link newconnection boolean string 
 deprecated as of release     replaced by link newconnection boolean hiveconf string 
 deprecated as of release     replaced by link newconnection boolean hiveconf usergroupinformation string 
 acquire a new connection to metastore for streaming
 acquire a new connection to metastore for streaming
 acquire a new connection to metastore for streaming to connect using kerberos authenticateduser argument should have been used to do a kerberos login additionally the hive metastore kerberos principal setting should be set correctly either in hive site xml or in the conf argument if not null if using hive site xml it should be in classpath 
 acquires a new batch of transactions from hive 
 represents a batch of transactions acquired from metastore
 activate the next available transaction in the current transaction batch
 write record using recordwriter
 a transaction batch opens a single hdfs file and writes multiple transaction to it if there is any issue with the write we can t continue to write to the same file any as it may be corrupted now at the tail this ensures that a client can t ignore these failures and continue to write 
 write records using recordwriter
 commit the currently open transaction
 abort the currently open transaction
 close the transactionbatch this will abort any still open txns in this batch 
 returns a base  encoded representation of the supplied link acidtable 
 returns the link acidtable instance decoded from a base  representation 
 attempts to acquire a read lock on the table returns if successful throws exception otherwise 
 attempts to acquire a read lock on the table returns if successful throws exception otherwise 
 connects to the link imetastoreclient meta store that will be used to manage link transaction life cycles also checks that the tables destined to receive mutation events are able to do so the client should only hold one open transaction at any given time todo enforce this 
 visible for testing only 
 visible for testing only 
 we expect records grouped by partitionvalues bucketid and ordered by origwriteid rowid 
 we expect records grouped by partitionvalues bucketid and ordered by origwriteid rowid 
 we expect records grouped by partitionvalues bucketid and ordered by origwriteid rowid 
 param bucketproperty from existing link recordidentifier getbucketproperty 
 param endpoint the end point to write to
 param regex to parse the data
 make sure it works with table where bucket col is not st col
 test that streaming can write to unbucketed table 
 deprecated use link checkdatawritten path long long int string boolean string there is little value in using inputformat directly
 param validationquery query to read from table to compare data against code records 
 creates an instance of hcatclient 
 setter for hcattable required for deserialization 
 method to adopt the specified attributes from rhs into this hcattable object 
 create factory instance for instantiating replicationtasks the order precedence is as follows a if a factory has already been instantiated and is valid use it b if a factoryclassname has been provided through resetfactory attempt to instantiate that c if a hive repl task factory has been set in the default hive conf use that d if none of the above methods work instantiate an anoymous factory that will return an error whenever called till a user calls resetfactory 
 command implements writable but that s not terribly easy to use compared to string even if it plugs in easily into the rest of hadoop provide utility methods to easily serialize and deserialize commands deserializecommand instantiates a concrete command and initializes it given a base string representation of it 
 test for event based replication scenario does not test if replication actually happened merely tests if we re able to consume a repl task iter appropriately calling all the functions expected of the interface without errors 
 test for detecting schema changes for an hcatalog table across  different hcat instances a table is created with the same schema on  hcat instances the table schema is modified on the source hcat instance columns io formats serde definitions etc the table metadata is compared between source and target the changes are detected and propagated to target 
 test that partition definitions can be replicated between hcat instances independently of table metadata replication  identical tables are created on  different hcat instances source and target on the source instance  one partition is added with the old format textfile  the table is updated with an additional column and the data format changed to orc  another partition is added with the new format  the partitions metadata is copied to the target hcat instance without updating the target table definition  the partitions metadata is tested to be an exact replica of that on the source 
 test that partition definitions can be replicated between hcat instances independently of table metadata replication using partitionspec interfaces this is essentially the same test as testpartitionregistrationwithcustomschema transliterated to use the partitionspec apis  identical tables are created on  different hcat instances source and target on the source instance  one partition is added with the old format textfile  the table is updated with an additional column and the data format changed to orc  another partition is added with the new format  the partitions metadata is copied to the target hcat instance without updating the target table definition  the partitions metadata is tested to be an exact replica of that on the source 
 call the callback url with the jobid to let them know it s finished if the url has the string jobid in it it will be replaced with the completed jobid 
 run the program synchronously as the given user we rate limit the number of processes that can simultaneously created for this instance 
 run the program synchronously as the given user warning commandline will trim the argument strings 
 given a program name lookup the fully qualified path throws an exception if the program is missing or not authorized 
 return a json description of the database 
 return a json show databases like this will return a list of databases 
 create a database with the given name
 drop the given database
 create a table 
 create a table like another 
 return a json description of the table 
 return a json show table like this will return a list of tables 
 return a json show table extended like with extra info from desc exteded this will return table with exact name match 
 drop a table 
 rename a table 
 describe one table property 
 list the table properties 
 add one table property 
 return a json description of the partitions 
 return a json description of one partition 
 add one partition 
 drop a partition 
 return a json description of the columns same as describetable 
 return a json description of one column 
 add one column 
 create a new map object from the existing json 
 create a new map object 
 create a new map error object 
 turn the map back to json 
 enqueue the templetoncontrollerjob directly calling doas 
 verifies a that proxyuser is making the request from authorized host and that doas user belongs to one of the groups for which proxyuser is allowed to impersonate users 
 create the delegation token 
 cleanup
 execute an hcat ddl expression on the local box it is run as the authenticated user and rate limited 
 list all the tables in an hcat database 
 create a new table 
 create a new table like another table 
 describe an hcat table this is normally a simple list of columns using desc table but the extended format will show more information using show table extended like 
 drop an hcat table 
 rename an hcat table 
 describe a single property on an hcat table 
 list all the properties on an hcat table 
 add a single property on an hcat table 
 list all the partitions in an hcat table 
 describe a single partition in an hcat table 
 create a partition in an hcat table 
 drop a partition in an hcat table 
 list all databases or those that match a pattern 
 describe a database
 create a database
 drop a database
 list the columns in an hcat table currently the same as describe table 
 describe a single column in an hcat table 
 create a column in an hcat table 
 run a mapreduce streaming job 
 run a mapreduce jar job params correspond to the rest api params
 run a pig job params correspond to the rest api params if usehcatalog is in the code pigargs useshcatalog is interpreted as true 
 run a sqoop job 
 run a hive job 
 return the status of the jobid 
 kill a job in the queue 
 return all the known job ids for this user based on the optional filter conditions example usages  curl s http localhost  templeton v jobs user name hsubramaniyan return all the job ids submitted by hsubramaniyan  curl s http localhost  templeton v jobs user name hsubramaniyan showall true return all the job ids that are visible to hsubramaniyan  curl s http localhost  templeton v jobs user name hsubramaniyan jobid job   return all the job ids for hsubramaniyan after job    curl s http localhost  templeton v jobs user name hsubramaniyan jobid job   numrecords  return the first  atmost job ids submitted by hsubramaniyan after job    curl s http localhost  templeton v jobs user name hsubramaniyan numrecords  return the first  atmost job ids submitted by hsubramaniyan after sorting the job id list lexicographically supporting pagination using jobid and numrecords parameters step  get the start jobid   job xxx  numrecords   n step  issue a curl command by specifying the user defined numrecords and jobid step  if list obtained from step  has size equal to numrecords retrieve the list s last record and get the job id of the last record as job yyy k else quit step  set jobid  job yyy k and go to step  
 returns canonical host name from which the request is made used for doas validation
 run the cleanup loop 
 the percent complete of a job
 add a jobid to the list of children of this job 
 set parent job of this job
 get a list of jobstates for jobs that are children of this job 
 the system exit value of the job 
 when this job was created 
 the user who started this job 
 the url callback
 the status of a job once it is completed 
 the time when the callback was sent 
 fetch an integer field from the store 
 get an id for each currently existing job which can be used to create a jobstate object 
 retrieves the templeton launcher job submit time from the configuration if not available throws 
 attempts to reconnect to an already running child job of the templeton launcher this is used in cases where the templeton launcher task has failed and is retried by the mr framework if reconnect to the child job is possible the method will continue tracking its progress until completion 
 enqueue the job and print out the job id for later collection 
 returns all files non recursive in code dirname 
 get the given url returns the number of bytes received 
 find a jar that contains a class of the same name and which file name matches the given pattern 
 run the cleanup loop 
 open a zookeeper connection for the jobstate 
 create a node in zookeeper
 a helper method that sets a field value 
 check that we return correct status code when the url doesn t map to any method in link server 
 tries to drop table in a db that doesn t exist
 tries to drop table in a db that doesn t exist
 tries to drop table that doesn t exist with ifexists true 
 it s expected that templeton returns a properly formatted json object when it encounters an error it should have code error code element in it whichshould be the hive canonical error msg code 
 run cmp command
 compare the results
 define the sql query to access data
 execute a sql query
 prepare a sql query
 execute a sql statement
 run pre sql statements 
 get a connection
 open a new connection
 run copy command
 copy the query results to another table
 copy the query results to a file
 compile and run hpl sql script 
 initialize pl hql
 include statements from a file
 execute statements from an include file
 get the program return code
 save local functions and procedures to a file will be added to the distributed cache 
 run a hive command line
 trace values retrived from the database
 create a file
 open an existing file
 check if the directory or file exists
 to timestamp function
 part count function
 part count by function
 get data types for all columns of the select statement
 read the column data from the database and cache it
 close statement results
 executing or building select statement
 describe statement
 fetch cursor statement
 assignment from select statement 
 for cursor statement
 exec execute and execute immediate statement to execute dynamic sql or stored procedure
 execute udf
 set the new value from the result set
 set row values from the result set
 return an integer value
 run a test file
 get test output
 run a test file
 initializes resources that will be needed for each of the benchmark tests 
 this method is invoked before every call to the methods to test it creates resources that are needed for each call not in a benchmark level 
 create a minihs with the hive service principal and keytab in minihivekdc
 create a minihs with the hive service principal and keytab in minihivekdc
 test a nonkrb user could login the kerberized hs with authentication type sasl none
 test a nonkrb user could login the kerberized hs with authentication type sasl none
 negative test verify that connection to secure hs fails if it is nosasl
 negative test verify that nonkrb connection to secure hs fails if it is user pwd do not match 
 negative test for token based authentication verify that token is not applicable to non kerberos sasl user
 basic connection test
 negative test verify that connection to secure hs fails when required connection attributes are not provided
 test isvalid method
 negative test isvalid method
 test token based authentication over kerberos login as super user and retrieve the token for normal user use the token to connect connect as normal user
 negative test for token based authentication verify that a user can t retrieve a token for user that it s not allowed to impersonate
 test connection using the proxy user connection property
 test connection using the proxy user connection property verify proxy connection fails when super user doesn t have privilege to impersonate the given user
 verify the config property value
 throws java lang exception
 test the if authorization failed passed for functioninvoker that invokes a metastore client api call
 test that with no verification and record verification enabled hive populates the schema and version correctly
 test that with verification enabled hive works when the correct schema is already populated
 store garbage version in metastore and verify that hive fails when verification is on
 store higher version in metastore and verify that hive works with the compatible version
 test to ensure that hiveconf does not try to load hivemetastore site xml when remote metastore is used 
 set new hive site xml file location that has remote metastore config
 tests basic operation creates a db with  tables  ptned and  unptned inserts data into one of the ptned tables and one of the unptned tables and verifies that a repl dump followed by a repl load is able to load it appropriately this tests bootstrap behaviour primarily 
 verify replication when string partition column value has special chars
 all the results that are read from the hive output will not preserve case sensitivity and will all be in lower case hence we will check against only lower case data values unless for null values it actually returns in uppercase and hence explicitly lowering case before assert 
 all the results that are read from the hive output will not preserve case sensitivity and will all be in lower case hence we will check against only lower case data values unless for null values it actually returns in uppercase and hence explicitly lowering case before assert 
 verify s result without regard for ordering 
 tests non acid to acid conversion where starting table has non standard layout i e where original files are not immediate children of the partition dir
 tests non acid to acid conversion where starting table has non standard layout i e where original files are not immediate children of the partition dir partitioned table how to do this ctas is the only way to create data files which are not immediate children of the partition dir ctas union tez doesn t support partition tables the only way is to copy data files in directly actually insert into select union all with hive optimize union remove and hivefetchtaskconversion  none will create subdirs but if writing to non acid table there is a merge task on mr but not on tez 
 ctas  tez  union creates a non standard layout in table dir each leg of the union places data into a subdir of the table partition subdirs are named hive union subdir  hive union subdir  etc for acid tables the writer for each dir must have a different statementid ensured by link org apache hadoop hive ql optimizer queryplanpostprocessor link org apache hadoop hive ql metadata hive moveacidfiles filesystem filestatus path list drops the union subdirssince each delta file has a unique name 
  insert into regular unbucketed table from union all union is removed and data is placed in subdirs of target table  convert to acid table and check data  compact and check data compare with link testacidinsertwithremoveunion where t is transactional true
  insert into unbucketed acid table from union all union is removed and data is placed in subdirs of target table  convert to acid table and check data  compact and check data compare with link testinsertwithremoveunion where t is transactional false
 tests if previous table data skips trash when insert overwrite table is run against a table which has auto purge property set
 tests when auto purge is set to a invalid string trash should be used for insert overwrite queries
 test when auto purge property is not set data should be moved to trash for insert overwrite queries
 tests if the auto purge property works correctly for external tables old data should skip trash when insert overwrite table is run when auto purge is set to true
 tests auto purge when managed table is partitioned old data should skip trash when insert overwrite table is run and auto purge property is set to true
 tests auto purge for an external partitioned table old partition data should skip trash when auto purge is set to true
 tests when auto purge is set to false older data is moved to trash when insert overwrite table is run
 tests when auto purge is set to false on a external table older data is moved to trash when insert overwrite table is run
 tests when auto purge is set to false on a partitioned table older data is moved to trash when insert overwrite table is run
 tests when auto purge is set to false on a partitioned external table older data is moved to trash when insert overwrite table is run
 test util method to run the insert overwrite table or truncate table test on a table
 test util method to run the insert overwrite table or truncate table test on a table
 validate only that the location is correct 
 verify that the location of the partition is valid in this case the path should end in parta and not dt a the default 
 simple schema evolution add columns with partitioning 
 after each major compaction stats need to be updated on each column of the table partition which previously had stats  create a bucketed orc backed table orc is currently required by acid  populate  partitions with data  compute stats  insert some data into the table using streamingapi  trigger major compaction which should update stats  check that stats have been updated
 users have the choice of specifying compaction related tblproperties either in create table statement or in alter table compact statement this tests both cases 
 convenience method to execute a select stmt and dump results to log file
 test script formatting
 test nested script formatting
 test nested script formatting
 test script formatting
 test script formatting
 test nested script formatting
 test script formatting
 start up a local hive server  for these tests
 test if beeline prompts for a password when optional password option is at the end of arguments
 test if beeline prompts for a password when optional password option is at the middle of arguments
 test if beeline prompts for a password when optional password option is used in conjunction with additional commandline options after p
 test if beeline prompts for a password when optional password option is used in conjunction with additional beelineopts options after p
 test if beeline prompts for a password when optional password option is used in conjunction with additional beelineopts options after p also verifies the beelineopt value is set as expected
 tests if beeline prompts for a password and also confirms that hiveconf argument works when given immediately after p with no password
 tests if beeline doesn t prompt for a password and connects with empty password when no password option provided
 tests if beeline doesn t prompt for a password and connects with no password username option provided
 tests if beeline prompts for password when p is the last argument and arglist has commandline options as well as beelineopts
 connects to minihs using beeline with the given string value for the prompt if the prompt is null uses beeline with null inputstream in which this method expects that the arglist is sufficient to make a successful beeline connection with no prompt required from user
 create table for use by tests
 start up a local hive server  for these tests
 create table for use by tests
 execute a script with beeline f or i 
 attempt to execute a simple script file with the f and i option to beeline to test for presence of an expected pattern in the output stdout or stderr fail if not found print passed or failed
 attempt to execute a simple script file with the f or i option to beeline or both to test for presence of an expected pattern in the output stdout or stderr fail if not found print passed or failed
 attempt to execute the enclosed query with the e option to beeline test for presence of an expected pattern in the output stdout or stderr fail if not found print passed or failed
 hive 
 call the hs metadata api s with authorizer allowing those calls
 call the hs metadata api s with authorizer disallowing those calls
 test cli kill command of a query that is running we spawn  threads one running the query and the other attempting to cancel we re using a dummy udf to simulate a query that runs for a sufficiently long time 
 verify explain resultset
 execute non select statements using execute and executeupdated apis of preparedstatement interface
 execute set x and extract value from key val format result verify the extracted value
 test the type returned for pre created table type table and view type table
 test if connection getmetadata gettabletypes returns expected tabletypes
 validate the metadata for the result set of a metadata getcolumns call 
 validate schema generated by set command
 validate error on closed resultset
 test bad args to getxxx 
 verify selecting using builtin udfs
 verify selecting named expression columns
 test getprocedurecolumns 
 test testproccedures 
 test getprimarykeys 
 test getimportedkeys 
 test the cursor repositioning to start of resultset
 test the cursor repositioning to start of resultset from non mr query
 test for cursor repositioning to start of resultset for non sql commands
 test for cursor repositioning to start of resultset for non sql commands
 negative test for cursor repositioning to start of resultset verify unsupported jdbc resultset attributes
 negative test for cursor repositioning to start of resultset verify unsupported jdbc resultset methods
 read the results locally then reset the read position to start and read the rows again verify that we get the same results next time 
 test the cancellation of a query that is running we spawn  threads one running the query and the other attempting to cancel we re using a dummy udf to simulate a query that runs for a sufficiently long time 
 loads data from a table containing non ascii value column runs a query and compares the return value
 test getting query log method in jdbc
 test getting query log when hs disable logging 
 not a unit test this simply runs a minihs cluster which can be used for manual testing 
 test if the minihs configuration gets passed down to the session configuration
 verify explain resultset
 execute non select statements using execute and executeupdated apis of preparedstatement interface
 test the type returned for pre created table type table and view type table
 validate the metadata for the result set of a metadata getcolumns call 
 validate schema generated by set command
 validate error on closed resultset
 test bad args to getxxx 
 verify selecting using builtin udfs
 verify selecting named expression columns
 test getprimarykeys 
 test testprimarykeysnotnull 
 test the cursor repositioning to start of resultset
 test the cursor repositioning to start of resultset from non mr query
 test for cursor repositioning to start of resultset for non sql commands
 test for cursor repositioning to start of resultset for non sql commands
 negative test for cursor repositioning to start of resultset verify unsupported jdbc resultset methods
 read the results locally then reset the read position to start and read the rows again verify that we get the same results next time 
 tests for query cancellation
 test the cancellation of a query that is running we spawn  threads one running the query and the other attempting to cancel we re using a dummy udf to simulate a query that runs for a sufficiently long time 
 test the non null value of the yarn ats guid we spawn  threads one running the query and the other attempting to read the ats guid we re using a dummy udf to simulate a query that runs for a sufficiently long time 
 test link hivestatement executeasync string for a select query
 test link hivestatement executeasync string for a create table
 test link hivestatement executeasync string for an insert overwrite into a table
 verify that the connection to hs with minimr is successful 
 run nonmr query 
 run nonmr query 
 verify if the given property contains the expected value 
 verify that the connection to minihs is successful
 run nonmr query
 run nonmr query
 verify if the given property contains the expected value
 this test is to connect to any database without using the command use   connect to default database  create a new db test default  connect to test default database  connect and create table under test default test  connect and display all tables  connect to default database and shouldn t find table test default test  connect and drop test default test  drop test default database 
 verify that the current db is the one expected first create table as tab and then describe that table to check if is the current database
 this method tests whether while creating a new connection the config variables specified in the jdbc uri are properly set for the connection this is a test for hiveconnection configureconnection 
 tests the creation of the  scratch dirs hdfs local downloaded resources which is also local  test with doas false open a new jdbc session and verify the presence of directories permissions  test with doas true open a new jdbc session and verify the presence of directories permissions
 test udf whitelist verify default value verify udf allowed with default whitelist verify udf allowed with specific whitelist verify udf disallowed when not in whitelist
 test udf blacklist verify default value verify udfs allowed with default blacklist verify udf disallowed when in blacklist
 test udf blacklist overrides whitelist
 tests the creation of the root hdfs scratch dir which should be writable by all 
 test for http header size
 test for jdbc driver retry on nohttpresponseexception
 tests that datanucleus nucleuscontext classloaderresolvermap clears cached class objects hence doesn t leak classloaders on closing any session
 tests add jar uses hives reflectionutil constructor cache
 a test that checks that lineage is correct when a multiple concurrent requests are make on a connection
 test describe extended on tables that have special white space characters in the row format 
 get detailed table information via jdbc
 setup a mini hs with minimr 
 setup a connection to the test database before each test 
 close connection after each test 
 create a database 
 test explain on fs with erasure coding 
 test describe on fs with erasure coding 
 test mr stats 
 get extended explain output via jdbc 
 test cli kill command of a query that is running we spawn  threads one running the query and the other attempting to cancel we re using a dummy udf to simulate a query that runs for a sufficiently long time 
 run nonspark query
 run spark query
 initiate a non sasl connection the session hook will verfiy the user name set correctly
 tests to ensure sslv and sslv are disabled
 test ssl client with non ssl server fails
 test non ssl client with ssl server fails
 test ssl client connection to ssl server
 test ssl client connection to ssl server
 start hs in ssl mode open a ssl connection and fetch data
 start hs in http mode with ssl enabled open a ssl connection and fetch data
 test a new connection when server sends a certificate with wrong cn sends a certificate for www example com instead of localhost opening a new connection with this wrong certificate should fail
 test hms server with ssl
 open a session and create a table for cases usage
 open a session and create a table for cases usage
 assert that the appender for the given queryid is in the expected state 
 get the appender associated with a query 
 test if webui captures information on current historic sql operations 
 test for the hiveconf option hive server webui explain output 
 test for the hiveconf options hive server webui show graph hive server webui max graph size 
 test for the hiveconf option hive server webui show stats which is available for mapreduce jobs only 
 sanity check if basic information is delivered in this html let s not go too crazy and assert each element to make it easier to add ui improvements 
 configure a wrong service endpoint for the client transport and test for error 
 test additional http headers passed to request interceptor 
 test additional http headers passed to request interceptor 
 test if addresses in x forwarded for are passed to hiveauthorizer calls
 wait for metastore to have initialized token manager this does not have to be done in other metastore test cases as they use metastore client which will retry few times on failure
 verify test is running with mr and queue mapping defaults are set queue mapping is set for the connected user 
 verify test is running with mr and queue mapping are set correctly for primary group rule 
 verify test is running with mr and queue mapping are set correctly for primary group rule 
 verify that the queue refresh doesn t happen when configured to be off 
 verify that the given property contains the expected value 
 clear out any side effects of running tests
 clear out any side effects of running tests
 setup to execute a set of query files uses qtestutil to do so 
 executes a set of query files in sequence 
 executes a set of query files parallel each query file is run in a separate thread the caller has to arrange that different query files do not collide in terms of destination tables 
 compare the filtered file with the expected golden file
 return connection url for this server instance
 return connection url for this server instance
 return connection url for this server instance
 return connection url for this server instance
 param columnindex the first column is  the second is  
 create transport per the connection options supported transport options are sasl based transports over  kerberos  delegation token  ssl  non ssl raw non sasl socket kerberos and delegation token supports sasl qop configurations
 package scoped access to the driver s major version
 package scoped access to the driver s minor version
 loads the manifest attributes from the jar 
 package scoped to allow manifest fetching from other hivedriver classes helper to initialize attributes and return one 
 invokes executequery sql using the sql provided to the constructor 
 invokes executequery sql using the sql provided to the constructor 
 starts the query execution asynchronously on the server and immediately returns to the client the client subsequently blocks on resultset next or statement getupdatecount depending on the query type users should call resultset next or statement getupdatecount depending on whether query returns results to ensure that query completes successfully calling another execute method or close before query completion would result in the async query getting killed if it is not already finished note this method is an api for limited usage outside of hive by applications like apache ambari although it is not part of the interface java sql statement 
 close statement if needed and reset the flags 
 get the execution logs of the given sql statement this method is a public api for usage outside of hive although it is not part of the interface java sql statement this method gets the incremental logs during sql execution and uses fetchsize holden by hivestatement object 
 get the execution logs of the given sql statement this method is a public api for usage outside of hive although it is not part of the interface java sql statement 
 parse jdbc connection url the new format of the url is jdbc hive dbname sess var list hive conf list hive var list where the optional sess conf and var lists are semicolon separated   pairs for utilizing dynamic service discovery with hiveserver multiple comma separated host port pairs can be specified as shown above the jdbc driver resolves the list of uris and picks a specific server instance to connect to currently dynamic service discovery using zookeeper is supported in which case the host port pairs represent a zookeeper ensemble as before if the host port is not specified it the driver runs an embedded hive examples jdbc hive ubuntu  db hive cli conf printheader true hive exec mode local auto inputbytes max  stab salestable icol customerid jdbc hive hive cli conf printheader true hive exec mode local auto inputbytes max  stab salestable icol customerid jdbc hive ubuntu  db user foo password bar connect to http server  hs with specified basicauth credentials and initial database jdbc hive server  db user foo password bar hive server transport mode http hive server thrift http path hs
 this method handles the base parsing of the given jdbc uri some of jdbcconnectionparams returned from this method are updated if zookeeper is used for service discovery
 takes a version string delimited by and characters and returns a partial version 
 method to get canonical ized hostname given a hostname possibly a cname this should allow for service principals to use simplified cnames 
 apply configs published by the server configs specified from client s jdbc uri override configs published by the server 
 release all db resources
 retry an operation using fuzzy exponentially increasing backoff the wait time after the nth failed attempt is min ms ms pow  n  fuzzed by a number drawn from a gaussian distribution with mean  and standard deviation   if maxtries is exhausted or if shouldretry returns false the last exception thrown by f will be thrown by this function 
 the non abort close method can be split into  parts part one is to flush to kafka all the buffered records then log topic partition offset part two is to either commit the tx or save the state of the tx to wal and let hs do the commit 
 will return hostname stored in inetaddress 
 will return fqdn of the host after doing reverse dns lookip 
 param nodeid hostname  post 
 verifies the token available as serialized bytes 
 calling getsplits will open a hiveserver connection which should be closed by the calling application using llapbaseinputformat close when the application is done with the splits 
 close the connection associated with the handle id if getsplits was configured with a handle id call when the application is done using the splits generated by getsplits 
 close all outstanding connections created by getsplits calls
 sends out final events for task success 
 sends out final events for task failure 
 throws nullpointerexception inheritdoc 
 throws nullpointerexception inheritdoc 
 throws nullpointerexception inheritdoc 
 throws nullpointerexception inheritdoc 
 throws nullpointerexception inheritdoc 
 throws nullpointerexception inheritdoc 
 get and create if required local dirs for a query
 register completion for a query
 register an update to a source within an executing dag
 creates the record reader and checks the input specific compatibility 
 gets file metadata for the split from cache or reads it from the file 
 reads the metadata for all stripes in the file 
 unlike the other overload of processoneslice doesn t cache data 
 register a base dir for an application
 this method transfers data using local buffer it transfers data from a disk to a local buffer in memory and then it transfers data from the buffer to the target this is used only if transferto is disallowed in the configuration file super transferto does not perform well on windows due to a small io request generated customshuffletransfer can control the size of the io requests by changing the size of the intermediate buffer 
 this method gets the index information for the given mapid and reduce it reads the index file into cache if it is not already present 
 param node
 param timeoutms
 returns a cached client if exists or else creates one caches and returns it it also checks that the client is healthy and can be reused
 getdeserializer get the deserializer for a table 
 finds the expired date partitioned events directory based on ttl and delete them 
 wait for the cache entry to go from pending to valid status 
 updates a pending cache entry with a fetchwork result from a finished query if successful the cache entry will be set to valid status and be usable for cached queries important adding the entry to the cache will increment the reader count for the cache entry cacheentry releasereader should be called when the caller is done with the cache entry 
 dumps process heap 
 outputs some bytes as hex w printable characters prints helpful debug method c p from hbase bytes 
 do authorization using post semantic analysis information in the semantic analyzer the original command is also passed so that authorization interface can provide more useful information in logs 
 acquire read and write locks needed by the statement the list of objects to be locked are obtained from the inputs and outputs populated by the compiler locking strategy depends on hivetxnmanager and hivelockmanager configured this method also records the list of valid transactions this must be done after any transactions have been opened 
 polls running tasks to see if a task has ended 
 creates helper for archive 
 makes sure that uri points to directory by adding slash to it useful in relativizing uris 
 determines if one can insert into partition s or there s a conflict with archive it can be because partition is itself archived or it is to be created inside existing archive the second case is when partition doesn t exist yet but it would be inside of an archive if it existed this one is quite tricky to check we need to find at least one partition inside of the parent directory if it is archived and archiving level tells that the archival was done of directory partition is in it means we cannot insert otherwise we can this method works both for full specifications and partial ones in second case it checks if any partition that could possibly match such specification is inside archive 
 forward a record of join results 
 write list of string entries into given file
 rename a partition in a table
 write a list of partitions to a file 
 write a statement of how to create a table to a file 
 write a list of the available databases to a file 
 write a list of the tables views in the database to a file 
 write a list of the columns in the table to a file 
 write a list of the user defined functions to a file 
 write a list of the current locks to a file 
 shows a description of a function 
 write the status of tables to a file 
 write the properties of a table to a file 
 write the description of a table to a file 
 alter a given table 
 drop a given table or some partitions droptabledesc is currently used for both 
 create a database
 create a new table 
 create a new table like an existing table 
 gets the evaluator class for the udaf given the parameter types 
 processes the provided task logs using the known error heuristics to get the matching errors 
 processes the provided task logs to extract stack traces 
 evaluate the expression given the row this method should use the rowinspector passed in from initialize to inspect the row object the return value will be inspected by the return value of initialize if this evaluator is referenced by others store it for them
 if the genericudf is a base comparison it returns an integer based on the result of comparing the two sides of the udf like the compareto method in comparable if the genericudf is not a base comparison or there is an error executing the comparison it returns null 
 get the next row and push down it to operator tree currently only used by fetchtask 
 get the next row the fetch context is modified appropriately 
 clear the context if anything needs to be done 
 create new path 
 returns the set of synonyms of the supplied function 
 get the genericudaf evaluator for the name and argumentclasses 
 this method is shared between udfregistry and udafregistry methodname will be evaluate for udfregistry and aggregate evaluate evaluatepartial for udafregistry 
 a shortcut to get the index genericudf this is used for getting elements out of array and getting values out of map 
 a shortcut to get the and genericudf 
 both udf and udaf functions can imply order for analytical functions
 use this to check if function is ranking function
 estimate the number of entries in map side hash table the user can specify the total amount of memory to be used by the map side hash by default all available memory is used the size of each row is estimated rather crudely and the number of entries are figure out based on that 
 flush hash table this method is used by hash based aggregations
 forward a record of keys and aggregation results 
 forward all aggregations to children it is only used by demuxoperator 
 we need to forward all the aggregations to children 
 return the value as a standard object standardobject can be inspected by a standard objectinspector 
 continue processing join between spilled hashtable s and spilled big table
 iterate over the big table row container and feed process with leftover rows
 initializes this map op as the root of the tree it sets jobconf mapredwork and starts initialization of the operator tree rooted at this op 
 execute a query plan using hadoop 
 given a hive configuration object generate a command line fragment for passing such configuration information to execdriver 
 fetch http tracker om gc jsp threshold period 
 collects all the parent s output object inspectors and calls actual initialization method 
 unlike other operator interfaces which are called from map or reduce task jobclose is called from the jobclient side once the job has completed 
 initialize an array of exprnodeevaluator and put the return values into a structobjectinspector with integer field names 
 clones only the operator the children and parent are set to null 
 recursively clones all the children of the tree fixes the pointers to children parents and the pointers to itself coming from the children it does not fix the pointers to itself coming from parents parents continue to point to the original child 
 writes first value and lengths to finish the first record after the key has been written 
 writes the value and value length for non first record 
 called when loading the hashtable 
 move the hashtable of a specified partition from memory into local file system
 serializes row to output for vectorized path 
 loads the table container from a folder only used on spark path 
 loads the small table into a vectormapjoinfasttablecontainer only used on spark path 
 looks up the function name in the registry if enabled will attempt to search the metastore for the function 
 get the genericudaf evaluator for the name and argumentclasses 
 unregisters all the functions belonging to the specified database
 returns the root task for adding a partition
 create scratch directory for spark session if it does not exist this method is not thread safe 
 if the existingsession can be reused return it otherwise close it and remove it from the list create a new session and add it to the list 
 process one row using a dummy iterator or add row to vector batch 
 given a vertex group and a vertex createedge will create an edge between them 
 given two vertices and the configuration for the source vertex createedge will create an edge object that connects the two 
 localizes files archives and jars the user has instructed us to provide on the cluster as resources for execution 
 localizes files archives and jars from a provided array of names 
 testing related creates an empty archive to served being localized as hive exec
 localizes a resources should be thread safe 
 create a vertex from a given work object 
 logic for the thread that restarts the sessions expired during foreground operations 
 generate groups of splits separated by schema evolution boundaries 
 generate groups of splits separated by schema evolution boundaries 
 reopens the session that was found to not be running 
 creates a tez session a session is tied to either a cli hs session you can submit multiple dags against a session as long as they are executed serially 
 close a tez session will cleanup any tez am related resources after closing a session no further dags can be executed against it only called by session management classes some sessions should not simply be closed by users e g pool sessions need to be restarted 
 returns a local resource representing a jar this resource will be used to execute the plan on the cluster 
 return true if there are more key values and advances to next key values
 return true if there are more key values and advances to next key values
 called once when hs initializes 
 this is the main method of the master thread the processes one set of events be mindful of the fact that events can be queued while we are processing events so in addition to making sure we keep the current set consistent e g no need to handle update errors for a session that should already be destroyed this needs to guard itself against the future iterations e g what happens if we kill a query due to plan change but the dag finished before the kill happens and the user queues a return event etc do not block for a long time in this method 
 try store the non vectorized key 
 perform basic checks and initialize topnhash for the new vectorized row batch 
 flushes all the rows cached in the heap 
 returns index for key value hashcode if it s acceptable     can be returned for other actions  for forward should be forwarded to output collector for gby  for excluded not in top k ignore it
 convert an output stream to a compressed output stream based on codecs and compression options specified in the job configuration 
 create a sequencefile output stream based on job configuration 
 moves files from src to dst if it is within the specified set of paths
 rename src to dst or in the case dst already exists move files in src to dst if there is an existing file with the same name the new file s name will be appended with   etc 
 remove all temporary files and duplicate double committed files from a given directory 
 remove all temporary files and duplicate double committed files from a given directory 
 create a url from a string representing a path to a local file the path string can be just a path or can start with file file 
 extract db and table name from dbtable string where db and table are separated by if there is no db name part set the current sessions default db
 calculate the total size of input files 
 retry sql execution with random backoff same as the one implemented in hdfs  this function only retries when the sql query throws a sqltransientexception which might be able to succeed with a simple retry it doesn t retry when the exception is a sqlrecoverableexception or sqlnontransientexception for sqlrecoverableexception the caller needs to reconnect to the database and restart the whole transaction 
 retry connecting to a database with random backoff same as the one implemented in hdfs  this function only retries when the sql query throws a sqltransientexception which might be able to succeed with a simple retry it doesn t retry when the exception is a sqlrecoverableexception or sqlnontransientexception for sqlrecoverableexception the caller needs to reconnect to the database and restart the whole transaction 
 retry preparing a sql statement with random backoff same as the one implemented in hdfs  this function only retries when the sql query throws a sqltransientexception which might be able to succeed with a simple retry it doesn t retry when the exception is a sqlrecoverableexception or sqlnontransientexception for sqlrecoverableexception the caller needs to reconnect to the database and restart the whole transaction 
 on tez we re not creating dummy files when getting setting input paths we let tez handle the situation we re also setting the paths in the am so we don t want to depend on scratch dir and context 
 computes a list of all input paths needed to compute the given mapwork all aliases are considered and a merged list of input paths is returned if any input path points to an empty table or partition a dummy file in the scratch dir is instead created and added to the list this is needed to avoid special casing the operator pipeline for these cases 
 hive uses tmp directories to capture the output of each filesinkoperator this method creates all necessary tmp directories for filesinks in the mapwork 
 hive uses tmp directories to capture the output of each filesinkoperator this method creates all necessary tmp directories for filesinks in the reducework 
 get header line count for a table 
 get footer line count for a table 
 returns the full path to the jar containing the class it always return a jar 
 checks if path passed in exists and has writable permissions the path will be created if it does not exist 
 convert input string to a decimal at position i in the respective vectors 
 convert input string to a double at position i in the respective vectors 
 evaluate the child expressions on the given input batch 
 compiles the appropriate vector expression writer based on an expression info exprnodedesc 
 compiles the appropriate vector expression writer based on an expression info exprnodedesc 
 compiles the appropriate vector expression writer based on an expression info exprnodedesc 
 compiles the appropriate vector expression writers based on a struct object inspector 
 helper function to create an array of writers from a list of expression descriptors 
 helper function to create an array of writers from a list of expression descriptors 
 creates the value writers for a column vector expression list creates an appropriate output object inspector 
 creates the value writers for a column vector expression list creates an appropriate output object inspector 
 creates the value writers for an struct object inspector creates an appropriate output object inspector 
 returns link vectorexpressionwriter objects for the fields in the givenobject inspector 
 generate join results for a single small table value match 
 generate results for a n x m cross product 
 generate optimized results for a large n x m cross product using repeated vectorized row batch optimization 
 generate optimized results when entire batch key is repeated and it matched the hash map 
 forward the overflow batch and reset the batch 
 on close make sure a partially filled overflow batch gets forwarded 
 generate the inner big table only join output results for one vectorized row batch 
 generate the left semi join output results for one vectorized row batch 
 generate the outer join output results for one vectorized row batch 
 generate the outer join output results for one vectorized row batch with a repeated key any filter expressions will apply now since hash map lookup for outer join is complete 
 we are processing a batch from reduce processor that is only for one reducer key or ptf group for a simple over partition by column or over order by column the reduce processor s group key is the partition or order by key for an over partition by column order by column the reduce shuffle group key is the combination of the partition column and the order by column in this case this method has to watch for changes in the partition and reset the group aggregations the reduce processor calls setnextvectorbatchgroupstatus beforehand to tell us whether the batch supplied to our process method is the last batch for the group key or not this helps us intelligently process the batch 
 builds the assigners from an object inspector and from a list of columns 
 store one row column value that is the current value in deserializeread 
 convert one row column value that is the current value in deserializeread we deserialize into a writable and then pass that writable to an instance of vectorassignrow to convert the writable to the target data type and assign it into the vectorizedrowbatch 
 deserialize a row from the range of bytes specified by setbytes use getdetailedreadpositionstring to get detailed read position information to help diagnose exceptions that are thrown this version of deserialize does not keep byte references to string char varchar binary data type field the bytes are copied into the bytescolumnvector buffer with setval see deserializebyref below if keep references is safe 
 deserialize a row from the range of bytes specified by setbytes use this method instead of deserialize when it is safe to retain references to the bytes source for deserializeread i e the string char varchar data can be set in bytescolumnvector with setref instead of with setval which copies data an example of a safe usage referring to bytes in a hash table entry that is immutable an example of a unsafe usage referring to bytes in a reduce receive buffer that will be overwritten with new data use getdetailedreadpositionstring to get detailed read position information to help diagnose exceptions that are thrown 
 locates the aggregation buffer sets to use for each key in the current batch the keywrappersbatch must have evaluated the current batch first 
 flushes the entries in the hash table by emiting output forward when parameter all is true all the entries are flushed 
 checks if the ht reduces the number of entries by at least minreductionhashaggr factor
 changes the processing mode to streaming this is done at the request of the hash agg mode if the number of keys exceeds the minreductionhashaggr factor
 emits a single row made from the key and the row aggregation buffers values kw is null if keyexpressions length is 
 emits a reduce group row made from the key copied in at the beginning of the group and the row aggregation buffers values
 returns a vector expression for a given expression description 
 add a cast expression to the expression tree if needed the output of child expressions of a given udf might need a cast if their return type is different from the return type of the udf 
 the genericudfs might need their children output to be cast to the given casttype this method returns a cast expression that would achieve the required casting 
 handles only the special cases of cast ve ve operator on a constant 
 used as a fast path for operations that don t modify their input like unary  and casting boolean to long identityexpression and its children are always projections 
 create a filter or boolean valued expression for column in 
 invoke special handling for expressions that can t be vectorized by regular descriptor based lookup 
 iterates thru all the columns in a given row and populates the batch from a given offset
 add only the projected column of a regular row to the specified vectorized row batch
 iterates thru all the columns in a given row and populates the batch from a given offset
 make a new scratch batch which is exactly like the batch provided except that it s empty
 forwards the row mode record into the vectorized output batch
 construct hivehistoryimpl object and open history log file 
 parses history file and calls call back functions also look at hivehistoryviewer
 get the location of the entity 
 analyzes a predicate 
 parse a bucket filename back into the options that would have created the file 
 this will look at a footer of one of the files in the delta to see if the file is in acid format i e has acid metadata columns the assumption is that for any dir either all files are acid or all are not 
 get the acid state of the given directory it finds the minimal set of base and diff directories note that because major compactions don t preserve the history we can t use a base directory that includes a write id that we must exclude 
 we can only use a base if it doesn t have an open txn from specific reader s point of view a base with open txn in its range doesn t have enough history info to produce a correct snapshot for this reader note that such base is not obsolete obsolete files are those that are covered by other files within the snapshot a base produced by insert overwrite is different logically it s a delta file but one that causes anything written previously is ignored hence the overwrite in this case base x is visible if writeid x is committed for current reader 
 find the original files non acid layout recursively under the partition directory 
 note this is generally called in hive java so the callers of hive java make sure to set up the acid state during compile and hive java retrieves it if needed 
 note this is generally called in hive java so the callers of hive java make sure to set up the acid state during compile and hive java retrieves it if needed 
 checks if the files in base delta dir are a result of load data statement and thus do not have row ids embedded in the data 
 this is smart enough to handle streaming ingest where there could be a link orcacidutils delta side file suffix side file 
 safety check to make sure a file take from one acid table is not added into another acid table since the row ids embedded as part a write to one table won t make sense in different table cluster 
 safety check to make sure the given location is not the location of acid table and all it s files will be not added into another acid table
 writable interface 
 writable interface 
 create hive splits based on combinefilesplit 
 gets all the path indices that should not be combined
 create hive splits based on combinefilesplit 
 flatfilerecordreader constructor constructs the underlying stream potentially decompressed and creates the deserializer 
 returns the next row and value 
 create the final out file and output row by row after one row is appended a configured row separator is appended
 this uses the name of the generic udf being used by the filter to determine whether we should perform a binary search and what the comparisons we should use to signal the end of the linear scan are 
 this should be called after the binary search is finished and before the linear scan begins
 checks if files are in same format as the given input format 
 create the final out file and output row by row after one row is appended a configured row separator is appended
 reads bytes from the source stream into the byte array buffer the number of bytes actually read is returned 
 read at most length bytes from this datainputstream and stores them in byte array buffer starting at offset answer the number of bytes actually read or  if no bytes were read and end of stream was encountered 
 reads a boolean from this stream 
 reads an  bit byte value from this stream 
 reads a  bit character value from this stream 
 reads a  bit double value from this stream 
 reads a  bit float value from this stream 
 reads bytes from this stream into the byte array buffer this method will block until buffer length number of bytes have been read 
 reads bytes from this stream and stores them in the byte array buffer starting at the position offset this method blocks until count bytes have been read 
 reads a  bit integer value from this stream 
 answers a string representing the next line of text available in this bufferedreader a line is represented by  or more characters followed by n r n r or end of stream the string does not include the newline sequence 
 reads a  bit long value from this stream 
 reads a  bit short value from this stream 
 reads an unsigned  bit byte value from this stream and returns it as an int 
 reads a  bit unsigned short value from this stream and returns it as an int 
 reads a utf format string from this stream 
 reads a utf format string from the datainput stream in 
 skips count number of bytes in this stream subsequent read s will not return these bytes unless reset is used 
 uncompresses part of the stream rgs can overlap so we cannot just go and decompress and remove what we have returned we will keep iterator as a hint point 
 subset of readencodedstream specific to compressed streams separate to avoid long methods 
 reads one compression block from the source handles compression blocks read from multiple ranges usually that would only happen with zcr adds stuff to cachedbuffers todecompress and torelease see below what each does 
 modifies the sarg replacing column names with column indexes in target table schema this basically does the same thing as all the shennannigans with included columns except for the last step where orc gets direct subtypes of root column and uses the ordered match to map table columns to file columns the numbers put into predicate leaf should allow to go into said subtypes directly by index to get the proper index in the file this won t work with schema evolution although it s probably much easier to reason about if schema evolution was to be supported because this is a clear boundary between table schema columns and all things orc none of the orc stuff is used here and none of the table schema stuff is used after that orc doesn t need a bunch of extra crap to apply the sarg thus modified 
 create an orc file writer this method is provided for api backward compatability with hive   
 divide the adjacent stripes in the file into input splits based on the block size and the configured minimum and maximum sizes 
 create a reader that reads from the first key larger than minkey to any keys equal to maxkey 
 find the key range for original bucket files for unbucketed tables the insert event data is still written to bucket n file except that n is just a writer id it still matches link recordidentifier getbucketproperty for original files ubucketed the same applies a file   encodes a taskid wirterid and at read time we synthesize link recordidentifier getbucketproperty to match the file nameand so the same bucketproperty is used here to create minkey maxkey i e these keys are valid to filter data from delete delta files even for unbucketed tables 
 create a reader that merge sorts the acid events together this handles  normal reads on behalf of a query non vectorized  compaction reads major minor  delete event reads to create a sorted view of all delete events for vectorized read this makes the logic in the constructor confusing and needs to be refactored liberal use of asserts below is primarily for documentation purposes 
 the insert event always uses link bucket that this link recordupdater was created withthus even for unbucketed tables the n in bucket n file name matches writerid bucketid even for late split
 there are  types of schema from the link basereader that this handles in the casethe data was written to a transactional table from the start every row is decorated with transaction related info and looks like  the other case is when data was written to non transactional table and thus only has the user data then this table was then converted to a transactional table but the data files are not changed until major compaction these are the original files in this case we may need to decorate the outgoing data with transactional column values at read time it s done somewhat out of band via vectorizedrowbatchctx ask teddy choi the owid writerid rowid columns represent link recordidentifier they are assigned each time the table is read in a way that needs to project link virtualcolumn rowid major compaction will attach these values to each row permanently it s critical that these generated column values are assigned exactly the same way by each read of the same row and by the compactor see link org apache hadoop hive ql txn compactor compactormr and link orcrawrecordmerger originalreaderpairtocompact for the compactor read path longer term should make compactor use this class this only decorates original rows with metadata if something above is requesting these values or if there are delete events to apply 
 this is not done quite right the intent of link compressedowid is a hedge against delete from t that generates a huge number of delete events possibly even g max array size assuming no one txn inserts  g rows in a bucket as implemented the algorithm first loads all data into one array owid and rowids which defeats the purpose in practice we should be filtering delete evens by min max row id from the split the later is also not yet implemented hive  
 build filter predicate with multiple constants
 create the parquet schema from the hive schema and return the recordwriterwrapper which contains the real output format
 method to convert string to utf  bytes 
 the function will set all data from parquet data page for an element in listcolumnvector
 advances to the next batch of rows returns false if there are no more 
 constructs a rcfile writer 
 append a row of values currently it only can accept size is less than the column number in the file zero bytes are appended for the empty columns if its size is greater then the column number in the file the exceeded columns bytes are ignored 
 flush a block out without doing anything except compressing the key part 
 create a new rcfile reader 
 seek to the next sync mark past a given position 
 fetch all data in the buffer for a given column this is useful for columnar operators which perform operations on an array data of one column it should be used together with link nextcolumnsbatch calling getcolumn with not change the result of link next longwritable and link getcurrentrow bytesrefarraywritable 
 read in next key buffer and throw any data in current key buffer and current value buffer it will influence the result of link next longwritable and link getcurrentrow bytesrefarraywritable 
 returns how many rows we fetched with next it only means how many rows are read by next the returned result may be smaller than actual number of rows passed by because link seek long link nextcolumnsbatch can change the underlying key buffer andvalue buffer 
 get the current row used make sure called link next longwritable first 
 parses all target paths from job input directory which contains symlink files and splits the target data using textinputformat 
 given list of directories containing symlink files read all target paths from symlink files and return as targetpaths list and for each targetpaths i symlinkpaths i will be the path to the symlink file containing the target path 
 create the final out file and output row by row after one row is appended a configured row separator is appended
 reads the next key value pair from the input for processing 
 throws ioexception
 how much of the input has the link recordreader consumed i e has been processed by 
 dispatch the current operator 
 starting point for walking 
 walk the current operator and its descendants 
 starting point for walking 
 enumerate numlevels of ancestors by putting them in the stack and dispatch the current node 
 walk the current operator and its descendants 
 walk the current operator and its descendants 
 this function returns the cost of the rule for the specified stack lower the cost the better the rule is matched
 starting point for walking 
 walk the current operator and its descendants 
 send a lock request to the metastore this is intended for use by link dbtxnmanager 
 we don t expect multiple threads to call this method concurrently but link lockmgr willbe read by a different threads than one writing it thus it s code volatile 
 ensures that the current sql statement is appropriate for the current state of the transaction manager e g can call commit unless you called start transaction note that support for multi statement txns is a work in progress so it s only supported in hiveconf hive in test hiveconf tez hive in test 
 normally client should call link acquirelocks org apache hadoop hive ql queryplan org apache hadoop hive ql context string 
 param delay time to delay for first heartbeat
 start the heartbeater threadpool and return the task 
 send a heartbeat to the metastore for locks and transactions 
 param lockobjects list of objects and the modes of the locks requested
 param hivelocks list of hive locks to be released release all the locks specified if some of the locks have already been released ignore them
 param key the object to be locked
 creates a primitive lock object on zookeeper 
 param conf hive configuration
 get the sequence number from the path the sequence number is always at the end of the path 
 write an error message 
 show a list of tables 
 show a list of materialized views 
 describe table 
 show the table partitions 
 show a list of databases
 show the description of a database
 write an error message 
 show a list of tables 
 show a list of materialized views 
 show the table partitions 
 show the list of databases
 describe a database
 gets hive object for the current thread if one is not initialized then a new one is created if the new configuration is different in metadata conf vars or the owner will be different then a new one is created 
 same as link get hiveconf except that it checks only the object identity of existing ms client assuming the relevant settings would be unchanged within the same conf object 
 same as link get hiveconf except that it checks only the object identity of existing ms client assuming the relevant settings would be unchanged within the same conf object 
 get a connection to metastore see get hiveconf function for comments
 hive
 create a database
 create a database raise an error if a database with the same name already exists 
 drop a database 
 drop a database
 drop a database
 creates a table metadata and the directory for the table data
 creates a table metadata and the directory for the table data
 create a table metadata and the directory for the table data
 updates the existing table metadata with the new metadata 
 updates the existing partition metadata with the new metadata 
 updates the existing partition metadata with the new metadata 
 updates the existing table metadata with the new metadata 
 rename a old partition to new partition
 creates the table with the give objects
 creates the table with the given objects it takes additional arguments for primary keys and foreign keys associated with the table 
 drops table along with the data in it if the table doesn t exist then it is a no op if ifpurge option is specified it is passed to the hdfs command that removes table data from warehouse to make it skip trash 
 drops table along with the data in it if the table doesn t exist then it is a no op
 drops table along with the data in it if the table doesn t exist then it is a no op
 drops the table 
 drops the table 
 truncates the table partition as per specifications just trash the data files
 returns metadata for the table named tablename
 returns metadata for the table named tablename
 returns metadata of the table
 returns metadata of the table
 returns metadata of the table
 get all table names for the current database 
 get all table names for the specified database 
 get all tables for the specified database 
 get all materialized view names for the specified database 
 get all materialized views for the specified database 
 get materialized views for the specified database that match the provided regex pattern 
 returns all existing tables from default database which match the given pattern the matching occurs as per java regular expressions
 returns all existing tables from the specified database which match the given pattern the matching occurs as per java regular expressions 
 returns all existing tables from the given database which match the given pattern the matching occurs as per java regular expressions
 returns all existing tables of a type virtual viewexternal tablemanaged table from the specified database which match the given pattern the matching occurs as per java regular expressions 
 get the materialized views that have been enabled for rewriting from the metastore if the materialized view is in the cache we do not need to parse it to generate a logical plan for the rewriting instead we return the version present in the cache further information provided by the invalidation cache is useful to know whether a materialized view can be used for rewriting or not 
 get materialized views for the specified database that have enabled rewriting 
 get all existing database names 
 get all existing databases that match the given pattern the matching occurs as per java regular expressions
 param privileges a bag of privileges
 query metadata to see if a database with the given name already exists 
 get the database by name 
 get the database by name 
 get the database object for current database
 load a directory into a hive table partition alters existing content of the partition with the contents of loadpath if the partition does not exist one is created files in loadpath are moved into hive but the directory itself is not removed 
 move all the files from loadpath into hive if the partition does not exist one is created files in loadpath are moved into hive but the directory itself is not removed 
 walk through sub directory tree to construct list bucketing location map 
 construct location map from path
 given a source directory name of the load path load all dynamically generated partitions into the specified table and return a list of strings that represent the dynamic partition paths 
 load a directory into a hive table alters existing content of table with the contents of loadpath if table does not exist an exception is thrown files in loadpath are moved into hive but the directory itself is not removed 
 creates a partition 
 returns partition metadata
 drop the partitions specified as directory names associated with the table 
 get all the partitions that the table has
 get all the partitions unlike link getpartitions table does not include auth 
 get all the partitions of the table that matches the given partial specification partition columns whose value is can be anything should be an empty string 
 get all the partitions of the table that matches the given partial specification partition columns whose value is can be anything should be an empty string 
 get all the partitions of the table that matches the given partial specification partition columns whose value is can be anything should be an empty string 
 get all partitions of the table that matches the list of given partition names 
 get a list of partitions by filter 
 get a list of partitions by expr 
 get a number of partitions by filter 
 get all existing role names 
 param objecttype hive object type
 param objecttype hive object type
 recycles the files recursively from the input path to the cmroot directory either by copying or moving it 
 copy files this handles building the mapping for buckets and such between the source and destination
 replaces files in the partition with new data set specified by srcf works by renaming directory of srcf to the destination file srcf destf and tmppath should resident in the same dfs but the oldpath can be in a different dfs 
 trashes or deletes all files under a directory leaves the directory as is 
 creates a metastore client currently it creates only jdbc based client as file based store support is removed
 return synchronized metastore client
 return the metastore client for the current thread
 return the metastore client for the current thread
 deprecated use link compact string string string string map 
 enqueue a compaction request only  compaction for a given resource db table partspec can be scheduled running at any given time 
 get all primary key columns associated with the table 
 get primary key columns associated with the table that are available for optimization 
 get all foreign keys associated with the table 
 get foreign keys associated with the table that are available for optimization 
 get all unique constraints associated with the table 
 get unique constraints associated with the table that are available for optimization 
 get all not null constraints associated with the table 
 get not null constraints associated with the table that are available for optimization 
 get not null constraints associated with the table that are enabled enforced 
 get check constraints associated with the table that are enabled
 get default constraints associated with the table that are enabled
 create a new instance of hiveauthorizationprovider
 if org apache hadoop util jarfinder is available   hadoop finds the jar for a class or creates it if it doesn t exist if the class is in a directory in the classpath it creates a jar on the fly with the contents of the directory and returns the path to that jar if a jar is created it is created in the system temporary directory otherwise returns an existing jar that contains a class of the same name maintains a mapping from jar contents to the tmp jar created 
 add entries to packagedclasses corresponding to class files contained in jar 
 find a jar that contains a class of the same name if any it will return a jar file even if that is not the first thing on the class path that has a class with the same name looks first on the classpath and then in the packagedclasses map 
 invoke getjar on a jarfinder implementation useful for some job configuration contexts hbase  and also for testing on mrv first check if we have hadoop  lacking that fall back to the backport 
 returns the full path to the jar containing the class it always return a jar 
 create an empty partition semanticanalyzer code requires that an empty partition when the table is not partitioned 
 create partition object with the given info 
 inheritdoc 
 inheritdoc 
 hive loadpartition calls this 
 loading dynamic partitions calls this the partitions which are loaded must belong to the same table 
 param partialpvals partition values can be partial this really means that missing valuesare represented by empty str 
 returns a list of partition names i e p  q  type strings the values rhs of   are escaped 
 partnames are like p  q  type strings the values rhs of   are escaped 
 splits out the equi join and optionally a single non equi components of a join condition and returns what s left projection might be required by the caller to provide join keys that are not direct field references 
 creates a relational expression which projects the output fields of a relational expression according to a partial mapping a partial mapping is weaker than a permutation every target has one source but a source may have   or more than one targets usually the result will have fewer fields than the source unless some source fields are projected multiple times this method could optimize the result as link permute does but doesnot at present 
 converts an aggregate relational expression that contains only count distinct to grouping sets with count for example select count distinct department id count distinct gender count distinct education level from employee can be transformed to select count case when i  and department id is not null then  else null end as c count case when i  and gender is not null then  else null end as c count case when i  and education level is not null then  else null end as c from select grouping id as i department id gender education level from employee group by department id gender education level grouping sets department id gender education level subq 
 gb rs gb construct gb rs gb pipe line user has enabled map side gb specified no skew and grp set is below the threshold 
 gb rs gb rs gb
 gb rs gb rs gb
 gb rs gb rs gb
 gb rs gb
 rs gb
 rs gb rs gb
 todo  issamplingpred  sampledesc  issortedfilter
 convert hive types to calcite types 
 if the input filter operator has direct child ren which are union operator and the filter s column is not the same as union s create select operator between them the select operator has same number of columns as pruned child operator 
 since we pruned the select operator we should let its children operator know that reducesinkoperator may send out every output columns of its parent select when the select operator is pruned its child reduce sink direct child operator should also be pruned 
 fold input expression desc 
 combines the logical not operator with the child operator if possible 
 fold input expression desc only performing short cutting unnecessary and or operations involving a constant true false value will be eliminated 
 fold input expression desc this function recursively checks if any subexpression of a specified expression can be evaluated to be constant and replaces such subexpression with the constant if the expression is a deterministic udf and all the subexpressions are constants the value will be calculated immediately during compilation time vs runtime e g concat year month    for year  month  since concat is deterministic udf unix timestamp time   unix timestamp  for time  since unix timestamp is nondeterministic udf
 can the udf be used for constant folding 
 obtain big table position for join 
 detect correlations and transform the query tree 
 this method is used to recursively traverse the tree to find reducesinkoperators which share the same key columns and partitioning columns those reducesinkoperators are called correlated reducesinkoperaotrs 
 start to exploit job flow correlation from op example here is the operator tree we have join rs rs gby join  rs rs rs the op will be rs if we can execute gby join and join in the same reducer this method will return rs rs rs 
 find all sibling reducesinkoperators which have the same child operator of op of op op included 
 find all sibling operators which have the same child operator of op of op op included 
 param newoperator the operator will be inserted between child and parent
 based on the correlation we transform the query plan tree operator tree in here we first create demuxoperator and all bottom reducesinkoperators bottom means near tablescanoperaotr in the correlation will be be the parents of the demuxoperaotr we also reassign tags to those reducesinkoperators then we use muxoperators to replace reducesinkoperators which are not bottom ones in this correlation example the original operator tree is join rs rs gby join  rs rs rs if gby join and join can be executed in the same reducer optimized by correlation optimizer the new operator tree will be join  mux gby join demux    rs rs rs
 current rsdedup remove replace child rs for key columns sorting order and the number of reducers copy more specific part of configurations of child rs to that of parent rs for partitioning columns if both child rs and parent rs have been assigned partitioning columns we will choose the more general partitioning columns if parent rs has not been assigned any partitioning column we will use partitioning columns if exist of child rs 
 this is a more strict version of the merge check where crs and prs should have exactly the same keys in the same positions and crs and prs should have exactly the same partition columns in the same positions and crs and prs should have exactly the same bucket columns in the same positions and crs and prs should sort in the same direction
 returns merge directions between two rss for criterias ordering number of reducers reducer keys partition keys returns null if any of categories is not mergeable values for each index can be      means two configuration in the category is the same  for  configuration of parent rs is more specific than child rs  for  configuration of child rs is more specific than parent rs
 overlapping part of keys should be the same between parent and child and if child has more keys than parent non overlapping part of keys should be backtrackable to parent 
 initialize the current plan by adding it to root tasks 
 initialize the current union plan 
 merge the current task into the old task for the reducer
 if currtopop is not set for input of the task add input for to the task
 met crs in prs parenttask crs op childtask case split and link two tasks by temporary file prs fs ts crs op
 met crs in pop parenttask with rs crs cop notask case create new child task for crs cop and link two tasks by temporary file pop fs ts crs cop
 set the current task in the mapredwork 
 set the current task in the mapredwork 
 param fsinput the filesink operator 
 file sink operator encountered 
 union operator encountered currently the algorithm is pretty simple if all the sub queries are map only don t do anything otherwise insert a filesink on top of all the sub queries this can be optimized later on 
 gets the expression dependencies for the expression 
 prunes to the directories which match the skewed keys in where clause algorithm           for each possible skewed element combination  walk through exprnode tree  decide boolean true false unknown null go through each skewed element combination again  if it is skewed value skip the directory only if it is false otherwise keep it  skip the default directory only if all skewed elements non skewed value are false example         for example  skewed column list c c  skewed value list of list  a  b  c unique skewed elements for each skewed column list of list   other a b c other index        output matches order of skewed column output can be read as c has unique element list   other c has unique element list a b c other c c  a  b  c other    a  x   c x   x   b  x x other  x  x  x x complete dynamic multi dimension collection    a  t    b  t    c  f    other  f    a  f    b  t    c  f    other  f   other a  t   other b  t   other c  t   other other  t is skewed value entry expression tree c  and c a or c  or c b or and or c  c a c  c b for each entry in dynamic multi dimension container  walk through the tree to decide value please see map s value above  if it is skewed value   remove the entry from the map   add directory to path unless value is false  otherwise add value to map once it is done go through the rest entries in map to decide default directory  we know all is not skewed value  we skip default directory only if all value is false what we choose at the end  directory for  a because it s skewed value and match returns true  directory for  b because it s skewed value and match returns true  default directory because not all non skewed value returns false we skip directory for  c since match returns false note unknown is marked in link transform parsecontext newcd   new exprnodeconstantdesc cd gettypeinfo null can be checked via child nd instanceof exprnodeconstantdesc exprnodeconstantdesc child nd getvalue    null 
 main skeleton for list bucketing pruning 
 find out complete skewed element collection for example  skewed column list c c  skewed value list of list  a  b  c it returns the complete collection  a  b  c  other  a  b  c  other other a other b other c other other 
 flat a dynamic multi dimension collection for example  skewed column list c c c  skewed value list of list  a x  b x  c x  a y unique skewed elements for each skewed column list of list   other a b c other index        complete dynamic multi dimension collection    a  t    b  t    c  f    other  f    a  f    b  t    c  f    other  f   other a  t   other b  t   other c  t   other other  t is skewed value entry
 flat the collection recursively 
  walk through the tree to decide value   true means the element matches the expression tree   false means the element doesn t match the expression tree   unknown means not sure if the element matches the expression tree example skewed column c c cell  a  b  c  other  a  b  c  other other a other b other c other other expression tree c  and c a or c  or c b or and or c  c a c  c b
 walk through expression tree recursively to evaluate 
 evaluate equal node 
 convert the join to a map join and also generate any local work needed 
 convert a regular join to a a map side join 
 transform the query tree for each join check if it is a map side join user specified if yes convert it to a map side join 
 recursively go through the children of the given link task and check if any child link sparktask containsthe specified link mapwork object 
 get the partition list for the ts operator that satisfies the partition pruner condition 
 get the partition list for the table that satisfies the partition pruner condition 
 pruning partition by getting the partition names first and pruning using hive expression evaluator on client 
 try to recursively add files in sub directories into retpathlist until reaching the sizeleft 
 try to generate a list of subset of files in the partition to reach a size limit with number of files less than filelimit
 annotate mapwork input is a smbjoinop that is part of a mapwork and its root ts operator  initializes the mapwork s aliastowork pointing to big table s ts  adds the bucketing information to the mapwork  adds localwork to the mapwork with localwork s aliastowork pointing to small table s ts 
 get the list of fieldschema out of the astnode 
 process the primary keys from the ast node and populate the sqlprimarykey list 
 process the unique constraints from the ast node and populate the sqluniqueconstraint list 
 get the constraint from the ast and populate the cstrinfos with the required information 
 validate and get the default value from the ast
 get the constraint from the ast and populate the cstrinfos with the required information 
 process the foreign keys from the ast and populate the foreign keys in the sqlforeignkey list
 get the list of fieldschema out of the astnode additionally populate the primarykeys and foreignkeys if any 
 checks if given specification is proper specification for prefix of partition cols for table partitioned by ds hr min valid ones are ds     ds     hr   ds     hr   min   invalid one is for example ds     min   
 handle skewed values in ddl it can be used by both skewed by on and set skewed location 
 this method is useful if we want to obtain the logical plan after being parsed and optimized by calcite 
 get optimized logical plan for the given qb tree in the semanalyzer 
 get optimized ast for the given qb tree in the semanalyzer 
 get optimized hive operator dag for the given qb tree in the semanalyzer 
 generate join logical plan relnode by walking through the join ast 
 generate gb plan 
 generate ob relnode and input select relnode that should be used to introduce top constraining project if input select relnode is not present then don t introduce top constraining select 
 param qb
 generate the statement of analyze table tablename compute statistics for columns in non partitioned table case it will generate ts sel gby rs gby sel fs operator in static partitioned table case it will generate ts fil partitionkey sel gby partitionkey rs gby sel fs operator in dynamic partitioned table case it will generate ts sel gby partitionkey rs gby sel fs operator however we do not need to specify the partition spec because  the data is going to be inserted to that specific partition  we can compose the static dynamic partition using a select operator in replaceselectoperatorprocess 
 generate the statement of select compute stats col compute stats col similar to the one generated from analyze table t compute statistics for columns but t is replaced by a table values cast null as int cast null as string as t col col we use table values statement for computing stats for ctas statement because in those cases the table has not been created yet once the plan for the select statement is generated we connect it to the existing ctas plan as we do for insert or insert overwrite 
 param ast is the original analyze ast
 a query like this will generate a tree as follows describe formatted default maptable partition b  id tok tabtype tok tabname  root for tablename  child nodes mean db specified default maptable tok partspec  root node for partition spec else columnname tok partval b  id  root node for columnname formatted
 add the task according to the parsed command tree this is used for the cli command show locks 
 add the task according to the parsed command tree this is used for the cli command lock table 
 add a task to execute abort transactions 
 add a task to execute kill query 
 add the task according to the parsed command tree this is used for the cli command unlock table 
 add one or more partitions to a table useful when the data has been copied to the right location by some other process 
 rewrite the metadata for one or more partitions in a table useful when an external process modifies files on hdfs and you want the pre post hooks to be fired for the specified partition 
 verify that the information in the metastore matches up with the data on the fs 
 get the partition specs from the tree 
 get the partition specs from the tree this stores the full specification with the comparator operator into the output list 
 add the table partitions to be modified in the output so that it is available for the pre execution hook if the partition does not exist no error is thrown 
 add the table partitions to be modified in the output so that it is available for the pre execution hook if the partition does not exist no error is thrown 
 analyze alter table s skewed table
 analyze alter table s skewed location
 initialize the uri where the exported data collection is to created for export or is present for import
 walk the given operator 
 starting point for walking 
 walk the given operator 
 the same code is used from both the repl load as well as import given that repl load now supports two modes repl load dbname location and repl load location in which case the database name has to be taken from the table metadata by default and then over ridden if something specified on the command line hence for import to work correctly we have to pass in the sessionstate default db via the parseddbname parameter
 helper method to set location properly in partspec
 create tasks for regular import no repl complexity
 create tasks for repl import
 listen for an event if it is a drop table event call export meta data
 parses the hive query 
 collect order expressions for range based windowing
 param parentqueryrr
 we have to get the replicationspec before we query for the function object from the hive metastore as the spec creation captures the latest event id for replication and we dont want to miss any events hence we are ok replaying some events as part of incremental load to achieve a consistent state of the warehouse 
 this writes the actual data in the exportrootdatadir from the source 
 this needs the root data directory to which the data needs to be exported to the data export here is a list of files either in table partition that are written to the files in the exportrootdatadir provided 
 this is currently referring to the export path for the data within a non partitioned table partition s data export directory is created within the export semantics of partition 
 retrieves the columninfo corresponding to a source expression which exactly matches the string rendering of the given astnode 
 adds column to rr checking for duplicate columns needed because cbo cannot handle the hive behavior of blindly overwriting old mapping in rr and still somehow working after that 
 return a new row resolver that is combination of left rr and right rr the schema will be schema of left schema of right
 this method creates a list of default constraints which corresponds to given schema targetschema or target table s column schema if targetschema is null 
 this method replaces astnode corresponding to default keyword with either default constraint expression if exists or null otherwise
 this method replaces default ast node with default expression
 dfs scan the expressiontree to find all aggregation subtrees and put them in aggregations 
 goes though the tabref tree and finds the alias for the table once found it records the table name  alias association in aliastotabs it also makes an association from the alias to the table ast in parse info 
 given the ast with tok join as the root get all the aliases for the tables or subqueries in the join 
 given the ast with tok lateral view as the root get the alias for the table or subquery in the lateral view and also make a mapping from the alias to all the lateral view ast s 
 phase  including but not limited to  gets all the aliases for all the tables subqueries and makes the appropriate mapping in aliastotabs aliastosubq  gets the location of the destination and names the clause inclause  i  creates a map from a string representation of an aggregation tree to the actual aggregation ast  creates a mapping from the clause name to the select expression ast in desttoselexpr  creates a mapping from a table alias to the lateral view ast s in aliastolateralviews
 gets the strongest encrypted table path 
 gets the staging directory where mr files will be stored temporary it walks through the qb plan to find the correct location where save temporary files this temporary location or staging directory may be created inside encrypted tables locations for security reasons if the qb has read only tables then the older scratch directory will be used or a permission error will be thrown if the requested query table is encrypted and the old scratch directory is not 
 parse the join condition for equality conjuncts break them into left and right expressions and store in the join tree for other conditions either add them to the post conditions if they apply to more than one input add them to the filter conditions of a given input if it applies only on one of them and should not be pushed e g left outer join with condition that applies only to left input or push them below the join if they apply only to one input and can be pushed e g left outer join with condition that applies only to right input 
 create a filter plan the condition and the inputs are specified 
 this modifies the select projections when the select is part of an insert statement and the insert statement specifies a column list for the target table e g create table source a int b int create table target x int y int z int insert into target z x select from source once the is resolved to a b this list needs to rewritten to b null a so that it looks as if the original query was written as insert into target select b null a from source if target schema is not specified this is no op
 generate the groupbyoperator for the query block parseinfo getxxx dest the new groupbyoperator will be a child of the reducesinkoperatorinfo 
 generate the groupbyoperator for the query block parseinfo getxxx dest the new groupbyoperator will be a child of the reducesinkoperatorinfo 
 generate the map side groupbyoperator for the query block qb getparseinfo getxxx dest the new groupbyoperator will be a child of the inputoperatorinfo 
 generate the reducesinkoperator for the group by query block qb getpartinfo getxxx dest the new reducesinkoperator will be a child of inputoperatorinfo it will put all group by keys and the distinct field if any in the map reduce sort key and all other fields in the map reduce value 
 generate the second reducesinkoperator for the group by plan parseinfo getxxx dest the new reducesinkoperator will be a child of groupbyoperatorinfo the second reducesinkoperator will put the group by keys in the map reduce sort key and put the partial aggregation results in the map reduce value 
 generate the second groupbyoperator for the group by plan parseinfo getxxx dest the new groupbyoperator will do the second aggregation based on the partial aggregation results 
 generate a group by plan using a single map reduce job  operators will be inserted reducesink keys   k exp k exp distinct exp values   a exp a exp sortgroupby keys   key  key  aggregations   count distinct key  sum value  count value  select final selects 
 generate a group by plan using a  map reduce jobs  operators will be inserted reducesink keys   k exp k exp distinct exp values   a exp a exp note if distinct exp is null partition by rand sortgroupby keys   key  key  aggregations   count distinct key  sum value  count value  reducesink keys     values     sortgroupby keys   key  key  aggregations   sum value  sum value  sum value  select final selects 
 generate a group by plan using  map reduce job first perform a map side partial aggregation to reduce the amount of data at this point of time we may turn off map side partial aggregation based on its performance then spray by the group by key and sort by the distinct key if any and compute aggregates based on actual aggregates the aggregation evaluation functions are as follows no grouping sets group by operator grouping keys group by expressions if no distinct grouping keys group by expressions  distinct keys if distinct mapper iterate terminatepartial mode   hash partitioning key grouping key sorting key grouping key if no distinct grouping  distinct key if distinct reducer iterate terminate if distinct merge terminate if no distinct mode mergepartial grouping sets group by operator grouping keys group by expressions  grouping id if no distinct grouping keys group by expressions  grouping id  distinct keys if distinct mapper iterate terminatepartial mode   hash partitioning key grouping key  grouping id sorting key grouping key  grouping id if no distinct grouping  grouping id  distinct key if distinct reducer iterate terminate if distinct merge terminate if no distinct mode mergepartial grouping sets with an additional mr job introduced distincts are not allowed group by operator grouping keys group by expressions mapper iterate terminatepartial mode   hash partitioning key grouping key sorting key grouping key reducer merge terminate mode mergepartial group by operator grouping keys group by expressions  add a new grouping id key stage  partitioning key grouping key  grouping id sorting key grouping key  grouping id reducer merge terminate mode   final group by operator grouping keys group by expressions  grouping id 
 generate a group by plan using a  map reduce jobs however only  group by plan is generated if the query involves no grouping key and no distincts in that case the plan is same as generated by gengroupbyplanmapaggrmr otherwise the following plan is generated first perform a map side partial aggregation to reduce the amount of data then spray by the grouping key and distinct key or a random number if no distinct is present in hope of getting a uniform distribution and compute partial aggregates grouped by the reduction key grouping key  distinct key evaluate partial aggregates first and spray by the grouping key to compute actual aggregates in the second phase the aggregation evaluation functions are as follows no grouping sets stage  group by operator grouping keys group by expressions if no distinct grouping keys group by expressions  distinct keys if distinct mapper iterate terminatepartial mode   hash partitioning key random if no distinct grouping  distinct key if distinct sorting key grouping key if no distinct grouping  distinct key if distinct reducer iterate terminatepartial if distinct merge terminatepartial if no distinct mode   mergepartial group by operator grouping keys group by expressions stage  partitioning key grouping key sorting key grouping key reducer merge terminate mode   final in the presence of grouping sets the aggregation evaluation functions are as follows stage  group by operator grouping keys group by expressions  grouping id if no distinct grouping keys group by expressions   grouping id  distinct keys if distinct mapper iterate terminatepartial mode   hash partitioning key random if no distinct grouping  grouping id  distinct key if distinct sorting key grouping key  grouping id if no distinct grouping  grouping id  distinct key if distinct reducer iterate terminatepartial if distinct merge terminatepartial if no distinct mode   mergepartial group by operator grouping keys group by expressions  grouping id stage  partitioning key grouping key sorting key grouping key  grouping id reducer merge terminate mode   final 
 construct a selection operator for semijoin that filter out all fields other than the group by keys 
 extract the filters from the join condition and push them on top of the source operators this procedure traverses the query tree recursively 
 extract the filters from the join condition and push them on top of the source operators this procedure traverses the query tree recursively 
 parses semjoin hints in the query and returns the table names mapped to filter size or  if not specified hints can be in  formats  tablename columnname target tablename bloom filter entries  tablename columnname target tablename
 generates the sampling predicate from the tablesample clause information this function uses the bucket column list to decide the expression inputs to the predicate hash function in case usebucketcols is set to true otherwise the expression list stored in the tablesample is used the bucket columns of the table are used to generate this predicate in case no expressions are provided on the tablesample clause and the table has clustering columns defined in it s metadata the predicate created has the following structure hash expressions integer max value denominator    numerator
 generates the operator dag needed to implement lateral views and attaches it to the ts operator 
 generates an expression node descriptor for the expression with typecheckctx 
 generates an expression node descriptors for the expression and children of it with default typecheckctx 
 returns expression node descriptor for the expression if it s evaluated already in previous operator it can be retrieved from cache 
 analyze the create table command if it is a regular create table or create table like statements we create a ddlwork and return true if it is a create table as select we get the necessary info such as the serde and storage format and put it in qb and return false indicating the rest of the semantic analyzer need to deal with the select statement with respect to the serde and storage format 
 process analyze noscan command
 validate noscan command
 generate the query string for this query with fully resolved table references 
 return table name in db table form with proper quoting escaping to be used in a sql statement
 starting point for walking 
 walk the given operator 
 todo need to turn on rules that s commented out and add more if necessary 
 returns true if the passed token was a storage format token and thus was processed accordingly 
 walk the given operator 
 interprets the given value as columndesc if possible
 exporting an acid table is more complicated than a flat table it may contains delete events which can only be interpreted properly withing the context of the table metastore where they were generated it may also contain insert events that belong to transactions that aborted where the same constraints apply in order to make the export artifact free of these constraints the export does a insert into tmptable select from to filter apply the events in current context and then export the tmptable this export artifact can now be imported into any table on any cluster subject to schema checks etc see link analyzeacidexport astnode 
 see link isacidexport astnode  create the temp table t  compile insert into t select from acidtable  compile export acidtable acidtable will be replaced with t during execution  create task to drop t using a true temp session level table means it should not affect replication and the table is not visible outside the session that created for security
 append list of partition columns to insert statement i e the nd set of partcol partcol insert into t partition partcol partcol select col partcol partcol 
 return the metastore representation of the target table
 this supports update and delete statements
 here we take a merge statement ast and generate a semantically equivalent multi insert statement to execute each insert leg represents a single when clause as much as possible the new sql statement is made to look like the input sql statement so that it s easier to map query compiler errors from generated sql to original one this way the generated sql is a complete representation of the original input for the same reason in many places semanticanalyzer throws exceptions that contain line position coordinates if generated sql doesn t have everything and is patched up later these coordinates point to the wrong place 
 per sql spec iso iec    e section   under general rules item  subitem a subitem  subitem b an error should be raised if   row of source matches the same row in target this should not affect the runtime of the query as it s running in parallel with other branches of the multi insert it won t actually write any data to merge tmp table since the cardinality violation udf throws an error whenever it s called killing the query
 param onclauseasstring because there is no clone and we need to use in multiple places
 param onclauseasstring because there is no clone and we need to use in multiple places
 returns the table name to use in the generated query preserving original quotes escapes if any
 generates the insert leg of the multi insert sql to represent when not matched then insert clause
 add default order spec if there is no order and validate order spec for valued based windowing since only one sort key is allowed 
 param tblname table name
 param oldname old name of the table
 param name name of the table
 param name name of the table
 convert expressions in current operator to those in terminal operator which is an ancestor of current or null back to top operator possibly contain null values for non traceable exprs
 checks whether the keys of a parent operator are a prefix of the keys of a child operator 
 checks whether the keys of a child operator are a prefix of the keys of a parent operator 
 param part partition
 create the reduce sink descriptor 
 return a deserializer object corresponding to the tabledesc 
 validate skewed table information 
 take current operators pushdown predicates and merges them with children s pushdown predicates 
 authorize command of given type and arguments
 authorize command of given type arguments and for service hosts for service type authorization 
 creates an encryption key using the parameters passed through the create key action 
 creates an encryption zone using the parameters passed through the create zone action 
 deletes an encryption key using the parameters passed through the delete key action 
 get an erasure coding policy for a path 
 enable an erasure coding policy 
 remove an erasure coding policy 
 disable an erasure coding policy 
 sets an erasure coding policy on a directory at the specified path 
 unsets an erasure coding policy on a directory at the specified path 
 convert thrift hiveobjectref to plugin hiveprivilegeobject
 check privileges on user and db this is used before doing a check on table partition objects first check the user and db privileges if it passed on this check no need to check against the table partition hive object 
 check privileges on user db and table objects 
 check privileges on user db and table partition objects 
 create thrift privileges bag
 get the privileges this user username argument has on the object hiveprivobject argument if isadmin is true adds an admin privilege as well 
 check if user is owner of the given object
 map permissions for this uri to sql standard privileges
 calls getvalidatedprincipal on each principal in list and updates the list
 re initialize currentrolenames if necessary 
 add role names of parentroles and its parents to processedrolesmap
 param rolename
 return true only if current role of user is admin
 authorization privileges against a path 
 checks the permissions for the given path and current user on hadoop fs if the given path does not exists it checks for its parent folder 
 read operation execution logs from log file
 create dirs session paths for this session  hdfs scratch dir  local scratch dir  local downloaded resource dir  hdfs session path  hold a lock file in hdfs session dir to indicate the it is in use  local session path  hdfs temp table space
 setup authentication and authorization plugins for this session 
 collect table partition and column level statistics
 find the bytes on disks occupied by list of partitions
 get basic stats of table
 get basic stats of partitions
 run compaction which may consist of several jobs on the cluster 
 get the partition being compacted 
 get the hostname that this worker is run on made static and public so that other classes can use the same method to know what host their worker threads are running on 
 this function will be called by groupbyoperator when it sees a new input row 
 this function will be called by groupbyoperator when it sees a new input row 
 initialize this genericudf additionally if the arguments are constant and the function is eligible to be folded then the constant value returned by this udf will be computed and stored in the constantobjectinspector returned otherwise the function behaves exactly like initialize 
 gets the udf class and checks it against the whitelist if any 
 update returnobjectinspector and valueinspectorsarethesame based on the objectinspector seen 
 update returnobjectinspector and valueinspectorsarethesame based on the objectinspector seen for unionall 
 update returnobjectinspector and valueinspectorsarethesame based on the objectinspector seen for comparison for example genericudfin 
 create a primitiveconversionhelper for method m the objectinspector s input parameters are specified in parameters 
 returns a local resource representing a jar this resource will be used to execute the plan on the cluster 
 returns the final top k n grams in a format suitable for returning to hive 
 adds a new n gram to the estimation 
 takes a serialized n gram estimator object created by the serialize method and merges it with the current n gram object 
 get the aggregation for the whole partition used in the case where windowing is unbounded or the function value is calculated based on all the rows in the partition such as percent rank 
 given the current row get the aggregation for the window
 check structure of arguments first arg should be a string then there should be an even number of arguments string expression expression should be convertible to boolean finally there should be a string convert pattern into a nnode chain convert symbol args into a symbol map parse selectlist into selectlist struct the inputoi used to translate these expressions should be based on the columns in the input the path attr 
 convert from string to a double this is called for cast as double 
 convert from string to a float this is called for cast as float 
 param uri
 param querystring
 recursively check the file owner and permissions setting them to the passed in values if the owner perms of the file do not match 
 recursively check the file owner and permissions setting them to the passed in values if the owner perms of the file do not match 
 param path
 todo this should accept a file of table names to exclude from non acid to acid conversion todo change script comments to a preamble instead of a footer
 assumes https issues apache org jira browse hive  is in how does this work with storage based auth 
 todo handle exclusion list figures out which tables to make acid mm and optionally performs the operation 
 creates a writer for the given filename with date as today 
 create a path for the given date and filename this can be used to create a reader 
 this cannnot be run against derby thus in ut but it can run againt mysql  add to metastore pom xml mysql mysql connector java     hack in the c tor of this class conf setvar hiveconf confvars metastoreconnecturlkey jdbc mysql localhost metastore conf setvar hiveconf confvars metastore connection user name hive conf setvar hiveconf confvars metastorepwd hive conf setvar hiveconf confvars metastore connection driver com mysql jdbc driver  remove txndbutil prepdb in txnhandler checkqfiletesthack 
 tests multi user scenario like hiveserver where each user gets a session and uses it multiple times 
 test hive  by default we force cloning of configurations for spark jobs
 sets up the environment and configurations
 tests the number of times hive droppartitions calls are executed with total number of partitions to be added are equally divisible by batch size 
 tests the number of times hive droppartitions calls are executed with total number of partitions to be added are not exactly divisible by batch size 
 tests the number of times hive droppartitions calls are executed with total number of partitions exactly equal to batch size 
 tests the number of times hive droppartitions calls are executed with total number of partitions to is less than batch size 
 tests the number of calls to droppartitions and the respective batch sizes when first call to droppartitions throws metastoreexception the batch size should be reduced once by the decayingfactor  iow after batch size is halved 
 tests the retries exhausted case when hive droppartitions method call always keep throwing metastoreexception the batch sizes should exponentially decreased based on the decaying factor and ultimately give up when it reaches  
 tests the maximum retry attempt is set to  
 tests when max number of retries is set to  
 test for link utilities getinputpaths jobconf mapwork path context boolean with a singlethreaded 
 test for link utilities getinputpaths jobconf mapwork path context boolean with multiplethreads 
 generates an orc file based on the provided record class in the specified file system at the output path 
 create the binary contents of an orc file that just has enough information to test the getinputsplits 
 test vectorization non acid non combine 
 test vectorization non acid non combine 
 test the orcrecordupdater with the orcrawrecordmerger when there is a base and a delta 
 test the orcrecordupdater with the orcrawrecordmerger when there is a base and a delta 
 test the recordreader when there is a new base and a delta this test creates multiple stripes in both base and delta files which affects how many splits are created on read with orc  this could be done in ee fashion with a query or streaming ingest writing data 
 test the recordreader when the delta has been flushed but not closed 
 tests methods of link nonsyncdatainputbuffer 
 tests link rcfile reader getcolumn int bytesrefarraywritable method 
 for debugging and testing 
 test combine symlink text input file two input dir and each contains one file and then create one symlink file containing these  files normally without combine it will return at least  splits
 test scenario two data directories one symlink file that contains two paths each point to a file in one of data directories 
 writes the given string to the given file 
 writes a symlink file that contains given list of paths 
 hive 
 check that locks in waiting state show what they are waiting on this test is somewhat abusive in that it make dblockmanager retain locks for  different queries which are not part of the same transaction which can never happen in real use cases but it makes testing convenient 
 normally the compaction process will clean up records in txn components completed txn components compaction queue and completed compactions but if a table partition has been dropped before compaction and there are still relevant records in those metastore tables the initiator will complain about not being able to find the table partition this method is to test and make sure we clean up relevant records as soon as a table partition is dropped note here we don t need to worry about cleaning up txns table since it s handled separately 
 collection of queries where we ensure that we get the locks that are expected
 check to make sure we acquire proper locks for queries involving acid and non acid tables
 txns update same resource but do not overlap in time no conflict
 txns overlap in time but do not update same resource no conflict
 txns overlap and update the same resource can t commit nd txn
 txns overlap update same resource simulate multi stmt txn case also tests that we kill txn when it tries to acquire lock if we already know it will not be committed
 overlapping txns updating the same resource but st one rolls back nd commits
 check that read query concurrent with txn works ok
  concurrent txns update different partitions of the same table and succeed
 concurrent updates with partition pruning predicate and w o one
 concurrent update delete of different partitions should pass
 concurrent update delete of same partition should fail to commit
 concurrent delete detele of same partition should pass
 todo multi insert into txn table and non tx table should be prevented
 param cc whether to cause a ww conflict or not
 run a merge statement using un partitioned target table and a concurrent op on the target check that proper locks are acquired and write conflict detection works and the state of internal table 
 check that dp with partial spec properly updates txn components
 run an update and merge concurrently check that correct locks are acquired check state of auxiliary acid tables 
 this test is mostly obsolete the logic in the driver java no longer acquires any locks for show tables keeping the test for now in case we change that logic 
 t is a table with  partitions  run select from t  run drop partition from t concurrently with  starting first so that  blocks  start another concurrent select on t it should block behind waiting x from drop lm should be fair  finish  so that drop unblocks  rollback the drop to release its x lock  should unblock
 see also link org apache hadoop hive ql testtxnaddpartition 
 tests create and fetch of a thrift based table 
 test for purge support for dropping partitions  drop partitions without purge and check that the data isn t moved to trash  drop partitions with purge and check that the data is moved to trash 
 test that tables set up with auto purge skip trash directory when tables partitions are dropped 
 tests single threaded implementation for deeply nested partitioned tables
 tests the case when the number of partition keys are more than the threadpool size 
 test if checker throws hiveexception when the there is a dummy directory present in the nested level of sub directories
 tests if there exists a unknown partition directory on the fs with in valid order of partition keys than what is specified in table specification 
 creates partition sub directories for a given table on the file system used to test the use cases when partitions for the table are not present in the metastore db
 creates a partition directory structure on file system but with a reverse order of sub directories compared to the partition keys defined in the table eg if the partition keys defined in table are a int b int c int this method will create an invalid directory c val  b val  a val 
 finds a free port 
 create ddlwork from given command string
 create role 
 drop role 
 grant on table to user 
 grant on table to role 
 grant on table to group 
 revoke on table from user 
 revoke on table from role 
 revoke on table from group 
 grant role to user 
 grant role to role 
 grant role to group 
 revoke role from user 
 revoke role from role 
 revoke role from group 
 show role grant user 
 show role grant role 
 show role grant group 
 show grant user on table 
 show grant role on table 
 show grant group on table 
 grant all on uri
 grant all on server
 check acceptable privileges in grant statement
 check acceptable privileges in grant statement
 check acceptable privileges in grant statement
 test if the authorization factory gets the username provided by the authenticator if sesstionstate is created without username
 test if the authorization factory gets the username set in the sessionstate constructor
 test that the groupnames are retrieved properly from ugi
 test that the groupnames returned is null when the user name is null the user name is null in the case of embedded hs and we assert that we don t throw an npe in that case 
 both update and insert
 we always expect  or  update delete when clause and  or  insert when clause and  or  when clauses altogether 
 here we reverse the order of when matched when not matched should we allow it 
 test if sqlstdhiveaccesscontroller is applying configuration security policy on hiveconf correctly
 return list of parameters that should be possible to set
 test that setting hive authorization sql std auth config whitelist append config works
 test that setting hive authorization sql std auth config whitelist config works
 param path
 test that checks that the queries above the quota are timed out so the compilation quota maximum is honored 
 useful for debugging dumps orc file in json to cwd 
 dump all data in the table by bucket in json format
 this is run  times  with ppd on  with off also the queries are such that if we were to push predicate down to an update delete delta the test would produce wrong results
 see hive  see also link testtxncommands testnonacidtoacidconversion link testtxnnobuckets testctas 
 in current implementation of acid altering the value of transactional properties or trying to set a value for previously unset value for an acid table will throw an exception 
 test the query correctness and directory layout for acid table conversion  insert a row to non acid table  convert non acid to acid table  insert a row to acid table  perform major compaction  clean
 test the query correctness and directory layout for acid table conversion  insert a row to non acid table  convert non acid to acid table  update the existing row in acid table  perform major compaction  clean
 test the query correctness and directory layout for acid table conversion  insert a row to non acid table  convert non acid to acid table  perform major compaction  insert a new row to acid table  perform another major compaction  clean
 test update that hits multiple partitions i e requries dynamic partition insert to process 
 https issues apache org jira browse hive 
 https issues apache org jira browse hive 
 hive 
 make sure there s no filesystemcachekey leak due to ugi use
 hive  has details
 simulate the scenario when a heartbeat failed due to client errors such as no locks or no txns being found when a heartbeat fails the query should be failed too 
 make sure aborted txns don t red flag a base xxxx hive  
 test that acid works with multi insert statement
 this tests that we handle non trivial on clause correctly
 test combines update  insert clauses
 test combines delete  insert clauses
 investigating dp and writeentity etc
 using nested partitions and thus dummypartition
 test the scenario when iow comes in before a major compaction happens
 test the scenario when iow comes in after a major compaction happens
 test compaction for micro managed table  regular compaction shouldn t impact any valid subdirectories of mm tables  compactions will only remove subdirectories for aborted transactions of mm tables if any
 test cleaner for txn to write id table 
 this tests that delete delta x y dirs will be not produced during minor compaction if no input delete events see hive  
 test that rows are routed to proper files based on bucket col row id only the vectorized acid reader checks if bucketid in row id inside the file matches the file name and only for files in delete delta
 test compaction for micro managed table  regular compaction shouldn t impact any valid subdirectories of mm tables  compactions will only remove subdirectories for aborted transactions of mm tables if any
 simplest export test 
 tests import where target table already exists 
 tests import where target table already exists 
 tests import where target table does not exists 
 mm tables already work mm exim q export creates a bunch of metadata in addition to data including all table props if of etc import from export can create a table any name specified or add data into existing table if importing into existing table un partitioned it must be empty if import is creating a table it will be exactly like exported one except for the name 
 load data overwrite in to an un partitioned acid converted table
 make sure load data assigns row ids correctly when there is statementid suffix on delta dir for example delta x x  
 this method drives the test it takes the data from getbasetable and feeds it through a select operator with a collect operator after each row that is produced by the collect operator is compared to getexpectedresult and if every row is the expected result the method completes without asserting 
 runs vectorized explain on the query and checks if the plan is vectorized as expected
 we have to use a different query to check results for vectorized tests because to get the file name info we need to use link org apache hadoop hive ql metadata virtualcolumn filename which will currently make the query non vectorizable this means we can t check the file name for vectorized version of the test 
 test empty link reusablestringreader 
 param basedir
 throws ioexception
 return clientport return clientport if there is another zk backup can runwhen killing the current active return  if there is no backups 
 kill one back up zk servers
 initialize the serde by default this will use one set of properties either the table properties or the partition properties if a serde needs access to both sets it should override this method eventually once all serdes have implemented this method we should convert it to an abstract method 
 deserialize an avro record recursing into its component fields and deserializing them as well fields of the record are matched by name against fields in the hive row because avro has some data types that hive does not these are converted during deserialization to types hive will work with 
 determine the schema to that s been provided for avro serde work 
 retrieve or create if it doesn t exist the correct instance for this seedobject
 retrieve or create if it doesn t exist the correct instance for this seedobject using seenschemas to resolve circular references
 generate a list of of typeinfos from an avro schema this method is currently public due to some weirdness in deserializing unions but will be made private once that is resolved 
 generate a list of of typeinfos from an avro schema this method is currently public due to some weirdness in deserializing unions but will be made private once that is resolved 
 generate a typeinfo for an avro map this is made slightly simpler in that avro only allows maps with strings for keys 
 inheritdoc 
 inheritdoc 
 returns a copy of the underlying bytes referenced by this instance 
 returns the underlying bytes 
 always reuse the bytes array if length of bytes array is equal or greater to the current record otherwise create a new one readfields will corrupt the array please use set whenever possible 
 inheritdoc 
 inheritdoc 
 given an object and object inspector pair traverse the object and generate a text representation of the object 
 writes a date in sql date format to the output stream 
 create a hierarchical objectinspector for lazyobject with the given typeinfo 
 create a hierarchical objectinspector for lazyobject with the given typeinfo 
 create a hierarchical objectinspector for lazyobject with the given typeinfo 
 create a hierarchical objectinspector for lazyobject with the given typeinfo 
 create a hierarchical objectinspector for lazyobject with the given typeinfo 
 create a hierarchical objectinspector for lazystruct with the given columnnames and columntypeinfos 
 create a hierarchical objectinspector for lazystruct with the given columnnames and columntypeinfos 
 create a hierarchical objectinspector for lazystruct with the given columnnames and columntypeinfos 
 create a hierarchical objectinspector for columnarstruct with the given columnnames and columntypeinfos 
 create a hierarchical objectinspector for columnarstruct with the given columnnames and columntypeinfos 
 writes hivedecimal object to output stream as string
 writes hivedecimal object to output stream as string
 writes hivedecimalwritable object to output stream as string
 writes out the text representation of an integer using base  to an outputstream in utf  encoding note division by a constant like  is much faster than division by a variable that s one of the reasons that we don t make radix a parameter here 
 writes out the text representation of an integer using base  to an outputstream in utf  encoding note division by a constant like  is much faster than division by a variable that s one of the reasons that we don t make radix a parameter here 
 serialize a row of data 
 serialize the row into the stringbuilder 
 this method is deprecated and is only used for backward compatibility replaced by see org apache hadoop hive serde lazy lazyserdeparameters lazyserdeparameters configuration properties string 
 initilizes lazytimestamp object by interpreting the input bytes as a jdbc timestamp string
 writes a timestamp in jdbc timestamp format to the output stream
 write the bytes with special characters escaped 
 write out the text representation of a primitive object to a utf byte stream 
 write out a binary representation of a primitiveobject to a byte stream 
 return the byte value of the number string 
 serialize an object to a byte buffer in a binary compact way 
 serialize a struct object without writing the byte size this function is shared by both row serialization and struct serialization 
 a recursive function that serialize an object to a byte buffer based on its object inspector 
 get the integer value out of a primitive object note that nullpointerexception will be thrown if o is null note that numberformatexception will be thrown if o is not a valid number 
 get the long value out of a primitive object note that nullpointerexception will be thrown if o is null note that numberformatexception will be thrown if o is not a valid number 
 get the double value out of a primitive object note that nullpointerexception will be thrown if o is null note that numberformatexception will be thrown if o is not a valid number 
 returns generic type for a field in a thrift class the type is the return type for the accessor method for the field e g isfieldname for a boolean type or getfieldname for other types the return type works for both structs and unions reflecting directly based on fields does not work for unions 
 this method is only intended to be used by utilities class in this package the reason that this method is not recursive by itself is because we want to allow recursive types 
 read varchar n the representation of varchar in teradata binary format is the first two bytes represent the length n of this varchar field the next n bytes represent the content of this varchar field to pad the null varchar the length will be  and the content will be none 
 read timestamp p the representation of timestamp in teradata binary format is the byte number to read is based on the precision of timestamp each byte represents one char and the timestamp is using string representation eg for timestamp  we need to read  bytes     d   d      a   a   e       will represent        the null timestamp will use space to pad 
 read date the representation of date in teradata binary format is the date d is a int with  bytes using little endian the representation is d tostring  yyyymmdd eg date  b     in little endian       the null date will use  to pad 
 read char n the representation of char in teradata binary format is the byte number to read is based on the charlength byteperchar  totallength byteperchar is decided by the charset latain charset is  bytes per char and unicode charset is  bytes per char the null char will use space to pad 
 write timestamp n the representation of timestamp in teradata binary format is the byte number to read is based on the precision of timestamp each byte represents one char and the timestamp is using string representation eg for        in timestamp  we will cut it to be        and write     d   d      a   a   e    the null timestamp will use space to pad 
 write char n the representation of char in teradata binary format is the byte number to read is based on the charlength byteperchar  totallength byteperchar is decided by the charset latain charset is  bytes per char and unicode charset is  bytes per char the null char will use space to pad 
 initialize the hiveserializer 
 serialize an object by navigating inside the object with the objectinspector in most cases the return value of this function will be constant since the function will reuse the writable object if the client wants to keep a copy of the writable the client needs to clone the returned value 
 deserialize an object out of a writable blob in most cases the return value of this function will be constant since the function will reuse the returned object if the client wants to keep a copy of the object the client needs to clone the returned value by calling objectinspectorutils getstandardobject 
 this method always return the same instance of tmap to avoid creating new instances it is the responsibility of the caller to read the value before calling this method again 
 this method always return the same instance of tlist to avoid creating new instances it is the responsibility of the caller to read the value before calling this method again 
 this method always return the same instance of tset to avoid creating new instances it is the responsibility of the caller to read the value before calling this method again 
 initialize the tprotocol 
 write tcolumn objects to the underlying stream of tprotocol
 create primitivetypeinfo instance for the given full name of the type the returned type is one of the parameterized type info such as varchartypeinfo 
 test serializing and deserializing timestamps that can be represented by a number of seconds from  to  since the unix epoch 
 test timestamps that don t necessarily fit between  and  this depends on hive  being fixed 
 test for escaping 
 verify the serialized format for given type dtype when it is nested in an array with nestinglevel levels with extended nesting enabled 
 tests the deprecated usage of serdeparameters 
 return stringified base encoded kerberosauthheader on success
 closes this search object and releases any system resources associated with it if the search object is already closed then invoking this method has no effect 
 inheritdoc 
 inheritdoc 
 returns all entries from the search result 
 checks whether search result contains exactly one entry 
 returns a single entry from the search result throws code namingexception if the search result doesn t contain exactly one entry 
 returns all entries and all attributes for these entries 
 allows for custom processing of the search results 
 reads the temporary results for non hive non driver commands to the resulting list of strings 
 wait until the operation terminates and returns false if timeout 
 verify if the given fetch orientation is part of the default orientation types 
 cancel the running operation unless it is already in a terminal state
 cancel the running operation unless it is already in a terminal state
 compile the query and extract metadata
 close the file systems for the session and remove it from the filesystem cache cancel the session s delegation token and close the metastore connection
 opens a new session and creates a session handle the username passed to this method is the effective username if withimpersonation is true   doas true we wrap all the calls in hivesession within a ugi doas where ugi corresponds to the effective user 
 create a delegation token object for the given token string and service add the token to given ugi
 add a given service to delegation token string 
 returns the effective username  if hive server allow user substitution   false the username of the connecting user  if hive server allow user substitution   true the username of the end user that the connecting user is trying to proxy for this includes a check whether the connecting user is allowed to proxy for the end user 
 create a session handle
 do the ldap pam authentication
 get the signature of the input string based on sha digest algorithm 
 remove all znodes corresponding to the given version number from zookeeper
 close the closeable objects and ignore any link ioexception ornull pointers must only be used for cleanup in exception handlers 
 test async execution of a well formed and a malformed query with different long polling durations test malformed query with default long polling timeout test well formed query with default long polling timeout test well formed query with long polling timeout set to  test well formed query with long polling timeout set to  millis test well formed query cancellation
 sets up a test specific table with the given column definitions and config
 create session and fetch the property set in global init file test if the global init file hiverc is loaded correctly by checking the expected setting property 
 tests metrics regarding async thread pool the test does the following steps submit four tasks wait with the metrics verification until the first two tasks are running if for some reason the tasks are not started within a timeout period make the test fail make the tasks wait until the metrics are checked verify the metrics both the exec async pool size and exec async queue size should be  let the first two tasks complete so the remaining two tasks can be removed from the queue and started wait until the remaining tasks are running do the metrics check only if they are started to avoid the failures when the queue size was not  if for some reason the tasks are not started within a timeout period make the test fail verify the metrics the exec async pool size should be  and the exec async queue size should be  let the remaining tasks complete 
 throws java lang exception
 test asynchronous query execution and error reporting to the client
 throws java lang exception
 throws java lang exception
 returns a shim to wrap minimrcluster
 returns a shim to wrap minimrtez
 returns a shim to wrap minisparkonyarncluster
 proxy file system also needs to override the access method behavior cannot add override annotation since filesystem access may not exist in the version of hadoop used to build hive 
 compares two encryption key strengths 
 return true if the runtime version of hdfs supports erasure coding
 returns a new instance of the hdfserasurecoding shim 
 return true if if the runtime mr stat for erasure coding is available 
 grab a handle to a job that is already known to the jobtracker 
 grab a handle to a job that is already known to the jobtracker 
 kill a job 
 param p
 calculate md checksum from data in fsdatainputstream
 a generic recordreader that can hand out different recordreaders for each chunk in the combinefilesplit 
 return the major version of hadoop currently on the classpath releases in the  x and  x series are mapped to the appropriate  x release series e g  x is mapped to  s and  x is mapped to   
 creates an rpc client for a server running on the given remote host and port 
 parses the port string like    into the port list a default  is added for the empty port string 
 this function converts an application in form of a string into a applicationid 
 create a url from a string representing a path to a local file the path string can be just a path or can start with file file 
 tests that we don t timeout with a short timeout but the spark application isn t running 
 creates a client connection between the server and a client 
 this was added to avoid a race condition where we try to create multiple sparkcontexts in the same process since spark master   local everything is run in the same jvm since we don t wait for the remotedriver to shutdown it s sparkcontext its possible that we finish a test before the sparkcontext has been shutdown in order to avoid the multiple sparkcontexts in a single jvm exception we wait for the sparkcontext to shutdown after each test 
 adds a server instance to zookeeper as a znode 
 this method is supposed to be called from client code connecting to one of the servers managed by the configured zookeeper it starts and closes its own zookeeper client instead of using the class member 
 instantiate the metastore server handler directly instead of connecting through the network
 param new part
 exchange the partition between two tables
 exchange the partitions between two tables
 param tbl
 param name
 handles dropdatabase by invoking drop table in hms for each table useful when table list in db is too large to fit in memory it will retrieve tables in chunks and for each table with a drop table hook it will invoke drop table on both hms and the hook this is a timely operation so hookless tables are skipped and will be dropped on server side when the client invokes drop database note that this is less transactional than dropdatabasecascadeperdb since we re dropping table level objects so the overall outcome of this method might result in a halfly dropped db 
 handles dropdatabase by invoking drop database in hms useful when table list in db can fit in memory it will retrieve all tables at once and call drop database once also handles drop table hooks 
 drop the table and choose whether to delete the underlying table data throw if the table doesn t exist save the data in the trash 
 inheritdoc 
 drop a table equivalent to link droptable string string string boolean boolean boolean with ifpurge set tofalse 
 drop a table equivalent to link droptable string string string boolean boolean boolean with deletedataset and ignoreunknowntable set to true and ifpurge set to false 
 alter a table equivalent to link alter table string string string table environmentcontext withenvironmentcontext set to null 
 drop a database equivalent to link dropdatabase string string boolean boolean boolean with cascade   false 
 drop a database equivalent to link dropdatabase string string boolean boolean boolean with deletedata  true ignoreunknowndb   false cascade   false 
 drop partitions based on an expression 
 drop partitions based on an expression 
 updates a partition to new partition
 updates a list of partitions
 get the constructed lockrequest 
 move a path into cmroot if the path is a directory of a partition or table if nonpartitioned recursively move files inside directory to cmroot note the table must be managed table
 get original file specified by src and chksumstring if the file exists and checksum matches return the file otherwise use chksumstring to retrieve it from cmroot
 this constructor is meant for hive internal use only please use getproxy hiveconf conf hivemetahookloader hookloader for external purpose 
 this constructor is meant for hive internal use only please use getproxy hiveconf conf hivemetahookloader hookloader for external purpose 
 verify token string
 method to get canonical ized hostname given a hostname possibly a cname this should allow for service principals to use simplified cnames 
 create a ttransportfactory that upon connection of a client socket negotiates a kerberized sasl transport the resulting ttransportfactory can be passed as both the input and output transport factory when instantiating a tthreadpoolserver for example 
 inheritdoc 
 read and return hadoop sasl configuration which can be configured using hadoop rpc protection 
 copies files between filesystems 
 rename a file unlike link filesystem rename path path if the destpath already exists and is a directory this will not move the sourcepath into it it will throw an ioexception instead 
 get all file status from a root path and recursively go deep into certain levels 
 recursively lists status for all files starting from a particular directory or individual file as base case 
 check the permissions on a file 
 create an object of the given class 
 create an object of the given class using a no args constructor
 initialize logj 
 create a url from a string representing a path to a local file the path string can be just a path or can start with file file 
 create a delegation token object for the given token string and service add the token to given ugi
 return the user name set in hadoop job ugi param or the current user from system
 hadoop file system reverse lookups paths with raw ip addresses the file system uri always contains the canonical dns name of the namenode subsequently operations on paths with raw ip addresses cause an exception since they don t match the file system uri this routine solves this problem by replacing the scheme and authority of a path with the scheme and authority of the filesystem that it maps to 
 resolve the configured warehouse root dir with respect to the configuration this involves opening the filesystem corresponding to the warehouse root dir but that should be ok given that this is only called during ddl statements for non external tables 
 build the database path based on catalog name and database name this should only be used when a database is being created or altered if you just want to find out the path a database is already using call link getdatabasepath database if the passed in database already has a path set that will be used if not the location will be built using catalog s path and the database name 
 get the path specified by the database in the case of the default database the root of the warehouse is returned 
 returns the default location of the table path using the parent database s location
 given a partition specification return the path corresponding to the partition spec by default the specification does not include dynamic partitions 
 returns the default partition path of a table within a given database and partition key value pairs it uses the database location and appends it the table name and the partition key value pairs to create the path for the partition directory
 returns the path object for the given partition key value pairs and the base location
 given a database a table and the partition key value pairs this method returns the path object corresponding to the partition key value pairs it uses the table location if available else uses the database location for constructing the path corresponding to the partition key value pairs
 param desc
 param db database
 hyperloglog is serialized using the following format   byte  varlong varint optional    header  estimated count  register length  register   byte header is encoded like below  bytes hll magic string to identify serialized stream  bits p number of bits to be used as register index  spare bit not used  bits encoding  sparse   n bit packing  no bit packing followed by header are  fields that are required for reconstruction of hyperloglog estimated count variable length long to store last computed estimated count this is just for quick lookup without deserializing registers register length number of entries in the register required only for for sparse representation for bit packing the register length can be found from p 
 refer serializehll for format of serialization this function deserializes the serialized hyperloglogs
 this function deserializes the serialized hyperloglogs from a byte array 
 unpack the bitpacked hyperloglog register 
 get estimated cardinality without deserializing hll
 check if the specified input stream is actually a hll stream
 write variable length encoded longs to output stream
 read variable length encoded longs from input stream
 return aggregate stats for a column from the cache or null while reading from the nodelist for a key we wait maxreaderwaittime to acquire the lock failing which we return a cache miss i e null 
 add a new node to the cache may trigger the cleaner thread if the cache is near full capacity we ll however add the node even if we temporaily exceed maxcachenodes because the cleaner will eventually create space from expired nodes or by removing lru nodes 
 cleans the expired nodes or removes lru nodes of the cache until the cache size reduces to cleanuntil full 
 evict an lru node or expired node whichever we find first
 deprecated as of release    replaced by link altertable rawstore warehouse string string string table environmentcontext ihmshandler handles alter table the changes could be cascaded to partitions if applicable
 build the catalog object and create it in the metastore 
 build the database create it in the metastore and then return the db object 
 create the function object in the metastore and return it 
 check whether the long running method timeout 
 internal function to notify listeners to revert back to old values of keys that were modified during setmetaconf this would get called from hivemetastore cleanuprawstore
 get a cached rawstore 
 create default database if it doesn t exist this is a potential contention when hiveserver using embedded metastore and metastore server try to concurrently invoke createdefaultdb if one failed jdoexception was caught for one more time try if failed again simply ignored by warning which meant another succeeds 
 create default roles if they don t exist this is a potential contention when hiveserver using embedded metastore and metastore server try to concurrently invoke createdefaultroles if one failed jdoexception was caught for one more time try if failed again simply ignored by warning which meant another succeeds 
 add admin users if they don t exist this is a potential contention when hiveserver using embedded metastore and metastore server try to concurrently invoke addadminusers if one failed jdoexception was caught for one more time try if failed again simply ignored by warning which meant another succeeds 
 gets multiple tables from the hive metastore 
 validate a partition before creating it the validation checks if the database and table names set in the partition are not null and they are matching with the expected values set in the tblname and dbname parameters if the partition values are set if none of the partition values is null if the partition values are matching with the pattern set in the metastore partition name whitelist pattern configuration property if the partition doesn t already exist if the partition already exists an exception will be thrown if the ifnotexists parameter is false otherwise it will be just ignored if the partstoadd set doesn t contain the partition the partstoadd set contains the partitions which are already validated if the set contains the current partition it means that the partition is tried to be added multiple times in the same batch please note that the set will be updated with the current partition if the validation was successful 
 create the location folders for the partitions for each partition a separate thread will be started to create the folder the method will wait until all threads are finished and returns the partitions whose folders were created successfully if an error occurs during the execution of a thread a metaexception will be thrown 
 fire a pre event for read table operation if there are any pre event listeners registered
 return the schema of the table this function includes partition columns in addition to the regular columns 
 return the schema of the table this function includes partition columns in addition to the regular columns 
 check if newrole is in parent hierarchy of currole
 create retrying hms handler for embedded metastore important this method is called indirectly by hivemetastoreclient and hivemetastoreclientprecatalog using reflection it can not be removed and its arguments can t be changed without matching change in hivemetastoreclient and hivemetastoreclientprecatalog 
 start metastore based on a passed link hadoopthriftauthbridge 
 param port where metastore server is running
 start threads outside of the thrift service such as the compactor threads 
 return hostname without throwing exception 
 check the metastore for inconsistencies data missing in either the metastore or on the dfs 
 check the metastore for inconsistencies data missing in either the metastore or on the dfs 
 check the metastore for inconsistencies data missing in either the metastore or on the dfs 
 find partitions on the fs that are unknown to the metastore 
 assume that depth is  i e partition columns are a and b tblpath a    throw exception tblpath a  file   throw exception tblpath a  b  file   return a  b  tblpath a  b  c    return a  b  tblpath a  b  c  file   return a  b 
 get table names by using direct sql queries 
 get table names by using direct sql queries 
 gets partitions by using direct sql queries 
 gets partitions by using direct sql queries 
 this method can be used to return partially filled partitions when clients are only interested in some fields of the partition objects the partitionfields parameter is a list of dot separated partition field names for example if a client is interested in only partition location serializationlib values and parameters it can specify sd location sd serdeinfo serializationlib values parameters in the partitionfields list in such a case all the returned partitions will have only the requested fields set and the rest of the fields will remain unset the implementation of this method runs queries only for the fields which are requested and pushes down the projection to the database to improve performance 
 gets all partitions of a table by using direct sql queries 
 get partition ids for the query using direct sql queries to avoid bazillion queries created by dn retrieving stuff for each object individually 
 should be called with the list short enough to not trip up oracle etc 
 retrieve the column statistics for the specified columns of the table null is returned if the columns are not provided 
 should be called with the list short enough to not trip up oracle etc 
 this run the necessary logic to prepare for queries it should be called once after the txn on datanucleus connection is opened and before any queries are issued what it does currently is run db specific logic e g setting ansi quotes mode for mysql the reason it must be used inside of the txn is connection pooling there s no way to guarantee that the effect will apply to the connection that is executing the queries otherwise 
 drop partitions by using direct sql queries 
 drops partition s should be called with the list short enough to not trip up oracle etc 
 drops sd s should be called with the list short enough to not trip up oracle etc 
 drops serde s should be called with the list short enough to not trip up oracle etc 
 checks if the column descriptors still has references for other sd s if not then removes them should be called with the list short enough to not trip up oracle etc 
 merges applies the result of a pm sql query into a tree of object essentially it s an object join dn could do this for us but it issues queries separately for every object which is suboptimal 
 updates the connection url in hiveconf using the hook if a hook has been set using hive metastore ds connection url hook property 
 notify a list of listeners about a specific metastore event each listener notified might update the listenerevent event by setting a parameter key value pair these updated parameters will be returned to the caller 
 notify a list of listeners about a specific metastore event each listener notified might update the listenerevent event by setting a parameter key value pair these updated parameters will be returned to the caller sometimes these events are run inside a db transaction and might cause issues with the listeners for instance sentry blocks the hms until an event is seen committed on the db to notify the listener about this a new parameter to verify if a transaction is active is added to the listenerevent and is up to the listener to skip this notification if so 
 get the list of sql scripts required to upgrade from the give version to current
 metastorecheck see if the data in the metastore matches what is on the dfs current version checks for tables and partitions that are either missing on disk on in the metastore 
 write the result of msck to a writer 
 alter the database object in metastore currently only the parameters of the database or the owner can be changed 
 converts a storage descriptor to a db backed storage descriptor creates a new db backed column descriptor object for this sd 
 converts a storage descriptor to a db backed storage descriptor it points the storage descriptor s column descriptor to the one passed as an argument so it does not create a new mcolumn descriptor object 
 getting mpartition object use this method only if the partition name is not available since then the table will be queried to get the partition keys 
 convert a partition object into an mpartition which is an object backed by the db if the partition s set of columns is the same as the parent table s and usetablecd is true then this partition s storage descriptor s column descriptor will point to the same one as the table s storage descriptor 
 drop an mpartition and cascade deletes e g delete partition privilege grants drop the storage descriptor cleanly etc 
 retrieves a collection of partition related results from the database that match the partial specification given for a specific table 
 gets partition names from the table via orm jdoql filter pushdown 
 gets partition names from the table via orm jdoql name filter 
 makes a jdo query filter string makes a jdo query filter string for tables or partitions 
 alters an existing partition initiates copy of sd returns the old cd 
 get table s column stats
 get partition s column stats
 return true if the current statistics in the metastore is valid for the query of the given txnid and queryvalidwriteidlist note that a statistics entity is valid iff the stats is written by the current query or the conjunction of the following two are true  column state accurate csa state is true  isolation level snapshot compliant with the query
 return true if the current statistics in the metastore is valid for the query of the given txnid and queryvalidwriteidlist note that a statistics entity is valid iff the stats is written by the current query or the conjunction of the following two are true  column state accurate csa state is true  isolation level snapshot compliant with the query
 double dispatch for treevisitor 
 generates a jdo filter statement
 generates a general filter given a map of generates a statement of the form key operator value   key operator value currently supported types for value are string and long the like operator for longs is unsupported 
 get partition column index in the table partition column list that corresponds to the key that is being filtered on by this tree node 
 validates and gets the query parameter for jdo filter pushdown based on the column and the constant stored in this node 
 generate the jdoql filter for the given expression tree
 makes expression tree out of expr 
 primary constructor that fetches all partitions in a given table given a hive object and a table object and a partial partition spec 
 given a list of partition ids returns the list of partially filled partitions based on the projection list used to instantiate this partitionprojectionevaluator
 given a list of partition fields checks if all the fields requested are single valued if all the fields are single valued returns list of equivalent mpartition fieldnames which can be used in the setresult clause of a jdo query
 removed cached classloaders from datanucleus datanucleus caches classloaders in nucleuscontext in udfs this can result in classloaders not getting gced resulting in permgen leaks this is particularly an issue when using embedded metastore with hiveserver since the current classloader gets modified with each new add jar becoming the classloader for downstream classes which datanucleus ends up using the nucleuscontext cache gets freed up only on calling a close on it we re not closing nucleuscontext since it does a bunch of other things which we don t want we re not clearing the cache hashmap by calling hashmap clear to avoid concurrency issues 
 drop a constraint any constraint i have no idea why add and get each have separate methods for each constraint type but drop has only one 
 add a given service to delegation token string 
 synchronize master key updates sequence generation for multiple nodes note link abstractdelegationtokensecretmanager keeps currentkey private so we needto utilize this hook to manipulate the key through the object reference this s workaround should cease to exist when hadoop supports token store 
 extension of rollmasterkey to remove expired keys from store 
 get jdbc connection to metastore db
 check if the current schema version in metastore matches the hive version
 execute a given metastore script this default version uses sqlline to execute the files which requires only running one file subclasses can use other executors 
 generates insert into t a b c values   f   c for appropriate db
 make preparedstatement object with list of string type parameters to be set it is assumed the input sql string have the number of equal to number of parameters passed as input 
 once a table is marked transactional you cannot go back enforce this also in current version transactional properties of the table cannot be altered after the table is created any attempt to alter it will throw a metaexception 
 want to make a a newly create table acid unless it explicitly has transactional false param if table can support it also see semanticanalyzer adddefaultproperties which performs the same logic this code path is more general since it is activated even if you create a table via thrift webhcat etc but some operations like ctas create the table metastore object as the last step i e after the data is written but write itself is has to be aware of the type of table so this listener is too late 
 normalize case and make sure  true is the only value to be set for transactional if set at all  if set to true we should also enforce bucketing and orc format
 check that inputformatclass outputformatclass should implement acidinputformat acidoutputformat
 loads object from a row in select from compaction queue
 this will look through the completed txn components table and look for partitions or tables that may be ready for compaction also look through txns and txn components tables for aborted transactions that we should add to the list 
 this will grab the next compaction request off of the queue and assign it to the worker 
 this will mark an entry in the queue as compacted and put it in the ready to clean state 
 find entries in the queue that are ready to be cleaned 
 see doc at link txnstore findminopentxnid note that link opentxns opentxnrequest makes update of next txn and min history levela single atomic operation and no one else should update these tables except the cleaner which deletes rows from min history level which can only allow minopentxn to move higher 
 this will remove an entry from the queue after it has been compacted 
 clean up entries from txn to write id table less than min uncommited txnid as found by min next txn id ntxn next min min history level mhl min open txnid min aborted txns txn id 
 clean up aborted transactions from txns that have no components in txn components the reason such txns exist can be that now work was done in this txn e g streaming opened transactionbatch and abandoned it w o doing any work or due to link markcleaned compactioninfo being called 
 this will take all entries assigned to workers on a host return them to initiated state the initiator should use this at start up to clean entries from any workers that were in the middle of compacting when the metastore shutdown it does not reset entries from worker threads on other hosts as those may still be working 
 this call will return all compaction queue entries assigned to a worker but over the timeout back to the initiated state this should be called by the initiator on start up and occasionally when running to clean up after dead threads at start up link revokefromlocalworkers string should be calledfirst 
 queries metastore db directly to find columns in the table which have statistics information if code ci includes partition info then per partition stats info is examined otherwisetable level stats are examined 
 for any given compactable entity partition table if not partitioned the history of compactions may look like sssfffaaasffss for example the idea is to retain the tail most recent of the history such that a configurable number of each type of state is present any other entries can be purged this scheme has advantage of always retaining the last failure success even if it s not recent 
 returns code true if there already exists sufficient number of consecutive failures forthis table partition so that no new automatic compactions will be scheduled user initiated compactions don t do this check do we allow compacting whole table when it s partitioned no though perhaps we should that would be a meta operations i e first find all partitions for this table which have txn info and schedule each compaction separately this avoids complications in this logic 
 if there is an entry in compaction queue with ci id remove it make entry in completed compactions with status f if there is no entry in compaction queue it means initiator failed to even schedule a compaction which we record as attempted state entry in history 
 a tool to count the number of partitions tables and databases locked by a particular lockid 
 return true if the transaction of the given txnid is open 
 utility method used to run count queries like select count from against metastore tables
 this is logically part of c tor and must be called prior to any other method not physically part of c tor due to use of reflection
 retry by caller note worst case it will leave an open txn which will timeout 
 concurrency isolation notes this is mutexed with link opentxns opentxnrequest and other link committxn committxnrequest operations using selectupdate on next txn id also mutexes on txnx table for specific txnid x see more notes below in order to prevent lost updates we need to determine if any  transactions overlap each txn is viewed as an interval m n m is the txnid and n is taken from the same next txn id sequence so that we can compare commit time of txn t with start time of txn s this sequence can be thought of as a logical time counter if s committime  t starttime t and s do not overlap motivating example suppose we have multi statment transactions t and s both of which are attempting x   x   in order to prevent lost update problem the the non overlapping txns must lock in the snapshot that they read appropriately in particular if txns do not overlap then one follows the other assumig they write the same entity and thus the nd must see changes of the st we ensure this by locking in snapshot after link opentxns opentxnrequest call is made see org apache hadoop hive ql driver acquirelocksandopentxn and mutexing opentxn with commit in other words once a s commit starts we must ensure that txn t which will be considered a later txn locks in a snapshot that includes the result of s s commit assuming no other txns as a counter example suppose we have s   and t   commitid txnid means no other transactions were running in parallel if t and s both locked in the same snapshot for example commit of txnid  which is possible if committxn and opentxnx is not mutexed x would be updated to the same value by both i e lost update 
 replicate table write ids state to mark aborted write ids and writeid high water mark 
 get invalidation info for the materialization currently the materialization information only contains information about whether there was update delete operations on the source tables used by the materialization since it was created 
 as much as possible i e in absence of retries we want both operations to be done on the same connection but separate transactions this avoid some flakiness in bonecp where if you perform an operation on  connection and immediately get another from the pool the nd one doesn t see results of the first retry by caller note if the call to lock is from a transaction then in the worst case there will be a duplicate set of locks but both sets will belong to the same txn so they will not conflict with each other for locks w o txn context i e read only query this may lead to deadlock at least a long wait e g st call creates locks in code lock waiting mode and response gets lost then link org apache hadoop hive metastore retryingmetastoreclient retries and enqueues another set of locks in lock waiting the nd lockresponse is delivered to the dblockmanager which will keep dong link checklock checklockrequest until the stset of locks times out 
 note that by definition select for update is divorced from update i e you executequery to read and then executeupdate one other alternative would be to actually update the row in txns but to the same value as before thus forcing db to acquire write lock for duration of the transaction there is no real reason to return the resultset here other than to make sure the reference to it is retained for duration of intended lock scope and is not gc d thus unlikely causing lock to be released 
 this enters locks into the queue in link lock waiting mode isolation level notes  we use su withe read committed to generate the next ext lock id this serializes any  code enqueuelockwithretry calls  we use su on the relevant txns row to block any concurrent abort commit etc operations
 why doesn t this get a txnid as parameter the caller should either know the txnid or know there isn t one either way gettxnidfromlockid will not be needed this would be a thrift change also when lock acquisition returns waiting it s retried every  seconds best case see dblockmanager backoff in practice more often which means this is heartbeating way more often than hive txn timeout and creating extra load on db the clients that operate in blocking mode can t heartbeat a lock until the lock is acquired we should make checklockrequest include timestamp or last request to skip unnecessary heartbeats thrift change link checklock java sql connection long must run at serializable make sure some lock we are checkingagainst doesn t move from w to a in another txn but this method can heartbeat in separate txn at read committed retry by caller note retryable because link checklock connection long is
 this would have been made simpler if all locks were associated with a txn then only txn needs to be heartbeated committed etc no need for client to track individual locks when removing locks not associated with txn this potentially conflicts with heartbeat performtimeout which are update delete of hive locks thus will be locked as needed by db since this only removes from hive locks at worst some lock acquire is delayed
 code ids should only have txnid or lockid but not both ideally currently dbtxnmanager heartbeat enforces this 
 retry by caller note this may be retried after dbconn commit at worst it will create duplicate entries in txn components which won t affect anything see more comments in link committxn committxnrequest 
 clean up corresponding records in metastore tables when corresponding object is dropped specifically txn components completed txn components compaction queue completed compactions retry by caller note this is only idempotent assuming it s only called by droptable db etc operations hive locks is presumably expected to be removed by acidhousekeeperservices ws set is presumably expected to be removed by acidwritesetservice
 catalog hasn t been added to transactional tables yet so it s passed in but not used 
 for testing only do not use 
 close statement instance 
 close the resultset 
 determine the current time using the rdbms as a source of truth
 determine the string that should be used to quote identifiers 
 todo expose this as an operation to client useful for streaming api to abort all remaining trasnactions in a batch on ioexceptions caller must rollback the transaction if not all transactions were aborted since this will not attempt to delete associated locks in this case 
 lock acquisition is meant to be fair so every lock can only block on some lock with smaller hl lock ext id by only checking earlier locks for any given sql statement all locks required by it are grouped under single extlockid and are granted all at once or all locks wait this is expected to run at read committed if there is a concurrent committxn rollbacktxn those can only remove rows from hive locks if they happen to be for the same txnid there will be a ww conflict in ms db if different txnid checklock will in the worst case keep locks in waiting state a little longer 
 heartbeats on the lock table this commits so do not enter it with any state should not be called on a lock that belongs to transaction 
 returns the state of the transaction iff it s able to determine it some cases where it cannot  txnid was aborted committed and then gc d compacted  txnid was committed but it didn t modify anything nothing in completed txn components 
 checks if all the txns in the list are in open state 
 checks if all the txns in the list are in open state 
 checks if all the txns in the list are in committed if yes throw eception 
 returns the state of the transaction with code txnid or throws if code raiseerror is true 
 isolation level notes plain rc is ok this will find transactions that have timed out and abort them will also delete locks which are not associated with a transaction and have timed out tries to keep transactions against metastore db small to reduce lock contention 
 updates the numfiles and totalsize parameters for the passed partition by querying the warehouse if the passed partition does not already have values for these parameters 
 create listener instances as per the configuration 
 a simple connect test to make sure that the metastore is up
 finds a free port on the machine 
 finds a free port on the machine but allow the ability to specify a port number to not use no matter what 
 this is a util method to set a nested property of a given object the nested property is a dot separated string where each nesting level is separated by a dot this method makes use of propertyutils methods from apache commons library and assumes that the field names provided in the input propertyname have valid setters eg the propertyname sd serdeinfo inputformat represents the inputformat field of the serdeinfo field of the sd field the argument bean should have these fields in this case it should be a partition object the value argument is the value to be set for the nested field note that if in case of one of nested levels is null you must set instantiatemissingfields argument to true otherwise this method could throw a npe 
 methods that test cachedstore
 testing alter partition string string partition  alter partition with environment context string string partition null 
 testing alter partition string string partition environmentcontext  alter partition with environment context string string partition environmentcontext 
 testing alter partitions string string list partition  alter partitions with environment context string string list partition null 
 testing alter partitions string string list partition environmentcontext  alter partitions with environment context string string list partition environmentcontext 
 testing renamepartition string string list string partition  renamepartition string string list string partition 
 this test creates and queries a database and then drops it good for testing the happy path 
 testing getpartition string string string  get partition by name string string string 
 testing getpartition string string list string  get partition string string list string 
 testing getpartitionsbynames string string list string  get partitions by names string string list string 
 testing getpartitionwithauthinfo string string list string string list string  get partition with auth string string list string string list string 
 testing listpartitions string string short  get partitions string string short 
 testing listpartitions string string list string short  get partitions string string list string short 
 testing listpartitionspecs string string int  get partitions pspec string string int 
 testing listpartitionswithauthinfo string string short string list string  get partitions with auth string string short string list string 
 testing listpartitionswithauthinfo string string list string short string list string  get partitions ps with auth string string list string short string list string 
 testing listpartitionsbyfilter string string string short  get partitions by filter string string string short 
 testing listpartitionspecsbyfilter string string string int  get part specs by filter string string string int 
 testing getnumpartitionsbyfilter string string string  get num partitions by filter string string string 
 testing listpartitionnames string string short  get partition names string string short 
 testing listpartitionnames string string list string short  get partition names ps string string list string short 
 testing listpartitionvalues partitionvaluesrequest  get partition values partitionvaluesrequest 
 this test creates and queries a table and then drops it good for testing the happy path
 param dbname
 param new part
 param table name
 param tbl
 param name
 param tbl name
 param db name
 inheritdoc 
 drop the table and choose whether to save the data in the trash 
 see droptable string string boolean boolean environmentcontext 
 drop the table and choose whether to delete the underlying table data throw if the table doesn t exist save the data in the trash 
 starts a metastore instance on get given port with the given configuration and thrift bridge use it only it the port is definitely free for tests use startmetastorewithretry instead so the metastore will find an emtpy port eventually so the different tests can be run on the same machine 
 starts a metastore instance with the given configuration and given bridge tries to find a free port and use it if failed tries another port so the tests will not fail if run parallel also adds the port to the warehouse dir so the multiple metastore instances will use different warehouse directories 
 a simple connect test to make sure that the metastore is up
 a simple connect test to make sure that the metastore uri is available in the zookeeper
 starts the service with adding extra configuration to the default ones be aware as the current metastore does not implement clean shutdown starting metastoreservice is possible only once per test 
 check if a path exists in the thrash directory 
 cleans the warehouse and the thrash dirs in preparation for the tests 
 tests create table and partition and tries to drop the table without droppping the partition
 tests for list partition by filter functionality 
 test filtering on table with single partition
 test filtering based on the value of the last partition
 verify that if another client either a metastore thrift server or a hive cli instance renames a table recently created by this instance and hence potentially in its cache the current instance still sees the change 
 this method simulates another hive metastore renaming a table by accessing the db and updating the name unfortunately derby cannot be run in two different jvms simultaneously but the only way to rename without having it put in this client s cache is to run a metastore in a separate jvm so this simulation is required 
 test changing owner and owner type of a database
 test table objects can be retrieved in batches
 while altering partition s verify do not calculate partition statistics if table property do not update stats is true stats auto gather is false is view 
 test to confirm that partitions can be added using partitionspecs 
 test to confirm that partition grouping behaves correctly when table schemas evolve partitions must be grouped by location and schema 
 test counts the number of liststatus calls in the msck core method of listing sub directories this is important to check since it unnecessary liststatus calls could cause performance degradation in remote filesystems like s the test creates a mock filesystem object and a mock directory structure to simulate a table which has  partition keys and  partition values at each level in the end it counts how many times the liststatus is called on the mock filesystem and confirm its equal to the current theoretical value 
 test database operations
 test the concurrent drop of same partition would leak transaction https issues apache org jira browse hive  note the leak happens during a race condition this test case tries to simulate the race condition on best effort it have two threads trying to drop the same set of partitions
 checks if the jdo cache is able to handle directsql partition drops in one session 
 checks if the jdo cache is able to handle directsql partition drops cross sessions 
 checks if the directsql partition drop removes every connected data from the rdbms tables 
 checks if the hms backend db row number is as expected if they are not an link assertionerror is thrown 
 test notification operations
 this test calls objectstore setconf methods from multiple threads each threads uses its own instance of objectstore to simulate thread local objectstore behaviour 
 tests partition operations
 test schema initialization
 initorupgrade takes init path
 initorupgrade takes upgrade path
 test validation for schema versions
 test schema upgrade
 test validate uri of locations
 write out a dummy pre upgrade script with given sql statement 
 verify queries can run against derby db as long as derby doesn t complain we assume the query is syntactically semantically correct 
 create a client to hive metastore if principal is specified create kerberised client 
 create database with the given name if it doesn t exist
 wrapper that moves all checked exceptions to runtimeexception 
 get server uri hms host is obtained from argument hms host environment parameter hms host java property use localhost if above fails hms port is obtained from argument host port string hms port environment variable hms port java property default port value 
 deserialize data written in the format used by the serializationutils methods readbiginteger writebiginteger and create a decimal using the supplied scale orc uses those serializationutils methods for its serialization a scratch bytes array is necessary to do the binary to decimal conversion for better performance pass a fast scratch buffer len serialization utils read byte array for scratchbytes 
 write the value of this decimal just like serializationutils writebiginteger it header comments are write the arbitrarily sized signed biginteger in vint format signed integers are encoded using the low bit as the sign bit using zigzag encoding each byte uses the low  bits for data and the high bit for stop continue bytes are stored lsb first note serializationutils writebiginteger sometimes pads the result with extra zeroes due to biginteger bitlength we do not emulate that serializationutils readbiginteger will produce the same result for both 
 standard writable method that deserialize the fields of this object from a datainput 
 standard writable method that serialize the fields of this object to a dataoutput 
 a variation of the standard writable method that serialize the fields of this object to a dataoutput with scratch buffers for good performance allocate scratchlongs with hivedecimal scratch longs len longs and allocate scratch buffer with hivedecimal scratch buffer len big integer bytes bytes 
 serialize a bloom filter
 deserialize a bloom filter read a byte stream which was written by linkplain serialize outputstream bloomfilter into a code bloomfilter 
 serialize a bloom filter serialized bloomkfilter format  byte for the number of hash functions  big endian int that is how outputstream works for the number of longs in the bitset big endian longs in the bloomkfilter bitset
 deserialize a bloom filter read a byte stream which was written by linkplain serialize outputstream bloomkfilter into a code bloomkfilter 
 returning a streaming connection to hive 
 initialize record writer 
 represents a batch of transactions acquired from metastore 
 close the transactionbatch this will abort any still open txns in this batch 
 make sure it works with table where bucket col is not st col
 test that streaming can write to unbucketed table 
 deprecated use link checkdatawritten path long long int string boolean string there is little value in using inputformat directly
 param validationquery query to read from table to compare data against code records 
 build a context with the properties read from an input stream 
 build a context with the properties read from a file
 build a context with the properties read from a file
 add properties from a file to the context
 add properties from a file to the context
 return properties loaded from files specified in qfiletests propertyfiles  filename   filepath 
 return failed tests
 executes parallel test until the parallel work queue is empty then executes the isolated tests on the host during each phase if a abortdroneexception is thrown the drone is removed possibly leaving this host with zero functioning drones if all drones are removed the host will be replaced before the next run 
 executes the test batch on the drone in question if the command exits with a status code of  throw an abortdroneexception 
 rsync file to all drones if any drones exit with a status of not  they will be removed from use possibly leaving this host with zero functioning drones 
 execute command on at least one drone the method will retry when the command exits with a status code of  until all drones have been utilized possibly excluding the host from future use 
 execute command on at least one drone the method will retry when the command exits with a status code of  until all drones have been utilized possibly excluding the host from future use 
 execute the given rsync if the command exits with a non zero exit status the command will be retried up to three times 
 execute the given command via the ssh command line tool if the command exits with status code  the command will be tries up to three times 
 this method will start a new thread to handle the yetus test patch script execution it creates a separate directory instantiates the yetus velocity template runs it and cleans it up after 
 creates  tables to query from
 param con
 param args
 todo change script comments to a preamble instead of a footer
 generates a set compaction commands to run on pre hive  cluster
 return any compaction commands to run for code table t 
 param location path to a partition or table if not partitioned dir
 preupgrade test tables that need to be compacted waits for compaction postupgrade generates scripts w o asserts
 param args